{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4b8aa3",
   "metadata": {},
   "source": [
    "# Permutation feature selection + LGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de43236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import eli5\n",
    "from IPython.display import display\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a90a5",
   "metadata": {},
   "source": [
    "## Set feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22f0d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.2\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/lib/python3/dist-packages (from pandas) (1.21.5)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, python-dateutil, pandas\n",
      "Successfully installed pandas-2.0.2 python-dateutil-2.8.2 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013ee29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DH_log*DU', 'DU*GL_log', 'BQ_log*EB', 'AB*CR_log', 'CR_log*EL',\n",
       "       'CD*CR_log', 'AB*BQ_log', 'DU_05*FR', 'DU_05*FL', 'AB*BQ_05',\n",
       "       'DU*FR_05', 'DU*FR_2', 'DU_2*FR', 'AB_2*BQ', 'DU*FL_2', 'BC*DU_2',\n",
       "       'BQ*EU_2'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# a = pd.read_csv('perm_df_plus_minus.csv')\n",
    "x = pd.read_csv('log.csv')\n",
    "y = pd.read_csv('pow_05.csv')\n",
    "z = pd.read_csv('pow_2.csv')\n",
    "\n",
    "res = pd.concat([x, y, z])\n",
    "res[res['importance'] >= 0.002]['Unnamed: 0'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c3e1aea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DU+FR',\n",
       " 'DU*FR',\n",
       " 'DU/GL',\n",
       " 'DU-EP',\n",
       " 'BQ/DA',\n",
       " 'DH/DU',\n",
       " 'AF/DL',\n",
       " 'AB+DU',\n",
       " 'AH-DU']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list_x = list(x[x['importance'] > 0.002].values[:,0])\n",
    "# list_y = list(y[y['importance'] > 0.002].values[:,0])\n",
    "# list_z = list(z[z['importance'] > 0.002].values[:,0])\n",
    "# list_zz = list(zz[zz['importance'] > 0.002].values[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3256ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_df.drop(['f_1', 'target'], axis=1).columns\n",
    "generated_features = pd.DataFrame()\n",
    "\n",
    "# with all of these features usually will work too long, so try to comment some of them \n",
    "# and select features separately\n",
    "for fe_a, fe_b in itertools.combinations(features, 2):\n",
    "\n",
    "    generated_features[f'{fe_a}+{fe_b}']   = train_df[fe_a] + train_df[fe_b]\n",
    "    generated_features[f'{fe_a}-{fe_b}']   = train_df[fe_a] - train_df[fe_b] \n",
    "    generated_features[f'{fe_a}*{fe_b}']   = train_df[fe_a] * train_df[fe_b]\n",
    "    generated_features[f'{fe_a}/{fe_b}']   = train_df[fe_a] / train_df[fe_b]\n",
    "\n",
    "    generated_features[f'{fe_a}*{fe_b}_2'] = train_df[fe_a] * train_df[fe_b].pow(2)\n",
    "    generated_features[f'{fe_a}_2*{fe_b}'] = train_df[fe_a].pow(2) * train_df[fe_b]\n",
    "    generated_features[f'{fe_a}_2']        = rain_df[fe_a].pow(2)\n",
    "    generated_features[f'{fe_b}_2']        = train_df[fe_b].pow(2)\n",
    "\n",
    "    generated_features[f'{fe_a}_05'] = train_df[fe_a].pow(0.5)\n",
    "    generated_features[f'{fe_b}_05'] = train_df[fe_b].pow(0.5)\n",
    "    generated_features[f'{fe_a}*{fe_b}_05'] = train_df[fe_a] * train_df[fe_b].pow(0.5)\n",
    "    generated_features[f'{fe_a}_05*{fe_b}'] = train_df[fe_a].pow(0.5) * train_df[fe_b]\n",
    "    \n",
    "    generated_features[f'{fe_a}_log'] = np.log(train_df[fe_a])\n",
    "    generated_features[f'{fe_b}_log'] = np.log(train_df[fe_b])\n",
    "    generated_features[f'{fe_a}*{fe_b}_log'] = train_df[fe_a] * np.log(train_df[fe_b])\n",
    "    generated_features[f'{fe_a}_log*{fe_b}'] = np.log(train_df[fe_a]) * train_df[fe_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e5704a",
   "metadata": {},
   "source": [
    "# Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM params\n",
    "params = {\n",
    "    'boosting_type': 'GBDT',\n",
    "    'objective':\"binary\",\n",
    "    'metric':'binary_logloss',\n",
    "    'random_state': 6052023,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "perm_df = pd.DataFrame()\n",
    "n_splits = 5\n",
    "n_rounds = 5\n",
    "\n",
    "#Balance & downsample\n",
    "sampler = RandomUnderSampler(sampling_strategy={0:200 , 1:100},random_state=0, replacement=False)\n",
    "# x_train_bal, y_train_bal = sampler.fit_resample(train_df.drop(['Class'], axis=1), train_df.Class)\n",
    "x_train_bal, y_train_bal = sampler.fit_resample(generated_features, train_df.Class)\n",
    "\n",
    "#Storage for oof scroe of current resampled dataset\n",
    "oof_score = pd.DataFrame(index=x_train_bal.index, columns=['preds'])\n",
    "\n",
    "#Split Data\n",
    "for i in range(n_rounds):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state = 5062023 + i)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(x_train_bal, y_train_bal)):\n",
    "        print(f'round - {i}, fold - {fold}')\n",
    "        \n",
    "        X_train, Y_train, X_val, Y_val = x_train_bal.iloc[train_index], y_train_bal[train_index], x_train_bal.iloc[val_index], y_train_bal[val_index]\n",
    "\n",
    "        clf = lgb.LGBMClassifier(**params, n_estimators = 1000)\n",
    "        clf.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_val, Y_val)], \n",
    "                early_stopping_rounds=30, eval_metric='logloss', verbose=50)\n",
    "\n",
    "        preds = clf.predict_proba(X_val)[:,1]\n",
    "\n",
    "        oof_score.loc[val_index, 'preds'] = preds\n",
    "\n",
    "\n",
    "        perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                     random_state=42, cv=None, refit=False).fit(X_val, Y_val)\n",
    "\n",
    "\n",
    "        perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                           index=X_val.columns).sort_index()\n",
    "\n",
    "        if perm_df.shape[0] == 0:\n",
    "            perm_df = perm_importance_df.copy()\n",
    "        else:\n",
    "            perm_df += perm_importance_df\n",
    "            \n",
    "        print('\\n')\n",
    "        \n",
    "# collect all permutation importances into the dataset and then avearge them by number of rounds * folds\n",
    "perm_df /= n_splits * n_rounds\n",
    "\n",
    "perm_df = perm_df.sort_values('importance', ascending=False)\n",
    "perm_df.to_csv('perm_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
