{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Install necesessary libraries","metadata":{}},{"cell_type":"code","source":"# norfair dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:01:39.995946Z","iopub.execute_input":"2022-02-14T14:01:39.996291Z","iopub.status.idle":"2022-02-14T14:02:55.290886Z","shell.execute_reply.started":"2022-02-14T14:01:39.996207Z","shell.execute_reply":"2022-02-14T14:02:55.290075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport cv2\nimport ast\nimport timm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport torch\nimport sys\nfrom fastai.vision.all import *\nfrom PIL import Image as Img\nfrom IPython.display import display\n# from norfair import Detection, Tracker","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:02:55.292983Z","iopub.execute_input":"2022-02-14T14:02:55.293236Z","iopub.status.idle":"2022-02-14T14:03:00.290303Z","shell.execute_reply.started":"2022-02-14T14:02:55.293197Z","shell.execute_reply":"2022-02-14T14:03:00.289471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Set model parameters","metadata":{}},{"cell_type":"code","source":"FOLD = 5\n\nROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef'\nDATASET_PATH = '/kaggle/input/tensorflow-great-barrier-reef/train_images/'\n\n!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/\n\narea_thr = 0\nimg_size = 2700\nconf = 0.1\naugment = True\niou = 0.4\nmin_bbox_size = 0\nconf_thr = 0.6\nconf_ratio = 0.4\n\n# yolov5s6_1920_batch_8_20_pcnt_empty_groupk = f'../input/yolov5s6/f2_sub2.pt'\nyolov5s6_1920_batch_8_20_pcnt_empty_groupk = f'../input/yolov5s6-1920-b8-10pct-adam-35ep-video/f2.pt'","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:03:00.291823Z","iopub.execute_input":"2022-02-14T14:03:00.292073Z","iopub.status.idle":"2022-02-14T14:03:01.784011Z","shell.execute_reply.started":"2022-02-14T14:03:00.292037Z","shell.execute_reply":"2022-02-14T14:03:01.783096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Evaluation\n\n## Evaluation utils","metadata":{}},{"cell_type":"code","source":"def calc_iou(bboxes1, bboxes2, bbox_mode='xywh'):\n    assert len(bboxes1.shape) == 2 and bboxes1.shape[1] == 4\n    assert len(bboxes2.shape) == 2 and bboxes2.shape[1] == 4\n    \n    bboxes1 = bboxes1.copy()\n    bboxes2 = bboxes2.copy()\n    \n    if bbox_mode == 'xywh':\n        bboxes1[:, 2:] += bboxes1[:, :2]\n        bboxes2[:, 2:] += bboxes2[:, :2]\n\n    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n    xA = np.maximum(x11, np.transpose(x21))\n    yA = np.maximum(y11, np.transpose(y21))\n    xB = np.minimum(x12, np.transpose(x22))\n    yB = np.minimum(y12, np.transpose(y22))\n    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n    return iou\n\ndef f_beta(tp, fp, fn, beta=2):\n    return (1+beta**2)*tp / ((1+beta**2)*tp+beta**2*fn+fp)\n\ndef calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th, verbose=False):\n    gt_bboxes = gt_bboxes.copy()\n    pred_bboxes = pred_bboxes.copy()\n    \n    tp = 0\n    fp = 0\n    for k, pred_bbox in enumerate(pred_bboxes): # fixed in ver.7\n        ious = calc_iou(gt_bboxes, pred_bbox[None, 1:])\n        max_iou = ious.max()\n        if max_iou > iou_th:\n            tp += 1\n            gt_bboxes = np.delete(gt_bboxes, ious.argmax(), axis=0)\n        else:\n            fp += 1\n        if len(gt_bboxes) == 0:\n            fp += len(pred_bboxes) - (k + 1) # fix in ver.7\n            break\n\n    fn = len(gt_bboxes)\n    return tp, fp, fn\n\ndef calc_is_correct(gt_bboxes, pred_bboxes):\n    \"\"\"\n    gt_bboxes: (N, 4) np.array in xywh format\n    pred_bboxes: (N, 5) np.array in conf+xywh format\n    \"\"\"\n    if len(gt_bboxes) == 0 and len(pred_bboxes) == 0:\n        tps, fps, fns = 0, 0, 0\n        return tps, fps, fns\n    \n    elif len(gt_bboxes) == 0:\n        tps, fps, fns = 0, len(pred_bboxes)*11, 0\n        return tps, fps, fns\n    \n    elif len(pred_bboxes) == 0:\n        tps, fps, fns = 0, 0, len(gt_bboxes)*11\n        return tps, fps, fns\n    \n    pred_bboxes = pred_bboxes[pred_bboxes[:,0].argsort()[::-1]] # sort by conf\n    \n    tps, fps, fns = 0, 0, 0\n    for iou_th in np.arange(0.3, 0.85, 0.05):\n        tp, fp, fn = calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th)\n        tps += tp\n        fps += fp\n        fns += fn\n    return tps, fps, fns\n\ndef calc_f2_score(gt_bboxes_list, pred_bboxes_list, verbose=False):\n    \"\"\"\n    gt_bboxes_list: list of (N, 4) np.array in xywh format\n    pred_bboxes_list: list of (N, 5) np.array in conf+xywh format\n    \"\"\"\n    tps, fps, fns = 0, 0, 0\n    for gt_bboxes, pred_bboxes in zip(gt_bboxes_list, pred_bboxes_list):\n        tp, fp, fn = calc_is_correct(gt_bboxes, pred_bboxes)\n        tps += tp\n        fps += fp\n        fns += fn\n        if verbose:\n            num_gt = len(gt_bboxes)\n            num_pred = len(pred_bboxes)\n            print(f'num_gt:{num_gt:<3} num_pred:{num_pred:<3} tp:{tp:<3} fp:{fp:<3} fn:{fn:<3}')\n    return f_beta(tps, fps, fns, beta=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:03:01.788235Z","iopub.execute_input":"2022-02-14T14:03:01.788455Z","iopub.status.idle":"2022-02-14T14:03:01.811415Z","shell.execute_reply.started":"2022-02-14T14:03:01.788428Z","shell.execute_reply":"2022-02-14T14:03:01.809376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tracking utils","metadata":{}},{"cell_type":"code","source":"from norfair import Detection, Tracker\n\n# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:03:01.812548Z","iopub.execute_input":"2022-02-14T14:03:01.812997Z","iopub.status.idle":"2022-02-14T14:03:01.871944Z","shell.execute_reply.started":"2022-02-14T14:03:01.812962Z","shell.execute_reply":"2022-02-14T14:03:01.871292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization utils","metadata":{}},{"cell_type":"code","source":"def show_prediction(img, bboxes, gts, show=True):\n    colors = [(0, 0, 255)]\n\n    obj_names = [\"s\"]\n\n    for box in bboxes:\n#         cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255,0,0), 2)\n        cv2.rectangle(img, (int(box[1]), int(box[2])), (int(box[1] + box[3]), int(box[2] + box[4])), (255,0,0), 2)\n        cv2.putText(img, f'{box[0]}', (int(box[1]), int(box[2])-3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n        \n    for gt in gts:\n        cv2.rectangle(img, (int(gt[0]), int(gt[1])), (int(gt[0]+gt[2]), int(gt[1]+gt[3])), (0,255,0), 2)\n    \n    if show:\n        img = Img.fromarray(img).resize((960, 540))\n    return img\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:03:01.873037Z","iopub.execute_input":"2022-02-14T14:03:01.873358Z","iopub.status.idle":"2022-02-14T14:03:01.882484Z","shell.execute_reply.started":"2022-02-14T14:03:01.873322Z","shell.execute_reply":"2022-02-14T14:03:01.881732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference utils","metadata":{}},{"cell_type":"code","source":"def is_cots(learner, img, bbox, bbox_conf, conf_ratio=0.5, visualize=False):   \n    cropped_img = img[bbox[1] : bbox[3], bbox[0] : bbox[2]]\n#     cropped_img = Img.fromarray(cropped_img).resize((240, 200))\n#     cropped_img = np.asarray(cropped_img)\n    \n    with learner.no_bar():\n        preds = learner.predict(cropped_img)\n    pred_conf = preds[2][0]\n    \n    conf = bbox_conf + (pred_conf-0.5) * conf_ratio\n    conf = max(conf, 0)\n    conf = min(conf, 1)\n    \n    if visualize:\n        display(Img.fromarray(cropped_img))\n        print(bbox_conf, bbox)\n        print(preds)\n        print(conf)\n        print('+'*80)\n    \n    return conf","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:03:01.884062Z","iopub.execute_input":"2022-02-14T14:03:01.884385Z","iopub.status.idle":"2022-02-14T14:03:01.897203Z","shell.execute_reply.started":"2022-02-14T14:03:01.884352Z","shell.execute_reply":"2022-02-14T14:03:01.896453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get GT bboxes from the dataset","metadata":{}},{"cell_type":"code","source":"dir = f'{DATASET_PATH}'\nimgs = [dir + f for f in ('video_2/5748.jpg',\n                          'video_2/5772.jpg',\n                          'video_2/5820.jpg',\n                          'video_1/4159.jpg', \n                          'video_1/4183.jpg', \n                          'video_1/4501.jpg', \n                          'video_1/5375.jpg', \n                          'video_1/5414.jpg',\n                          'video_1/5495.jpg',\n                          'video_1/4775.jpg', \n                          'video_0/9794.jpg', \n                          'video_0/4502.jpg', \n                          'video_0/9651.jpg', \n                          'video_0/9700.jpg',  \n                          'video_0/9674.jpg',\n                          'video_0/20.jpg', \n                          'video_0/17.jpg', \n                          'video_1/5474.jpg', \n                          'video_0/0.jpg')]\n\ndef get_path(row):\n    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n    return row\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\n# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\n# df = df[df.video_id == FOLD]\ndf = df.progress_apply(get_path, axis=1)\ndf['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf.head(2)\n\ndf['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")\n\ndf['bboxes'] = df.annotations.progress_apply(get_bbox)\n\ntest_df = df[df.sequence == 8503]\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:03:01.898481Z","iopub.execute_input":"2022-02-14T14:03:01.899001Z","iopub.status.idle":"2022-02-14T14:03:18.274363Z","shell.execute_reply.started":"2022-02-14T14:03:01.898959Z","shell.execute_reply":"2022-02-14T14:03:18.273456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate model","metadata":{}},{"cell_type":"code","source":"def clahe_hsv(img):\n    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n\n    h, s, v = hsv_img[:,:,0], hsv_img[:,:,1], hsv_img[:,:,2]\n    clahe = cv2.createCLAHE(clipLimit = 15.0, tileGridSize = (20,20))\n    v = clahe.apply(v)\n\n    hsv_img = np.dstack((h,s,v))\n\n    rgb = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n    \n    return rgb\n\ndef _load_model(path, conf, iou):\n    model = torch.hub.load('../input/yolov5-lib-ds', \n                          'custom', \n                          path = path,\n                          source='local',\n                          force_reload=True)  # local repo\n    model.conf = conf\n    model.iou = iou\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model\n\ndef evaluate(path, test_df, conf, iou, img_size, min_bbox_size, visualize=False, augment=True, tracking=True):\n    # Tracker will update tracks based on detections from current frame\n    tracker = Tracker(\n        distance_function=euclidean_distance, \n        distance_threshold=30,\n        hit_inertia_min=3,\n        hit_inertia_max=6,\n        initialization_delay=1,\n    )\n        \n    # Save frame_id into detection to know which tracks have no detections on current frame\n    frame_id = 0\n    \n    model = _load_model(path, conf, iou)\n    gt_bboxes_list, prd_bboxes_list = [], []\n\n    for idx, row in tqdm(test_df.iterrows()):\n        bboxes = np.empty((0,5), int)\n        gt_bboxes, pred_bboxes = [], []\n        \n        img_path = row.image_path\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n#         img = clahe_hsv(img)\n\n        # get GT bboxes for evaluation\n        for gt in row.bboxes:\n            gt_bbox = np.array(list(map(float, gt)))\n            gt_bboxes.append(gt_bbox)\n        gt_bboxes_list.append(np.array(gt_bboxes))\n\n        r = model(img, size=img_size, augment=augment)\n\n        if r.pandas().xyxy[0].shape[0] == 0:\n            anno = ''\n        else:\n            for idx, row in r.pandas().xyxy[0].iterrows():\n                  bboxes = np.append(bboxes, [[row.xmin, row.ymin, row.xmax, row.ymax, row.confidence]], axis=0)\n        \n        predictions = []\n        detects = []\n        \n        if len(bboxes) > 0:\n            for bbox in bboxes:\n                score = bbox[4]\n                width, height = int(bbox[2]-bbox[0]), int(bbox[3]-bbox[1])\n                area = width * height\n                \n                if width < min_bbox_size or height < min_bbox_size:\n                    continue\n                    \n                detects.append([int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]), score])\n                predictions.append('{:.2f} {} {} {} {}'.format(score, int(bbox[0]), int(bbox[1]), width, height))\n                pred_bboxes.append(np.array([score, int(bbox[0]), int(bbox[1]), width, height]))\n                    \n        #  Tracking\n        if tracking:\n            # Update tracks using detects from current frame\n            tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n            for tobj in tracked_objects:\n                bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n                if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n                    continue\n                    \n                # Skip too small boxes\n                if bbox_width < min_bbox_size or bbox_height < min_bbox_size:\n                    continue\n\n                # Add objects that have no detections on current frame to predictions\n                xc, yc = tobj.estimate[0]\n                x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n                score = tobj.last_detection.scores[0]\n\n                predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n                pred_bboxes.append(np.array([score, x_min, y_min, bbox_width, bbox_height]))\n        \n        if visualize and idx < 3:\n            display(show_prediction(img, pred_bboxes, gt_bboxes))\n        \n        # get pred bboxes for evaluation\n        prd_bboxes_list.append(np.array(pred_bboxes))\n        \n        prediction_str = ' '.join(predictions)\n        \n        frame_id += 1\n\n    f2_score = calc_f2_score(gt_bboxes_list, prd_bboxes_list, verbose=False)\n\n    return f2_score","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:03:18.275784Z","iopub.execute_input":"2022-02-14T14:03:18.276242Z","iopub.status.idle":"2022-02-14T14:03:18.30012Z","shell.execute_reply.started":"2022-02-14T14:03:18.276202Z","shell.execute_reply":"2022-02-14T14:03:18.299282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# area_dict = {}\n# best_area = 0\n# best_f2 = 0\n\n# for area_thr in np.arange(0, 450, 50):\n#     f2 = evaluate(path_m6, test_df, conf, iou, img_size, area_thr, augment=True, tracking=True)\n#     area_dict[area_thr] = f2\n#     print(80*'=')\n#     print(f'area threshold is {area_thr}, f2 is {f2}')\n#     print(80*'=')\n#     if f2 > best_f2:\n#         best_f2 = f2\n#         best_area = area_thr\n\n# best_area, best_f2, area_dict   \n# evaluate(path_l6, test_df, conf, iou, img_size, min_bbox_size, visualize=False, augment=True, tracking=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:03:18.303053Z","iopub.execute_input":"2022-02-14T14:03:18.303475Z","iopub.status.idle":"2022-02-14T14:03:18.319246Z","shell.execute_reply.started":"2022-02-14T14:03:18.303438Z","shell.execute_reply":"2022-02-14T14:03:18.318551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Inference\n\n## Initialize environment","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:03:18.323204Z","iopub.execute_input":"2022-02-14T14:03:18.323645Z","iopub.status.idle":"2022-02-14T14:03:18.348478Z","shell.execute_reply.started":"2022-02-14T14:03:18.32361Z","shell.execute_reply":"2022-02-14T14:03:18.347836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validation data","metadata":{}},{"cell_type":"code","source":"# def get_path(row):\n#     row['image_path'] = f'/kaggle/input/tensorflow-great-barrier-reef/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n#     return row\n\n# df = pd.read_csv('/kaggle/input/tensorflow-great-barrier-reef/train.csv')\n# df['annotations'] = df['annotations'].apply(lambda x: ast.literal_eval(x))\n# df['num_bbox'] = df['annotations'].apply(lambda x: len(x))\n# df['bboxes'] = df.annotations.progress_apply(get_bbox)\n# df = df.apply(get_path, axis=1)\n\n# image_paths = df[df.num_bbox>4].sample(10)\n# image_paths","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:09:16.895645Z","iopub.execute_input":"2022-02-14T14:09:16.896183Z","iopub.status.idle":"2022-02-14T14:09:16.899836Z","shell.execute_reply.started":"2022-02-14T14:09:16.896146Z","shell.execute_reply":"2022-02-14T14:09:16.899121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make predictions with tracking","metadata":{}},{"cell_type":"code","source":"submission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nmodel = _load_model(yolov5s6_1920_batch_8_20_pcnt_empty_groupk, conf, iou)\n\n#######################################################\n#                      Tracking                       #\n#######################################################\n\n# Tracker will update tracks based on detections from current frame\n# Matching based on euclidean distance between bbox centers of detections \n# from current frame and tracked_objects based on previous frames\n# You can check it's parameters in norfair docs\n# https://github.com/tryolabs/norfair/blob/master/docs/README.md\ntracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\n# Save frame_id into detection to know which tracks have no detections on current frame\nframe_id = 0\n#######################################################\n\n# Set parameters for fastai binary image classificator \nlearner_resnet18 = load_learner(f'../input/fastai-models/resnet18.pkl')\n\n# for idx, row in tqdm(image_paths.iterrows()):\n#     img = cv2.imread(row.image_path)\n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nfor img, pred_df in tqdm(iter_test):\n    anno = ''\n    bboxes = np.empty((0,5), int)\n    \n#     gt_bboxes, pred_bboxes = [], []\n    predictions, detects = [], []\n    \n    r = model(img, size=img_size, augment=augment)\n    \n#     # get GT bboxes for evaluation\n#     for gt in row.bboxes:\n#         gt_bbox = np.array(list(map(float, gt)))\n#         gt_bboxes.append(gt_bbox)\n    \n    if r.pandas().xyxy[0].shape[0] == 0:\n        anno = ''\n    else:\n        for idx, row in r.pandas().xyxy[0].iterrows():\n            bboxes = np.append(bboxes, [[row.xmin, row.ymin, row.xmax, row.ymax, row.confidence]], axis=0)\n        \n        # if image classifier recognize cots - increase bbox confidence, else decrease it\n        for bbox in bboxes:\n            score, xmin, ymin, xmax, ymax = bbox[4], int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n\n            bbox_conf = is_cots(learner_resnet18, img, [xmin, ymin, xmax, ymax], score, conf_ratio, visualize=False) \n            if bbox_conf > conf_thr:\n                width, height = xmax-xmin, ymax-ymin\n                detects.append(bbox)\n                predictions.append('{:.2f} {} {} {} {}'.format(bbox[4], int(bbox[0]), int(bbox[1]), width, height))\n#                 pred_bboxes.append(np.array([score, int(bbox[0]), int(bbox[1]), width, height]))\n                                   \n    #######################################################\n    #                      Tracking                       #\n    #######################################################\n    \n    # Update tracks using detects from current frame\n#     tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n#     for tobj in tracked_objects:\n#         bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n#         # Skip objects that were detected on current frame\n#         if last_detected_frame_id == frame_id:\n#             continue   \n#         # Add objects that have no detections on current frame to predictions\n#         xc, yc = tobj.estimate[0]\n#         x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n#         score = tobj.last_detection.scores[0]\n#         # Skip too small objects\n#         if bbox_width < min_bbox_size or bbox_height < min_bbox_size:\n#             continue \n        \n#         predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n#         pred_bboxes.append(np.array([score, x_min, y_min, bbox_width, bbox_height]))\n    #######################################################\n    \n    prediction_str = ' '.join(predictions)\n    pred_df['annotations'] = prediction_str\n    env.predict(pred_df)\n\n#     display(show_prediction(img, pred_bboxes, gt_bboxes))\n#     print('Prediction:', prediction_str)\n    frame_id += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:08:51.051151Z","iopub.execute_input":"2022-02-14T14:08:51.051711Z","iopub.status.idle":"2022-02-14T14:08:53.195269Z","shell.execute_reply.started":"2022-02-14T14:08:51.051675Z","shell.execute_reply":"2022-02-14T14:08:53.194389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check submission","metadata":{}},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:09:00.697881Z","iopub.execute_input":"2022-02-14T14:09:00.698184Z","iopub.status.idle":"2022-02-14T14:09:00.712699Z","shell.execute_reply.started":"2022-02-14T14:09:00.698154Z","shell.execute_reply":"2022-02-14T14:09:00.712057Z"},"trusted":true},"execution_count":null,"outputs":[]}]}