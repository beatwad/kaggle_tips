{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SAHI: Slicing Aided Hyper Inference for Yolov5 and YoloX\n\nA lightweight vision library for performing large scale object detection & instance segmentation on Kaggle. Full source code and tutorial you can find on Fatih Cagatay Akyon (author: Akyon, Fatih Cagatay and Cengiz, Cemil and Altinuc, Sinan Onur and Cavusoglu, Devrim and Sahin, Kadir and Eryuksel, Ogulcan) github: [SAHI: A vision library for large-scale object detection & instance segmentation](https://github.com/obss/sahi)\n\n* In this notebook (tutorial) you can find:\n* Installation of SAHI on Kaggle\n* Sliced inference with SAHI for Yolov5\n* Sliced inference with SAHI for YolovX (soon)\n\n\n<div class=\"alert alert-success\" role=\"alert\">\nOther my work in this competition:\n    <ul>\n        <li> <a href=\"https://www.kaggle.com/remekkinas/yolox-full-training-pipeline-for-cots-dataset\">YoloX full training pipeline for COTS dataset</a></li>\n        <li> <a href=\"https://www.kaggle.com/remekkinas/yolox-inference-on-kaggle-for-cots-lb-0-507\">YoloX detections submission made on COTS dataset</a></li>\n        <li> <a href=\"https://www.kaggle.com/remekkinas/yolor-p6-w6-one-more-yolo-on-kaggle-infer\">YoloR [P6/W6] ... one more yolo on Kaggle [INFER]</a></li>\n        <li> <a href=\"https://www.kaggle.com/remekkinas/yolor-p6-w6-one-more-yolo-on-kaggle-train\">YoloR [P6/W6]... one more yolo on Kaggle [TRAIN]</a></li>\n    </ul>\n    \n</div>\n\n\n<div class=\"alert alert-warning\">Note: My goal was to implement and share tool for experimentations  - I was not looking for best parameters to submit over 0.6 or ... even 0.7. This is your part of this journey. Enjoy experimenting and progressing!</div>","metadata":{}},{"cell_type":"markdown","source":"The concept of sliced inference is basically; performing inference over smaller slices of the original image and then merging the sliced predictions on the original image. It can be illustrated as below:","metadata":{}},{"cell_type":"markdown","source":"<div align=\"center\"><img src=\"https://raw.githubusercontent.com/obss/sahi/main/resources/sliced_inference.gif\"/></div>","metadata":{}},{"cell_type":"markdown","source":"# Install libraries","metadata":{}},{"cell_type":"code","source":"# norfair dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index\n\n# yolov5 + sahi\n%cd /kaggle/input/sahihub/s-lib\n!pip install ./fire-0.4.0/fire-0.4.0.tar -f ./ --no-index\n!pip install terminaltables-3.1.10-py2.py3-none-any.whl -f ./ --no-index\n!pip install sahi-0.8.22-py3-none-any.whl -f ./ --no-index\n!pip install thop-0.0.31.post2005241907-py3-none-any.whl -f ./ --no-index\n!pip install yolov5-6.0.6-py36.py37.py38-none-any.whl -f ./ --no-index\n!pip install yolo5-0.0.1-py36.py37.py38-none-any.whl -f ./ --no-index\n\n!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/sahihub/Arial.ttf /root/.config/Ultralytics/\n\n%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:10:37.624044Z","iopub.execute_input":"2022-01-31T08:10:37.624929Z","iopub.status.idle":"2022-01-31T08:12:52.748254Z","shell.execute_reply.started":"2022-01-31T08:10:37.624829Z","shell.execute_reply":"2022-01-31T08:12:52.746958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. IMPORT SAHI MODULES","metadata":{}},{"cell_type":"code","source":"FOLD = 1\nROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef'\nIMAGE_DIR = f'/kaggle/working/yolo_data/fold{FOLD}/images' # directory to save images\nLABEL_DIR = f'/kaggle/working/yolo_data/fold{FOLD}/labels' # directory to save labels.\nDATASET_PATH = '/kaggle/input/tensorflow-great-barrier-reef/train_images/'\nCKPT_PATH = '/kaggle/input/reef-baseline-fold12/l6_3600_uflip_vm5_f12_up/f1/best.pt'\n# CKPT_PATH = '/kaggle/input/yolov5-models/yolov5l_1920.pt'\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport sys\nimport cv2\nimport ast\nimport torch\nfrom PIL import Image as Img\nfrom IPython.display import display\nfrom norfair import Detection, Tracker\n\nfrom sahi.model import Yolov5DetectionModel\nfrom sahi.utils.cv import read_image\nfrom sahi.predict import get_prediction, get_sliced_prediction, predict\nfrom IPython.display import Image\nfrom sahi.utils.yolov5 import (\n    download_yolov5s6_model,\n)\nsys.path.append('../input/weightedboxesfusion/')\nfrom ensemble_boxes.ensemble_boxes_wbf import weighted_boxes_fusion","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:12:52.750564Z","iopub.execute_input":"2022-01-31T08:12:52.751045Z","iopub.status.idle":"2022-01-31T08:12:55.30224Z","shell.execute_reply.started":"2022-01-31T08:12:52.751004Z","shell.execute_reply":"2022-01-31T08:12:55.301356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A. YOLOv5 - get_sliced_prediction\nâ€‹\n* **image**: str or np.ndarray - Location of image or numpy image matrix to slice\n* **detection_model**: model.DetectionModel\n* **image_size**: int: Input image size for each inference (image is scaled by preserving asp. rat.).\n* **slice_height**: int: Height of each slice.  Defaults to ``512``.\n* **slice_width**: int: Width of each slice.  Defaults to ``512``.\n* **overlap_height_ratio**: float: Fractional overlap in height of each window (e.g. an overlap of 0.2 for a window of size 512 yields an overlap of 102 pixels). Default to ``0.2``.\n* **overlap_width_ratio**: float: Fractional overlap in width of each window (e.g. an overlap of 0.2 for a window of size 512 yields an overlap of 102 pixels). Default to ``0.2``.\n* **perform_standard_pred**: bool: Perform a standard prediction on top of sliced predictions to increase large object detection accuracy. Default: True.\n* **postprocess_type**: str: Type of the postprocess to be used after sliced inference while merging/eliminating predictions. Options are 'NMM', 'GRREDYNMM' or 'NMS'. Default is 'GRREDYNMM'.\n* **postprocess_match_metric**: str: Metric to be used during object prediction matching after sliced prediction. 'IOU' for intersection over union, 'IOS' for intersection over smaller area.\n* **postprocess_match_threshold**: float: Sliced predictions having higher iou than postprocess_match_threshold will be postprocessed after sliced prediction.\n* **postprocess_class_agnostic**: bool: If True, postprocess will ignore category ids.\n* **verbose**: int: 0: no print, 1: print number of slices (default), 2: print number of slices and slice/prediction durations\n\n### A1. CUSTOM Yolo5 PREDICTION CLASS\nThis is not obligatory but I decided to write this to have more control over prediction.\nIdea provided by Dewei Chen @dwchen in this discussion: https://www.kaggle.com/c/tensorflow-great-barrier-reef/discussion/302761","metadata":{}},{"cell_type":"code","source":"from sahi.prediction import ObjectPrediction\nfrom sahi.model import DetectionModel\nfrom typing import Dict, List, Optional, Union\nfrom sahi.utils.compatibility import fix_full_shape_list, fix_shift_amount_list\n\nclass COTSYolov5DetectionModel(DetectionModel):\n\n    \n    def load_model(self):\n        model = torch.hub.load('/kaggle/input/yolov5-lib-ds', \n                               'custom', \n                               path=self.model_path,\n                               source='local',\n                               force_reload=True)\n        \n        model.conf = self.confidence_threshold\n        self.model = model\n        \n        if not self.category_mapping:\n            category_mapping = {str(ind): category_name for ind, category_name in enumerate(self.category_names)}\n            self.category_mapping = category_mapping\n\n    def perform_inference(self, image: np.ndarray, image_size: int = None):\n        if image_size is not None:\n            warnings.warn(\"Set 'image_size' at DetectionModel init.\", DeprecationWarning)\n            prediction_result = self.model(image, size=image_size, augment=True)\n#             if debug_mode:\n#                 display(Img.fromarray(image).resize((320, 200)))\n        elif self.image_size is not None:\n            prediction_result = self.model(image, size=self.image_size, augment=True)\n        else:\n            prediction_result = self.model(image)\n\n        self._original_predictions = prediction_result\n\n    @property\n    def num_categories(self):\n        \"\"\"\n        Returns number of categories\n        \"\"\"\n        return len(self.model.names)\n\n    @property\n    def has_mask(self):\n        \"\"\"\n        Returns if model output contains segmentation mask\n        \"\"\"\n        has_mask = self.model.with_mask\n        return has_mask\n\n    @property\n    def category_names(self):\n        return self.model.names\n\n    def _create_object_prediction_list_from_original_predictions(\n        self,\n        shift_amount_list: Optional[List[List[int]]] = [[0, 0]],\n        full_shape_list: Optional[List[List[int]]] = None,):\n\n        original_predictions = self._original_predictions\n        shift_amount_list = fix_shift_amount_list(shift_amount_list)\n        full_shape_list = fix_full_shape_list(full_shape_list)\n\n        # handle all predictions\n        object_prediction_list_per_image = []\n        for image_ind, image_predictions_in_xyxy_format in enumerate(original_predictions.xyxy):\n            shift_amount = shift_amount_list[image_ind]\n            full_shape = None if full_shape_list is None else full_shape_list[image_ind]\n            object_prediction_list = []\n\n            # process predictions\n            for prediction in image_predictions_in_xyxy_format.cpu().detach().numpy():\n                x1 = int(prediction[0])\n                y1 = int(prediction[1])\n                x2 = int(prediction[2])\n                y2 = int(prediction[3])\n                bbox = [x1, y1, x2, y2]\n                score = prediction[4]\n                category_id = int(prediction[5])\n                category_name = self.category_mapping[str(category_id)]\n\n                # ignore invalid predictions\n                if bbox[0] > bbox[2] or bbox[1] > bbox[3] or bbox[0] < 0 or bbox[1] < 0 or bbox[2] < 0 or bbox[3] < 0:\n                    logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n                    continue\n                if full_shape is not None and (\n                    bbox[1] > full_shape[0]\n                    or bbox[3] > full_shape[0]\n                    or bbox[0] > full_shape[1]\n                    or bbox[2] > full_shape[1]\n                ):\n                    logger.warning(f\"ignoring invalid prediction with bbox: {bbox}\")\n                    continue\n\n                object_prediction = ObjectPrediction(\n                    bbox=bbox,\n                    category_id=category_id,\n                    score=score,\n                    bool_mask=None,\n                    category_name=category_name,\n                    shift_amount=shift_amount,\n                    full_shape=full_shape,\n                )\n                object_prediction_list.append(object_prediction)\n            object_prediction_list_per_image.append(object_prediction_list)\n\n        self._object_prediction_list_per_image = object_prediction_list_per_image ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:12:55.351566Z","iopub.execute_input":"2022-01-31T08:12:55.352395Z","iopub.status.idle":"2022-01-31T08:12:55.385045Z","shell.execute_reply.started":"2022-01-31T08:12:55.352354Z","shell.execute_reply":"2022-01-31T08:12:55.384294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. HELPER FUNCTIONS\n\n### Visualizaion","metadata":{}},{"cell_type":"code","source":"def show_prediction(img, bboxes, scores, gts, show=True):\n    colors = [(0, 0, 255)]\n\n    obj_names = [\"s\"]\n\n    for box, score in zip(bboxes, scores):\n#         cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255,0,0), 2)\n        cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[0] + box[2]), int(box[1] + box[3])), (255,0,0), 2)\n        cv2.putText(img, f'{score}', (int(box[0]), int(box[1])-3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n        \n    for gt in gts:\n        cv2.rectangle(img, (int(gt[0]), int(gt[1])), (int(gt[0]+gt[2]), int(gt[1]+gt[3])), (0,255,0), 2)\n    \n    if show:\n        img = Img.fromarray(img).resize((1280, 720))\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:12:55.387455Z","iopub.execute_input":"2022-01-31T08:12:55.388361Z","iopub.status.idle":"2022-01-31T08:12:55.414494Z","shell.execute_reply.started":"2022-01-31T08:12:55.388321Z","shell.execute_reply":"2022-01-31T08:12:55.413714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tracking","metadata":{}},{"cell_type":"code","source":"def predict(img, model, sw, sh, ohr, owr, pmt, img_size, verb):\n    result = get_sliced_prediction(img,\n                                   model,\n                                   slice_width = sw,\n                                   slice_height = sh,\n                                   overlap_height_ratio = ohr,\n                                   overlap_width_ratio = owr,\n                                   postprocess_match_threshold = pmt,\n                                   image_size = img_size,\n                                   verbose = verb,\n                                   perform_standard_pred = True)\n    \n    \n    bboxes = []\n    scores = []\n    result_len = result.to_coco_annotations()\n    for pred in result_len:\n        bboxes.append(pred['bbox'])\n        scores.append(pred['score'])\n    \n    return bboxes, scores ","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:12:55.416686Z","iopub.execute_input":"2022-01-31T08:12:55.418557Z","iopub.status.idle":"2022-01-31T08:12:55.429677Z","shell.execute_reply.started":"2022-01-31T08:12:55.418514Z","shell.execute_reply":"2022-01-31T08:12:55.428793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ORIG_IMG_SIZE = 1280\n\n# Prediction\ndef predict(img, model, sh, sw, ohr, owr, pmt, img_size, verb):\n    result = get_sliced_prediction(img,\n                                   model,\n                                   slice_height = sh,\n                                   slice_width = sw,\n                                   overlap_height_ratio = ohr,\n                                   overlap_width_ratio = owr,\n                                   postprocess_match_threshold = pmt,\n                                   image_size = img_size,\n                                   verbose = verb,\n                                   perform_standard_pred = True)\n    \n    bboxes = []\n    scores = []\n    result_len = result.to_coco_annotations()\n    for pred in result_len:\n        bboxes.append(pred['bbox'])\n        scores.append(pred['score'])\n    \n    return bboxes, scores \n\n\ndef add_bboxes(img, model, sw=768, sh=432, ohr=0.2, owr=0.2, pmt=0.45, img_size=3200):\n    global bboxes, scores\n    \n    # get predictions from SAHI and transform them to suitable format\n    bbox, score = predict(img, detection_model, sw=768, sh=432, ohr=0.2, owr=0.2, pmt=0.45, img_size=3200, verb=0)\n    \n    for i in range(len(bbox)):\n        bbox[i][2] += bbox[i][0]\n        bbox[i][3] += bbox[i][1]  \n    \n    if bbox:\n        bboxes.append(bbox)\n        scores.append(score)\n\n\n# WBF\ndef run_wbf(bboxes, scores, orig_image_size=ORIG_IMG_SIZE, iou_thr=0.4, skip_box_thr=0.0, weights=None):\n    # normalize bboxes\n    for i in range(len(bboxes)):\n        for j in range(len(bboxes[i])):\n            bboxes[i][j] = [bb/(ORIG_IMG_SIZE-1) for bb in bboxes[i][j]]\n    labels = [np.ones(len(score)) for score in scores]\n    # fuse bboxes with WBF\n    bboxes, scores, labels = weighted_boxes_fusion(bboxes, scores, labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    # return bboxes to the original size\n    bboxes = [bbox*(ORIG_IMG_SIZE-1) for bbox in bboxes]\n    return bboxes, scores","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:12:55.433941Z","iopub.execute_input":"2022-01-31T08:12:55.438851Z","iopub.status.idle":"2022-01-31T08:12:55.479789Z","shell.execute_reply.started":"2022-01-31T08:12:55.438802Z","shell.execute_reply":"2022-01-31T08:12:55.478955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:12:55.482668Z","iopub.execute_input":"2022-01-31T08:12:55.485384Z","iopub.status.idle":"2022-01-31T08:12:55.511161Z","shell.execute_reply.started":"2022-01-31T08:12:55.485339Z","shell.execute_reply":"2022-01-31T08:12:55.510213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Evaluation\n\n## Evaluation utils","metadata":{}},{"cell_type":"code","source":"def calc_iou(bboxes1, bboxes2, bbox_mode='xywh'):\n    assert len(bboxes1.shape) == 2 and bboxes1.shape[1] == 4\n    assert len(bboxes2.shape) == 2 and bboxes2.shape[1] == 4\n    \n    bboxes1 = bboxes1.copy()\n    bboxes2 = bboxes2.copy()\n    \n    if bbox_mode == 'xywh':\n        bboxes1[:, 2:] += bboxes1[:, :2]\n        bboxes2[:, 2:] += bboxes2[:, :2]\n\n    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n    xA = np.maximum(x11, np.transpose(x21))\n    yA = np.maximum(y11, np.transpose(y21))\n    xB = np.minimum(x12, np.transpose(x22))\n    yB = np.minimum(y12, np.transpose(y22))\n    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n    return iou\n\ndef f_beta(tp, fp, fn, beta=2):\n    return (1+beta**2)*tp / ((1+beta**2)*tp+beta**2*fn+fp)\n\ndef calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th, verbose=False):\n    gt_bboxes = gt_bboxes.copy()\n    pred_bboxes = pred_bboxes.copy()\n    \n    tp = 0\n    fp = 0\n    for k, pred_bbox in enumerate(pred_bboxes): # fixed in ver.7\n        ious = calc_iou(gt_bboxes, pred_bbox[None, 1:])\n        max_iou = ious.max()\n        if max_iou > iou_th:\n            tp += 1\n            gt_bboxes = np.delete(gt_bboxes, ious.argmax(), axis=0)\n        else:\n            fp += 1\n        if len(gt_bboxes) == 0:\n            fp += len(pred_bboxes) - (k + 1) # fix in ver.7\n            break\n\n    fn = len(gt_bboxes)\n    return tp, fp, fn\n\ndef calc_is_correct(gt_bboxes, pred_bboxes):\n    \"\"\"\n    gt_bboxes: (N, 4) np.array in xywh format\n    pred_bboxes: (N, 5) np.array in conf+xywh format\n    \"\"\"\n    if len(gt_bboxes) == 0 and len(pred_bboxes) == 0:\n        tps, fps, fns = 0, 0, 0\n        return tps, fps, fns\n    \n    elif len(gt_bboxes) == 0:\n        tps, fps, fns = 0, len(pred_bboxes)*11, 0\n        return tps, fps, fns\n    \n    elif len(pred_bboxes) == 0:\n        tps, fps, fns = 0, 0, len(gt_bboxes)*11\n        return tps, fps, fns\n    \n    pred_bboxes = pred_bboxes[pred_bboxes[:,0].argsort()[::-1]] # sort by conf\n    \n    tps, fps, fns = 0, 0, 0\n    for iou_th in np.arange(0.3, 0.85, 0.05):\n        tp, fp, fn = calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th)\n        tps += tp\n        fps += fp\n        fns += fn\n    return tps, fps, fns\n\ndef calc_f2_score(gt_bboxes_list, pred_bboxes_list, verbose=False):\n    \"\"\"\n    gt_bboxes_list: list of (N, 4) np.array in xywh format\n    pred_bboxes_list: list of (N, 5) np.array in conf+xywh format\n    \"\"\"\n    tps, fps, fns = 0, 0, 0\n    for gt_bboxes, pred_bboxes in zip(gt_bboxes_list, pred_bboxes_list):\n        tp, fp, fn = calc_is_correct(gt_bboxes, pred_bboxes)\n        tps += tp\n        fps += fp\n        fns += fn\n        if verbose:\n            num_gt = len(gt_bboxes)\n            num_pred = len(pred_bboxes)\n            print(f'num_gt:{num_gt:<3} num_pred:{num_pred:<3} tp:{tp:<3} fp:{fp:<3} fn:{fn:<3}')\n    return f_beta(tps, fps, fns, beta=2)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:12:55.51948Z","iopub.execute_input":"2022-01-31T08:12:55.522713Z","iopub.status.idle":"2022-01-31T08:12:55.630739Z","shell.execute_reply.started":"2022-01-31T08:12:55.521619Z","shell.execute_reply":"2022-01-31T08:12:55.629836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detection_model = COTSYolov5DetectionModel(\n   model_path = CKPT_PATH,\n   confidence_threshold = 0.28,\n   device=\"cuda\",\n   image_size=10250\n)\n\n# detection_model = Yolov5DetectionModel(\n#    model_path = CKPT_PATH,\n#    confidence_threshold = 0.25,\n#    device=\"cuda\",\n# )\n\ndetection_model.model.iou = 0.4\narea_thr = 300","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:12:55.63274Z","iopub.execute_input":"2022-01-31T08:12:55.636468Z","iopub.status.idle":"2022-01-31T08:13:06.79351Z","shell.execute_reply.started":"2022-01-31T08:12:55.636421Z","shell.execute_reply":"2022-01-31T08:13:06.792631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get GT bboxes from dataset","metadata":{}},{"cell_type":"code","source":"dir = f'{DATASET_PATH}'\nimgs = [dir + f for f in ('video_2/5748.jpg',\n                          'video_2/5772.jpg',\n                          'video_2/5820.jpg',\n                          'video_1/4159.jpg', \n                          'video_1/4183.jpg', \n                          'video_1/4501.jpg', \n                          'video_1/5375.jpg', \n                          'video_1/5414.jpg',\n                          'video_1/5495.jpg',\n                          'video_1/4775.jpg', \n                          'video_0/9794.jpg', \n                          'video_0/4502.jpg', \n                          'video_0/9651.jpg', \n                          'video_0/9700.jpg',  \n                          'video_0/9674.jpg',\n                          'video_0/20.jpg', \n                          'video_0/17.jpg', \n                          'video_1/5474.jpg', \n                          'video_0/0.jpg')]\n\ndef get_path(row):\n    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n    return row\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\n# Train Data\ndf = pd.read_csv(f'{ROOT_DIR}/train.csv')\n# df = df[df.video_id == FOLD]\ndf = df.progress_apply(get_path, axis=1)\ndf['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf.head(2)\n\ndf['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts(normalize=True)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")\n\ndf['bboxes'] = df.annotations.progress_apply(get_bbox)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:13:06.795212Z","iopub.execute_input":"2022-01-31T08:13:06.796303Z","iopub.status.idle":"2022-01-31T08:13:24.832366Z","shell.execute_reply.started":"2022-01-31T08:13:06.796264Z","shell.execute_reply":"2022-01-31T08:13:24.831495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate model","metadata":{}},{"cell_type":"code","source":"# #######################################################\n# #                      Tracking                       #\n# #######################################################\n\n# # Tracker will update tracks based on detections from current frame\n# # Matching based on euclidean distance between bbox centers of detections \n# # from current frame and tracked_objects based on previous frames\n# # You can check it's parameters in norfair docs\n# # https://github.com/tryolabs/norfair/blob/master/docs/README.md\n# tracker = Tracker(\n#     distance_function=euclidean_distance, \n#     distance_threshold=30,\n#     hit_inertia_min=3,\n#     hit_inertia_max=6,\n#     initialization_delay=1,\n# )\n    \n# # Save frame_id into detection to know which tracks have no detections on current frame\n# frame_id = 0\n# #######################################################\n\n# gt_bboxes_list, prd_bboxes_list = [], []\n\n# test_df = df[df.sequence == 8503]\n\n# # for idx, row in tqdm(df.iloc[7000:7200].iterrows()):\n# for idx, row in tqdm(test_df.iterrows()):\n# #     global bboxes, scores\n# #     bboxes, scores = [], []\n#     gt_bboxes, pred_bboxes = [], []\n    \n#     img_path = row.image_path\n#     img = cv2.imread(img_path)\n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n#     # get GT bboxes for evaluation\n#     for gt in row.bboxes:\n#         gt_bbox = np.array(list(map(float, gt)))\n#         gt_bboxes.append(gt_bbox)\n    \n#     gt_bboxes_list.append(np.array(gt_bboxes))\n\n# #     add_bboxes(img, detection_model, sw=768, sh=432, ohr=0.2, owr=0.2, pmt=0.45, img_size=3200)\n#     bboxes, scores = predict(img, detection_model, sw=768, sh=432, ohr=0.2, owr=0.2, pmt=0.45, img_size=3200, verb=0)\n    \n#     predictions = []\n#     detects = []\n    \n#     if len(bboxes) > 0:\n#         for bbox, score in zip(bboxes, scores):\n#             width, height = int(bbox[2]), int(bbox[3])\n#             area = width * height\n#             if area >= area_thr:\n#                 detects.append([int(bbox[0]), int(bbox[1]), int(bbox[0])+width, int(bbox[1])+height, score])\n#                 predictions.append('{:.2f} {} {} {} {}'.format(score, int(bbox[0]), int(bbox[1]), width, height))\n#                 pred_bboxes.append(np.array([score, int(bbox[0]), int(bbox[1]), width, height]))\n                \n# #     display(show_prediction(img, bboxes, scores, gt_bboxes))\n#     #######################################################\n#     #                      Tracking                       #\n#     #######################################################\n    \n#     # Update tracks using detects from current frame\n#     tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n#     for tobj in tracked_objects:\n#         bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n#         if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n#             continue\n\n#         # Add objects that have no detections on current frame to predictions\n#         xc, yc = tobj.estimate[0]\n#         x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n#         score = tobj.last_detection.scores[0]\n#         area = bbox_width * bbox_height\n#         if area >= area_thr:\n#             predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n#             pred_bboxes.append(np.array([score, x_min, y_min, bbox_width, bbox_height]))\n#     #######################################################\n    \n#     # get pred bboxes for evaluation\n#     prd_bboxes_list.append(np.array(pred_bboxes))\n    \n    \n#     prediction_str = ' '.join(predictions)\n    \n#     frame_id += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-30T22:25:07.957244Z","iopub.execute_input":"2022-01-30T22:25:07.95766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calc_f2_score(gt_bboxes_list, prd_bboxes_list, verbose=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"s6, IOU 0.25 - 0.5718199755816712\n\n\ns6, IOU 0.45 - 0.5703915447399105\n\n\nl6, IOU 0.25 - ","metadata":{}},{"cell_type":"markdown","source":"## 4. MAKE VIDEO FROM PREDS","metadata":{}},{"cell_type":"code","source":"import ast\nimport os\nimport pandas as pd\nimport subprocess\n\nfrom ast import literal_eval\nfrom tqdm.auto import tqdm\n\nfrom IPython.display import HTML\nfrom base64 import b64encode","metadata":{"execution":{"iopub.status.busy":"2022-01-30T11:32:41.958863Z","iopub.execute_input":"2022-01-30T11:32:41.959582Z","iopub.status.idle":"2022-01-30T11:32:41.968848Z","shell.execute_reply.started":"2022-01-30T11:32:41.959519Z","shell.execute_reply":"2022-01-30T11:32:41.967607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(video_id, video_frame, image_dir):\n    assert os.path.exists(image_dir), f'{image_dir} does not exist.'\n    img = cv2.imread(image_dir)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\n\n\ndef decode_annotations(annotaitons_str):\n    return literal_eval(annotaitons_str)\n\ndef load_image_with_annotations(img, annotations):\n#     annotations = decode_annotations(annotaitons_str)\n    if len(annotations) > 0:\n        for ann in annotations:\n            cv2.rectangle(img, (ann['x'], ann['y']),\n                (ann['x'] + ann['width'], ann['y'] + ann['height']),\n                (0, 255, 255), thickness=2,)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-01-30T11:32:41.970384Z","iopub.execute_input":"2022-01-30T11:32:41.971543Z","iopub.status.idle":"2022-01-30T11:32:41.982554Z","shell.execute_reply.started":"2022-01-30T11:32:41.971328Z","shell.execute_reply":"2022-01-30T11:32:41.981492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ## This code I found in: https://www.kaggle.com/bamps53/create-annotated-video Thank you for sharing.\n\n# def make_sahi_video(df, video_id, sequence_id, out_dir):\n#     fps = 15 \n#     width = 1280\n#     height = 720\n\n#     save_path = f'{out_dir}/video-{video_id}.mp4'\n#     tmp_path =  f'{out_dir}/tmp-video-{video_id}.mp4'\n#     output_video = cv2.VideoWriter(tmp_path, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, (width, height))\n    \n#     # I just generate any part of video\n#     video_df = df.query('video_id == @video_id and sequence == @sequence_id and video_frame > 5700 and video_frame < 6000')\n#     for _, row in tqdm(video_df.iterrows(), total=len(video_df)):\n#         video_id = row.video_id\n#         video_frame = row.video_frame\n#         annotations = row.annotations\n#         img_file = row.image_path\n#         img = load_image(video_id, video_frame, img_file)\n#         bboxes, scores = predict(img, detection_model, 768, 432, 0.2, 0.2, 0.45, 3200, 0)\n        \n#         gt_bboxes, pred_bboxes = [], []\n#         # get pred bboxes for evaluation\n#         for bbox in bboxes:\n#             width, height = int(bbox[2]), int(bbox[3])\n#             area = width * height\n#             if area >= area_thr:\n#                 pred_bboxes.append(bbox)\n#         # get GT bboxes for evaluation\n#         for gt in row.bboxes:\n#             gt_bbox = np.array(list(map(float, gt)))\n#             gt_bboxes.append(gt_bbox)\n\n#         img = show_prediction(img, pred_bboxes, scores, gt_bboxes, False)\n#         img = load_image_with_annotations(img, annotations)\n#         cv2.putText(img, f'{video_id}-{video_frame}', (10,70), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 1, cv2.LINE_AA)\n#         img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n#         output_video.write(img)\n\n    \n#     output_video.release()\n\n#     if os.path.exists(save_path):\n#         os.remove(save_path)\n#     subprocess.run(\n#         [\"ffmpeg\", \"-i\", tmp_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", save_path]\n#     )\n#     os.remove(tmp_path)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T12:22:24.47673Z","iopub.execute_input":"2022-01-30T12:22:24.477026Z","iopub.status.idle":"2022-01-30T12:22:24.493432Z","shell.execute_reply.started":"2022-01-30T12:22:24.476995Z","shell.execute_reply":"2022-01-30T12:22:24.492337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # To speed up I just generate any part of video\n# # This prediction is for sure overfitted but it is for demo only (I can see it on prediction)\n\n# debug_mode = False\n\n# make_sahi_video(df, 2, 22643, '/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T12:22:26.633742Z","iopub.execute_input":"2022-01-30T12:22:26.634671Z","iopub.status.idle":"2022-01-30T12:26:58.423048Z","shell.execute_reply.started":"2022-01-30T12:22:26.634618Z","shell.execute_reply":"2022-01-30T12:26:58.4188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def play(filename):\n#     html = ''\n#     video = open(filename,'rb').read()\n#     src = 'data:video/mp4;base64,' + b64encode(video).decode()\n#     html += '<video width=800 controls autoplay loop><source src=\"%s\" type=\"video/mp4\"></video>' % src \n#     return HTML(html)\n\n# play('/kaggle/working/video-2.mp4')","metadata":{"execution":{"iopub.status.busy":"2022-01-30T12:26:58.430065Z","iopub.execute_input":"2022-01-30T12:26:58.431973Z","iopub.status.idle":"2022-01-30T12:26:59.697833Z","shell.execute_reply.started":"2022-01-30T12:26:58.431915Z","shell.execute_reply":"2022-01-30T12:26:59.696471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Inference","metadata":{}},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env() # initialize the environment\niter_test = env.iter_test()       # an iterator which loops over the test set and sample submission\n\n#######################################################\n#                      Tracking                       #\n#######################################################\n\n# Tracker will update tracks based on detections from current frame\n# Matching based on euclidean distance between bbox centers of detections \n# from current frame and tracked_objects based on previous frames\n# You can check it's parameters in norfair docs\n# https://github.com/tryolabs/norfair/blob/master/docs/README.md\ntracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n    \n# Save frame_id into detection to know which tracks have no detections on current frame\nframe_id = 0\n#######################################################\n\nfor (img, sample_prediction_df) in tqdm(iter_test):\n    bboxes, scores = predict(img, detection_model, sw=768, sh=432, ohr=0.2, owr=0.2, pmt=0.45, img_size=3200, verb=0)\n    \n    predictions = []\n    detects = []\n    \n    if len(bboxes) > 0:\n         for bbox, score in zip(bboxes, scores):\n            width, height = int(bbox[2]), int(bbox[3])\n            area = width * height\n            if area >= area_thr:\n                detects.append([int(bbox[0]), int(bbox[1]), int(bbox[0])+width, int(bbox[1])+height, score])\n                predictions.append('{:.2f} {} {} {} {}'.format(score, int(bbox[0]), int(bbox[1]), width, height))\n    \n    #######################################################\n    #                      Tracking                       #\n    #######################################################\n    \n    # Update tracks using detects from current frame\n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n            \n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n        # Calculate area and add only those boxes which have area bigger than area threshold\n        area = bbox_width * bbox_height\n        if area >= area_thr:\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    #######################################################\n    \n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n    \n    frame_id += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:16:12.376206Z","iopub.execute_input":"2022-01-31T08:16:12.376523Z","iopub.status.idle":"2022-01-31T08:16:15.249582Z","shell.execute_reply.started":"2022-01-31T08:16:12.376488Z","shell.execute_reply":"2022-01-31T08:16:15.248844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-31T08:17:04.650836Z","iopub.execute_input":"2022-01-31T08:17:04.651633Z","iopub.status.idle":"2022-01-31T08:17:04.670152Z","shell.execute_reply.started":"2022-01-31T08:17:04.65159Z","shell.execute_reply":"2022-01-31T08:17:04.669035Z"},"trusted":true},"execution_count":null,"outputs":[]}]}