{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login in Weights&Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login your_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from ap_metric import score as ap_score\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.set_option('display.max_rows', 200)\n",
    "\n",
    "class CFG:\n",
    "    kaggle = False\n",
    "    type = 'train' # test, full\n",
    "    train_epochs = 2\n",
    "    sleep_period = 30\n",
    "    inter_sleep_period = 60\n",
    "    axis=3\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-4\n",
    "    decay_step_ratio = 0.4\n",
    "\n",
    "if CFG.kaggle:\n",
    "    path = '/kaggle/input/'\n",
    "else:\n",
    "    path = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load anglez\n",
    "anglez_features = np.load(f'{path}/anglez_features.npy')\n",
    "anglez_mean, anglez_std = np.load(f'{path}/anglez_mean.npy'), np.load(f'{path}/anglez_std.npy')\n",
    "anglez_features = (anglez_features - anglez_mean) / anglez_std\n",
    "\n",
    "# load data\n",
    "enmo_features = np.load(f'{path}/enmo_features.npy')\n",
    "enmo_mean, enmo_std = np.load(f'{path}/enmo_mean.npy'), np.load(f'{path}/enmo_std.npy')\n",
    "enmo_features = (enmo_features - enmo_mean) / enmo_std\n",
    "\n",
    "# load labels\n",
    "labels = np.load('data/labels.npy')\n",
    "\n",
    "# load the rest\n",
    "frames = pd.read_csv(f'{path}/frames.csv').drop('Unnamed: 0', axis=1)\n",
    "series_idxs = pd.read_csv(f'{path}/series_idxs.csv').drop('Unnamed: 0', axis=1)\n",
    "timeseries_idxs = pd.read_csv(f'{path}/timeseries_idxs.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_list = timeseries_idxs['series_id'].unique()\n",
    "train_threshold = int(len(series_list) * 0.8)\n",
    "train_list = series_list[:train_threshold]\n",
    "train_idx = timeseries_idxs[timeseries_idxs['series_id'].isin(train_list)].index\n",
    "X, y = (anglez_features[train_idx], enmo_features[train_idx]), labels[train_idx]\n",
    "val_list = series_list[train_threshold:]\n",
    "val_idx = timeseries_idxs[timeseries_idxs['series_id'].isin(val_list)].index\n",
    "X_val, y_val = (anglez_features[val_idx], enmo_features[val_idx]), labels[val_idx]\n",
    "del train_idx, val_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Model (ConvNet + LSTM)\n",
    "def cnn_bilstm(output_layer_width, height, width):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.TimeDistributed(layers.Conv2D(64, (5, 5), activation='elu'), input_shape=(None, height, width, 1)))\n",
    "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
    "    model.add(layers.TimeDistributed(layers.Conv2D(128, (3, 3), activation='elu')))\n",
    "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
    "    model.add(layers.TimeDistributed(layers.Conv2D(128, (3, 3), activation='elu')))\n",
    "    model.add(layers.TimeDistributed(layers.MaxPooling2D((2, 2))))\n",
    "    model.add(layers.TimeDistributed(layers.Flatten()))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(128, activation='elu')))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.TimeDistributed(layers.Dense(output_layer_width, activation=None)))\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    loss = tf.keras.losses.BinaryFocalCrossentropy(from_logits=True) # \n",
    "    model.compile(optimizer=opt, loss=loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(X, y):\n",
    "    # get data from two np arrays and concatenate them in one\n",
    "    X1, X2 = X\n",
    "    ds = tf.data.Dataset.from_tensor_slices((tf.concat([X1, X2], axis=3), y))\n",
    "    # ds = ds.shuffle(CFG.buffer_size).repeat()\n",
    "    ds = ds.batch(CFG.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# train function\n",
    "def train_model(model, X, y, X_val, y_val, epochs, decay_step_ratio, lr):\n",
    "    ds = get_data(X, y)\n",
    "    \n",
    "    if X_val is not None and y_val is not None:\n",
    "        val_ds = get_data(X_val, y_val)\n",
    "    else:\n",
    "        val_ds = None\n",
    "    \n",
    "    # checkpoint callback\n",
    "    checkpoint_path = os.path.join(path, 'train/weights', 'saved-model-{epoch:02d}-{val_loss:.4f}.keras')\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                                    filepath=checkpoint_path, \n",
    "                                                    save_weights_only=True,\n",
    "                                                    monitor='metrics',\n",
    "                                                    mode='max',\n",
    "                                                    save_best_only=False,\n",
    "                                                    save_freq='epoch',\n",
    "                                                    verbose=1\n",
    "                                                )\n",
    "    \n",
    "    # LR scheduler\n",
    "    decay_steps = epochs * decay_step_ratio\n",
    "    \n",
    "    cosine_decay = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                                                            lr,\n",
    "                                                            decay_steps=decay_steps,\n",
    "                                                            alpha=0.0,\n",
    "                                                            name=None,\n",
    "                                                            warmup_target=None,\n",
    "                                                            warmup_steps=0\n",
    "                                                        )\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(cosine_decay)\n",
    "\n",
    "    wnb_callback = WandbMetricsLogger()\n",
    "\n",
    "    # train history\n",
    "    hist = model.fit(ds, validation_data=val_ds, epochs=epochs, callbacks=[cp_callback, lr_callback, wnb_callback])\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=10, inter_op_parallelism_threads=10)\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=session_conf)\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init()\n",
    "\n",
    "# delete prevoius weights to avoid confusion\n",
    "file_list = sorted(glob.glob('train/weights/*.keras'), key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "for f in file_list:\n",
    "    os.remove(f) \n",
    "\n",
    "height = X[0].shape[2]\n",
    "width = X[0].shape[3] + X[1].shape[3]\n",
    "\n",
    "num_classes = y.shape[-1]\n",
    "model = cnn_bilstm(num_classes, height=height, width=width)\n",
    "hist = train_model(model, X, y, X_val, y_val, epochs=CFG.train_epochs, decay_step_ratio=CFG.decay_step_ratio, lr=CFG.learning_rate)\n",
    "    \n",
    "# df = pd.DataFrame(hist.history)\n",
    "# df.index.name = 'epoch'\n",
    " # df.to_csv(f'{path}train/model_training_log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = sorted(glob.glob('train/weights/*.keras'), key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "model = cnn_bilstm(num_classes, height=height, width=width)\n",
    "for f in file_list:\n",
    "    preds = model.predict(X_val)\n",
    "    # your code for evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
