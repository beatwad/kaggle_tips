{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17dc1f5",
   "metadata": {},
   "source": [
    "# LGBM + CV + training\n",
    "\n",
    "## Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "param_list = glob.glob(\"optuna_lgbm.csv\")\n",
    "models = list()\n",
    "best_lgbm_params = list()\n",
    "\n",
    "lgbm_params = pd.DataFrame()\n",
    "\n",
    "for f in param_list:\n",
    "    tmp = pd.read_csv(f, index_col='Unnamed: 0')\n",
    "    if lgbm_params.shape[0] == 0:\n",
    "        lgbm_params = tmp\n",
    "    else:\n",
    "        lgbm_params = pd.concat([lgbm_params, tmp])\n",
    "        \n",
    "lgbm_params = lgbm_params.sort_values('value').head(20)\n",
    "param_cols = [c for c in lgbm_params.columns if c.startswith('params_')]\n",
    "lgbm_params = lgbm_params[param_cols]\n",
    "\n",
    "for idx, row in lgbm_params.iterrows():\n",
    "    row_dict = {k[7:]: v for k, v in row.items()}\n",
    "    row_dict['objective'] = 'binary'\n",
    "    row_dict['metric'] = 'none'\n",
    "#     row_dict['subsample_for_bin'] = 300000\n",
    "    row_dict['force_col_wise'] = False\n",
    "    row_dict['early_stopping_rounds'] = 50\n",
    "    row_dict['verbose'] = -1\n",
    "    row_dict['max_bin'] = 255\n",
    "    row_dict['bagging_freq'] = int(row_dict['bagging_freq'])\n",
    "    row_dict['min_data_in_leaf'] = int(row_dict['min_data_in_leaf'])\n",
    "    row_dict['n_estimators'] = 3000 # int(row_dict['n_estimators'])\n",
    "    row_dict['learning_rate'] = float(row_dict['learning_rate'])\n",
    "    row_dict['num_leaves'] = int(row_dict['num_leaves'])\n",
    "    row_dict['max_depth'] = int(row_dict['max_depth'])\n",
    "    row_dict['is_unbalance'] = True\n",
    "    row_dict['class_weight'] = 'balanced'\n",
    "    row_dict['verbose'] = -1\n",
    "    \n",
    "    if row_dict['boosting_type'] == 'goss':\n",
    "        row_dict['bagging_fraction'] = None\n",
    "        \n",
    "    best_lgbm_params.append(row_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064c43c",
   "metadata": {},
   "source": [
    "## Train with the Cross Validation + RandomUnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def lgbm_training():\n",
    "    # Make random under-sampling to balance classes\n",
    "    positive_count_train = train_df['Class'].value_counts()[1]\n",
    "    sampler = RandomUnderSampler(sampling_strategy={0: positive_count_train, \n",
    "                                                    1: positive_count_train}, \n",
    "                                 random_state=15062021, \n",
    "                                 replacement=True)\n",
    "\n",
    "    X_re, y_re = pd.concat([train_df[features], greeks.iloc[:,1:4]], axis=1), train_df['Class']\n",
    "\n",
    "    if CFG.undersample:\n",
    "        X_re, y_re = sampler.fit_resample(X_re, y_re)\n",
    "    \n",
    "    kf = MultilabelStratifiedKFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=8062023+20)\n",
    "\n",
    "    oof_level2 = np.zeros([y_re.shape[0], len(best_lgbm_params) + 1])\n",
    "    oof_level2[:, len(best_lgbm_params)] = y_re\n",
    "    oof_level2_test = np.zeros([test_df.shape[0], len(best_lgbm_params)])\n",
    "\n",
    "    print(f\"Training with {blu}{X_re.shape[1]}{res} features\")\n",
    "\n",
    "    for fold, (fit_idx, val_idx) in tqdm(enumerate(kf.split(X=X_re, y=X_re.iloc[:,-3:]), start = 1),\n",
    "                                         total=CFG.n_stacking_folds):\n",
    "        X, y, test = X_re[features], y_re, test_df[features]\n",
    "        \n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        for i, params in enumerate(best_lgbm_params):\n",
    "            \n",
    "            clf = lgb.LGBMClassifier(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                    eval_metric=bll_metric, verbose=-1)\n",
    "\n",
    "            val_preds = clf.predict_proba(X_val)[:,1]\n",
    "            oof_level2[val_idx, i] = val_preds\n",
    "\n",
    "            val_score = balanced_log_loss(y_val, val_preds)\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(clf.best_iteration_)\n",
    "            \n",
    "            print(f'Fold: {blu}{fold:>3}{res}| bll_metric: {blu}{val_score:.5f}{res}'\n",
    "                  f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "            \n",
    "            oof_level2_test[:, i] += clf.predict_proba(test)[:,1]\n",
    "        \n",
    "    return oof_level2, oof_level2_test / CFG.n_stacking_folds\n",
    "\n",
    "if CFG.stacking:\n",
    "    oof_level2_lgbm, oof_level2_test_lgbm = lgbm_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36958e0",
   "metadata": {},
   "source": [
    "# CatBoost + CV + training\n",
    "\n",
    "## Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "param_list = glob.glob(\"optuna_catboost.csv\")\n",
    "models = list()\n",
    "best_cb_params = list()\n",
    "\n",
    "cb_params = pd.DataFrame()\n",
    "\n",
    "for f in param_list:\n",
    "    tmp = pd.read_csv(f, index_col='Unnamed: 0')\n",
    "    if cb_params.shape[0] == 0:\n",
    "        cb_params = tmp\n",
    "    else:\n",
    "        cb_params = pd.concat([cb_params, tmp])\n",
    "        \n",
    "cb_params = cb_params.sort_values('value').head(10)\n",
    "param_cols = [c for c in cb_params.columns if c.startswith('params_')]\n",
    "cb_params = cb_params[param_cols]\n",
    "\n",
    "\n",
    "for idx, row in cb_params.iterrows():\n",
    "    row_dict = {k[7:]: v for k, v in row.items()}\n",
    "    row_dict['task_type'] = 'CPU'\n",
    "    row_dict['auto_class_weights'] = 'Balanced'\n",
    "    row_dict['eval_metric'] = 'Logloss'\n",
    "    row_dict['loss_function'] = 'Logloss'\n",
    "    row_dict['random_seed'] = 13062023\n",
    "    row_dict['verbose'] = 0\n",
    "    row_dict['od_type'] = 'Iter'\n",
    "    row_dict['od_wait'] = 100\n",
    "    row_dict['border_count'] = 254\n",
    "    row_dict['iterations'] = 3000\n",
    "    row_dict['learning_rate'] = float(row_dict['learning_rate'])\n",
    "    row_dict['l2_leaf_reg'] = float(row_dict['l2_leaf_reg'])\n",
    "    row_dict['depth'] = int(row_dict['depth'])\n",
    "    row_dict['random_strength'] = float(row_dict['random_strength'])\n",
    "    row_dict['min_data_in_leaf'] = int(row_dict['min_data_in_leaf'])\n",
    "    \n",
    "    if row_dict[\"bootstrap_type\"] == \"Bayesian\":\n",
    "        row_dict['bagging_temperature'] = float(row_dict['bagging_temperature'])\n",
    "    else:    \n",
    "        row_dict['bagging_temperature'] = None\n",
    "        \n",
    "    if row_dict[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "        row_dict['subsample'] = float(row_dict['subsample'])\n",
    "    else:\n",
    "        row_dict['subsample'] = None\n",
    "    \n",
    "    if row_dict['grow_policy'] == 'Lossguide':\n",
    "        row_dict['max_leaves'] = int(row_dict['max_leaves'])\n",
    "    else:\n",
    "        row_dict['max_leaves'] = None\n",
    "    \n",
    "    if row_dict['grow_policy'] != 'SymmetricTree':\n",
    "        row_dict['boosting_type'] = 'Plain'\n",
    "    \n",
    "    best_cb_params.append(row_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05aaa6f",
   "metadata": {},
   "source": [
    "## Train with the Cross Validation + RandomUnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def cb_training():\n",
    "    # Make random under-sampling to balance classes\n",
    "    positive_count_train = train_df['Class'].value_counts()[1]\n",
    "    sampler = RandomUnderSampler(sampling_strategy={0: positive_count_train, \n",
    "                                                    1: positive_count_train}, \n",
    "                                 random_state=15062021, \n",
    "                                 replacement=True)\n",
    "\n",
    "    X_re, y_re = pd.concat([train_df[features], greeks.iloc[:,1:4]], axis=1), train_df['Class']\n",
    "\n",
    "    if CFG.undersample:\n",
    "        X_re, y_re = sampler.fit_resample(X_re, y_re)\n",
    "    \n",
    "    kf = MultilabelStratifiedKFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=8062023+20)\n",
    "\n",
    "    oof_level2 = np.zeros([y_re.shape[0], len(best_cb_params) + 1])\n",
    "    oof_level2[:, len(best_cb_params)] = y_re\n",
    "    oof_level2_test = np.zeros([test_df.shape[0], len(best_cb_params)])\n",
    "\n",
    "    print(f\"Training with {blu}{X_re.shape[1]}{res} features\")\n",
    "\n",
    "    for fold, (fit_idx, val_idx) in tqdm(enumerate(kf.split(X=X_re, y=X_re.iloc[:,-3:]), start = 1),\n",
    "                                         total=CFG.n_stacking_folds):\n",
    "        X, y, test = X_re[features], y_re, test_df[features]\n",
    "        \n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        train_pool = Pool(X_train, y_train, cat_features=['EJ'])\n",
    "        val_pool = Pool(X_val, y_val, cat_features=['EJ'])\n",
    "        \n",
    "        for i, params in enumerate(best_cb_params):\n",
    "            \n",
    "            model = cat.CatBoostClassifier(**params)\n",
    "            model.fit(train_pool, eval_set=val_pool, verbose=0)\n",
    "\n",
    "            val_preds = model.predict_proba(val_pool)[:,1]\n",
    "            oof_level2[val_idx, i] = val_preds\n",
    "\n",
    "            val_score = balanced_log_loss(y_val, val_preds)\n",
    "            best_iter = model.best_iteration_\n",
    "\n",
    "            print(model.best_iteration_)\n",
    "            \n",
    "            print(f'Fold: {blu}{fold:>3}{res}| bll_metric: {blu}{val_score:.5f}{res}'\n",
    "                  f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "        \n",
    "            oof_level2_test[:, i] += model.predict_proba(test)[:,1]\n",
    "        \n",
    "    return oof_level2, oof_level2_test / CFG.n_stacking_folds\n",
    "\n",
    "if CFG.stacking:\n",
    "    oof_level2_cb, oof_level2_test_cb = cb_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaccaa4",
   "metadata": {},
   "source": [
    "# XGBoost + CV + training\n",
    "\n",
    "## Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "param_list = glob.glob(\"optuna_xgb.csv\")\n",
    "models = list()\n",
    "best_xb_params = list()\n",
    "\n",
    "xb_params = pd.DataFrame()\n",
    "\n",
    "for f in param_list:\n",
    "    tmp = pd.read_csv(f, index_col='Unnamed: 0')\n",
    "    if xb_params.shape[0] == 0:\n",
    "        xb_params = tmp\n",
    "    else:\n",
    "        xb_params = pd.concat([cb_params, tmp])\n",
    "        \n",
    "xb_params = xb_params.sort_values('value').head(10)\n",
    "param_cols = [c for c in xb_params.columns if c.startswith('params_')]\n",
    "xb_params = xb_params[param_cols]\n",
    "\n",
    "for idx, row in xb_params.iterrows():\n",
    "    row_dict = {k[7:]: v for k, v in row.items()}\n",
    "    row_dict['n_estimators'] = 3000\n",
    "    row_dict['random_state'] = 14062023\n",
    "    row_dict['early_stopping_rounds'] = 100\n",
    "    row_dict['verbosity'] = 0\n",
    "    row_dict['scale_pos_weight'] = 4.71\n",
    "    row_dict['objective'] = \"binary:logistic\"\n",
    "    row_dict['eval_metric'] = \"logloss\"\n",
    "    row_dict['tree_method'] = \"exact\"\n",
    "    row_dict['booster'] = \"gbtree\"\n",
    "\n",
    "    if row_dict[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        row_dict[\"learning_rate\"] = float(row_dict['learning_rate'])\n",
    "        row_dict[\"max_depth\"] = int(row_dict['max_depth'])\n",
    "        row_dict[\"min_child_weight\"] = float(row_dict['min_child_weight'])\n",
    "        row_dict[\"gamma\"] = float(row_dict['gamma'])\n",
    "    else:\n",
    "        row_dict[\"learning_rate\"] = None\n",
    "        row_dict[\"max_depth\"] = None\n",
    "        row_dict[\"min_child_weight\"] = None\n",
    "        row_dict[\"gamma\"] = None\n",
    "        row_dict[\"grow_policy\"] = None     \n",
    "\n",
    "    if row_dict[\"booster\"] == \"dart\":\n",
    "        row_dict[\"rate_drop\"] = float(row_dict['rate_drop'])\n",
    "        row_dict[\"skip_drop\"] = float(row_dict['skip_drop'])\n",
    "    else:\n",
    "        row_dict[\"sample_type\"] = None\n",
    "        row_dict[\"normalize_type\"] = None\n",
    "        row_dict[\"rate_drop\"] = None\n",
    "        row_dict[\"skip_drop\"] = None\n",
    "\n",
    "    best_xb_params.append(row_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f99e9",
   "metadata": {},
   "source": [
    "## Train with the Cross Validation + RandomUnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def xgboost_training():\n",
    "    # Make random under-sampling to balance classes\n",
    "    positive_count_train = train_df['Class'].value_counts()[1]\n",
    "    sampler = RandomUnderSampler(sampling_strategy={0: positive_count_train, \n",
    "                                                    1: positive_count_train}, \n",
    "                                 random_state=15062020, \n",
    "                                 replacement=True)\n",
    "\n",
    "    X_re, y_re = pd.concat([train_df[features], greeks.iloc[:,1:4]], axis=1), train_df['Class']\n",
    "\n",
    "    if CFG.undersample:\n",
    "        X_re, y_re = sampler.fit_resample(X_re, y_re)\n",
    "    \n",
    "    kf = MultilabelStratifiedKFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=8062023+20)\n",
    "\n",
    "    oof_level2 = np.zeros([y_re.shape[0], len(best_xb_params) + 1])\n",
    "    oof_level2[:, len(best_xb_params)] = y_re\n",
    "    oof_level2_test = np.zeros([test_df.shape[0], len(best_xb_params)])\n",
    "\n",
    "    print(f\"Training with {blu}{X_re.shape[1]}{res} features\")\n",
    "\n",
    "    for fold, (fit_idx, val_idx) in tqdm(enumerate(kf.split(X=X_re, y=X_re.iloc[:,-3:]), start = 1),\n",
    "                                         total=CFG.n_stacking_folds):\n",
    "        X, y, test = X_re[features], y_re, test_df[features]\n",
    "        \n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        for i, params in enumerate(best_xb_params):\n",
    "            clf = xgb.XGBClassifier(**params)\n",
    "            \n",
    "            clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=500)\n",
    "\n",
    "            val_preds = clf.predict_proba(X_val)[:,1]\n",
    "            oof_level2[val_idx, i] = val_preds\n",
    "\n",
    "            val_score = balanced_log_loss(y_val, val_preds)\n",
    "            best_iter = 0\n",
    "\n",
    "#             print(clf.best_iteration_)\n",
    "            \n",
    "            print(f'Fold: {blu}{fold:>3}{res}| bll_metric: {blu}{val_score:.5f}{res}'\n",
    "                  f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "        \n",
    "            oof_level2_test[:, i] += clf.predict_proba(test)[:,1]\n",
    "        \n",
    "    return oof_level2, oof_level2_test / CFG.n_stacking_folds\n",
    "\n",
    "if CFG.stacking:\n",
    "    oof_level2_xgb, oof_level2_test_xgb = xgboost_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba604f33",
   "metadata": {},
   "source": [
    "# Stacking LGBM + CatBoost + XGBoost with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204709b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "oof_level2 = np.concatenate([oof_level2_lgbm[:,:-1] , oof_level2_cb[:,:-1], oof_level2_xgb[:,:-1]], axis=1)\n",
    "oof_level2_test = np.concatenate([oof_level2_test_lgbm , oof_level2_test_cb, oof_level2_test_xgb], axis=1)\n",
    "\n",
    "X = oof_level2\n",
    "y = oof_level2_lgbm[:,-1]\n",
    "\n",
    "# mean bll\n",
    "print(balanced_log_loss(y, np.mean(X, axis=1)))\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(X, y)\n",
    "\n",
    "pred = lr.predict_proba(X)[:,1]\n",
    "\n",
    "# lr bll\n",
    "print(balanced_log_loss(y, pred))\n",
    "\n",
    "weights = lr.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce02d1",
   "metadata": {},
   "source": [
    "# LGBM + TSS + Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d945bfb",
   "metadata": {},
   "source": [
    "## Create metafeatures for the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5121d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates for the second level\n",
    "dates_level2 = [28, 29, 30, 31, 32, 33]\n",
    "\n",
    "# train dates\n",
    "dates_train = X_train.date_block_num\n",
    "# dates_train_level2 = dates_train[dates_train.isin(dates)]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin(dates_level2)]\n",
    "\n",
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], len(best_params)+1])\n",
    "X_train_level2[:, len(best_params)] = y_train_level2\n",
    "\n",
    "meta_index_begin = 0\n",
    "meta_index_end = 0\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in tqdm_notebook(dates_level2):\n",
    "    \n",
    "    # split data\n",
    "    train_index = X_train.loc[dates_train <  cur_block_num].index\n",
    "    test_index  = X_train.loc[dates_train == cur_block_num].index\n",
    "    \n",
    "    X_train_l2 = X_train.loc[train_index, :]\n",
    "    X_test_l2 =  X_train.loc[test_index, :]\n",
    "\n",
    "    y_train_l2 = y_train[train_index]\n",
    "    y_test_l2 =  y_train[test_index]\n",
    "    \n",
    "    meta_index_end += y_test_l2.shape[0]\n",
    "    \n",
    "    # predict metafeatures for each of LGBM regressors\n",
    "    for i, params in enumerate(tqdm_notebook(best_params)):\n",
    "        lgb = LGBMRegressor(**params)\n",
    "        reg = lgb.fit(X_train_l2, y_train_l2)\n",
    "        pred = lgb.predict(X_test_l2)\n",
    "        X_train_level2[meta_index_begin:meta_index_end, i] = pred\n",
    "\n",
    "        del lgb, reg, pred\n",
    "        gc.collect()\n",
    "        \n",
    "    meta_index_begin = meta_index_end\n",
    "        \n",
    "X_train_level2 = pd.DataFrame(X_train_level2, columns=[f'lgbr_{i+1}' for i in range(len(best_params))]+['gt'])\n",
    "X_train_level2.to_pickle('LGBM_X_train_level2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cebfa7",
   "metadata": {},
   "source": [
    "## Create metafeatures for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_level2 = np.zeros([X_test.shape[0], len(best_params)])\n",
    "\n",
    "for i, params in enumerate(tqdm_notebook(best_params)):\n",
    "    lgb = LGBMRegressor(**params)\n",
    "    reg = lgb.fit(X_train, y_train)\n",
    "    pred = lgb.predict(X_test)\n",
    "    X_test_level2[:, i] = pred\n",
    "    \n",
    "    del lgb, reg, pred\n",
    "    gc.collect()\n",
    "    \n",
    "X_test_level2 = pd.DataFrame(X_test_level2, columns=[f'lgbr_{i+1}' for i in range(len(best_params))])\n",
    "X_test_level2.to_pickle('LGBM_X_test_level2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cada6",
   "metadata": {},
   "source": [
    "## Stacking with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = X_train_level2[[c for c in X_train_level2.columns if c != 'gt']]\n",
    "y = X_train_level2['gt']\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "pred = lr.predict(X_test_level2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
