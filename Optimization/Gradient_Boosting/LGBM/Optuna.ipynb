{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e36599c1",
   "metadata": {},
   "source": [
    "# Optuna for LightGBM\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd90382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8aa7ef62",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a32d03f4",
   "metadata": {},
   "source": [
    "For Better Accuracy:\n",
    "\n",
    "- Use large max_bin (may be slower)\n",
    "\n",
    "- Use small learning_rate with large num_iterations\n",
    "\n",
    "- Use large num_leaves (may cause over-fitting)\n",
    "\n",
    "- Use bigger training data\n",
    "\n",
    "- Try dart\n",
    "\n",
    "Deal with Over-fitting:\n",
    "\n",
    "- Use small max_bin\n",
    "\n",
    "- Use small num_leaves\n",
    "\n",
    "- Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "\n",
    "- Use bagging by set bagging_fraction and bagging_freq\n",
    "\n",
    "- Use feature sub-sampling by set feature_fraction\n",
    "\n",
    "- Use bigger training data\n",
    "\n",
    "- Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n",
    "\n",
    "- Try max_depth to avoid growing deep tree\n",
    "\n",
    "- Try extra_trees\n",
    "\n",
    "- Try increasing path_smooth\n",
    "\n",
    "# Optuna + Multilabel CV + RandomUnderSampling/RandomOverSampling + custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_log_loss(y_true, y_pred):\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability ùëù is replaced with max(min(ùëù,1‚àí10‚àí15),10‚àí15)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1 - y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def calc_log_loss_weight(y_true): \n",
    "    '''w0, w1 assign different weights to individual data points during training.'''\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1\n",
    "\n",
    "X, y = train_df[features], train_df['Class']\n",
    "best_iterations = list()\n",
    "    \n",
    "def optimize_model(params, how):\n",
    "    bll_list = list()\n",
    "    best_trial_iterations = list()\n",
    "    \n",
    "    for i in range(CFG.n_optimize_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "\n",
    "        # Make random under- or oversampling to balance classes\n",
    "        if CFG.undersample:\n",
    "            positive_count_train = train_df['Class'].value_counts()[1]\n",
    "            sampler = RandomUnderSampler(sampling_strategy={0: positive_count_train * class_imbalance, \n",
    "                                                            1: positive_count_train}, \n",
    "                                        random_state=15062023+i, \n",
    "                                        replacement=True)\n",
    "        elif CFG.oversample:\n",
    "            negative_count_train = train_df['Class'].value_counts()[0]\n",
    "            sampler = RandomOverSampler(sampling_strategy={0: negative_count_train, \n",
    "                                                        1: negative_count_train // class_imbalance}, \n",
    "                                        random_state=2306020231)\n",
    "\n",
    "\n",
    "        X_re, y_re = pd.concat([train_df[features], greeks.iloc[:,1:4]], axis=1), train_df['Class']\n",
    "        \n",
    "        if CFG.undersample:\n",
    "            X_re, y_re = sampler.fit_resample(X_re, y_re)\n",
    "        \n",
    "        # Create Stratified Multilabel k-Fold scheme\n",
    "        kf = MultilabelStratifiedKFold(n_splits=CFG.n_optimize_folds, shuffle=True, random_state=10062023+i)\n",
    "\n",
    "        # Create an oof array for inner loop\n",
    "        oof = np.zeros(X_re.shape[0])\n",
    "\n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X=X_re[features], y=X_re.iloc[:,-3:]), start=1): \n",
    "            X, y = X_re[features], y_re\n",
    "            \n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            # oversample\n",
    "            if CFG.oversample:\n",
    "                X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "            if how == 'lgbm':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=bll_metric, verbose=0)\n",
    "                best_iter = model.best_iteration_\n",
    "            elif how == 'xgboost':\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0)\n",
    "                best_iter = model.get_booster().best_iteration\n",
    "            elif how == 'catboost':\n",
    "                train_pool = Pool(X_train, y_train, cat_features=['EJ'])\n",
    "                val_pool = Pool(X_val, y_val, cat_features=['EJ']) \n",
    "                model = cat.CatBoostClassifier(**params)\n",
    "                model.fit(train_pool, eval_set=val_pool, verbose=0)\n",
    "                best_iter = model.best_iteration_\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "            val_preds = model.predict_proba(X_val)[:,1]\n",
    "            oof[val_idx] = val_preds\n",
    "        \n",
    "        bll_list.append(balanced_log_loss(y_re, oof))\n",
    "        best_trial_iterations.append(best_iter)\n",
    "    \n",
    "    best_iterations.append(int(np.mean(best_trial_iterations)))\n",
    "\n",
    "    return np.mean(bll_list)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        # Main parameters\n",
    "#                     'device': 'gpu',\n",
    "#                     'gpu_platform_id': 0,\n",
    "#                     'gpu_device_id': 0,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'none',\n",
    "        'boosting_type': trial.suggest_categorical('boosting_type', ['goss', 'gbdt']),#, 'dart']),   \n",
    "        # Hyperparamters (in order of importance decreasing)\n",
    "        'n_estimators': CFG.n_estimators, # trial.suggest_int('n_estimators', 500, 1500),  # max number of trees in model\n",
    "        'early_stopping_round': CFG.early_stopping_rounds, \n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 3e-1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True), # L1,  alias: lambda_l1\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True), # L2, alias: lambda_l2\n",
    "         # decrease to deal with overfit\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),   # tree max depth \n",
    "         # decrease to deal with overfit\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 4, 128),  # Max number of leaves in one tree\n",
    "                                                                # should be ~ 2**(max_depth-1)\n",
    "        'subsample': None, # Randomly select part of data without \n",
    "                                  # resampling if subsample < 1.0\n",
    "                                  # alias: bagging_fraction\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 0.7), # Randomly select a subset of features \n",
    "                                                                   # if colsample_bytree < 1.0\n",
    "                                                                   # alias:feature_fraction\n",
    "        # decrease to deal with overfit\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100), # Minimal number of data in one leaf\n",
    "                                                                             # aliases: min_data_in_leaf, \n",
    "        # increase for accuracy, decrease to deal with overfit\n",
    "        'max_bin': trial.suggest_int('max_bin', 32, 255), # Max number of bins that feature values will be bucketed in\n",
    "        # increase to deal with overfit\n",
    "        'subsample_freq': trial.suggest_int('subsample_freq', 1, 7), # Perform bagging at every k iteration, alias: bagging_freq\n",
    "\n",
    "#           'subsample_for_bin': 200000, # Number of data that sampled to construct feature discrete bins; setting this \n",
    "                                     # to larger value will give better training result but may increase train time \n",
    "#           'cat_smooth': trial.suggest_float('cat_smooth', 10.0, 100.0),  # this can reduce the effect of noises in \n",
    "                                                                       # categorical features, especially for \n",
    "                                                                       # categories with few data\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    if not CFG.oversample and not CFG.undersample:\n",
    "        params['is_unbalance'] = True\n",
    "        params['class_weight'] = 'balanced'\n",
    "    else:\n",
    "        params['scale_pos_weight'] = class_imbalance\n",
    "    \n",
    "    if params['boosting_type'] != 'goss':\n",
    "        params['subsample'] = trial.suggest_float('subsample', 0.3, 0.7)\n",
    "    \n",
    "    return optimize_model(params, how='lgbm')\n",
    "            \n",
    "\n",
    "if CFG.lgbm_optimize:\n",
    "#     study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=100), direction=\"minimize\")\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=CFG.n_trials)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    df = study.trials_dataframe()\n",
    "    df['n_estimators'] = best_iterations\n",
    "    df = df.sort_values('value')\n",
    "    df.to_csv(f'optuna_lgbm.csv')\n",
    "\n",
    "    display(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
