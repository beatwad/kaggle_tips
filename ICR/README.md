# ICR Kaggle Competition

## TODO list
* âœ… Run and train ***TabPFN*** model **-->** *Increased* LB position and *decreased* log_loss in during training
* ğŸŸ¦ Calculate and research cosine or different distances between objects
* ğŸŸ¦ Add to features distance feature (cosine, Euclidean etc)
* ğŸŸ¦ Add post-processing based on additional target data
* ğŸŸ¦ Split binary prediction into multi-label
* ğŸŸ¦ Balance class samples (undersampling, post-processing, SMOTE)


âœ… - Done <br>
ğŸŸ¦ - Planning <br>
âº - In progress <br>
â¸ - Put on hold <br>
âŒ - Canceled <br>
âš ï¸ - Check it out <br>

## Ideas
* ğŸŸ¦ Yo Daug I've heard you like boosting, so let's predict with TabPFN/KNN and boost it's errors with boosting

## Experiments
* Simple *LGBM* with no fine-tuning: **LB 0.44**
* Fine-tuned *LGBM* by AutoML: **LB 0.31**
* *TabPFN*: **LB: 0.26** 

## Resources:
* https://arxiv.org/abs/2207.01848
* https://arxiv.org/abs/2211.02941
* https://github.com/automl/TabPFN
