{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e36599c1",
   "metadata": {},
   "source": [
    "# Optuna for LightGBM\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd90382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa7ef62",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle(\"checkpoint_final.pkl\")\n",
    "# Downcast the float columns to reduce RAM usage\n",
    "floatcols = [c for c in matrix.columns if matrix[c].dtype==\"float32\"]\n",
    "matrix[floatcols] = matrix[floatcols].astype(\"float16\")\n",
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)\n",
    "keep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\n",
    "val_month = 33\n",
    "test_month = 34\n",
    "\n",
    "dropcols = [\n",
    "    \"shop_id\",\n",
    "    \"item_id\",\n",
    "    \"new_item\",\n",
    "]  # The features are dropped to reduce overfitting\n",
    "\n",
    "categoricals = [\n",
    "    \"item_category_id\",\n",
    "    \"month\",\n",
    "]\n",
    "matrix[categoricals] = matrix[categoricals].astype(\"category\") \n",
    "train = matrix.drop(columns=dropcols).loc[matrix.date_block_num < val_month, :]\n",
    "train = train[train.date_block_num >= keep_from_month]\n",
    "val = matrix.drop(columns=dropcols).loc[matrix.date_block_num == val_month, :]\n",
    "test = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, :]\n",
    "\n",
    "X_train = train.drop(columns=\"item_cnt_month\")\n",
    "y_train = train.item_cnt_month\n",
    "X_val = val.drop(columns=\"item_cnt_month\")\n",
    "y_val = val.item_cnt_month\n",
    "X_test = test.drop(columns=\"item_cnt_month\")\n",
    "\n",
    "del(matrix, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d03f4",
   "metadata": {},
   "source": [
    "For Better Accuracy:\n",
    "\n",
    "- Use large max_bin (may be slower)\n",
    "\n",
    "- Use small learning_rate with large num_iterations\n",
    "\n",
    "- Use large num_leaves (may cause over-fitting)\n",
    "\n",
    "- Use bigger training data\n",
    "\n",
    "- Try dart\n",
    "\n",
    "Deal with Over-fitting:\n",
    "\n",
    "- Use small max_bin\n",
    "\n",
    "- Use small num_leaves\n",
    "\n",
    "- Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "\n",
    "- Use bagging by set bagging_fraction and bagging_freq\n",
    "\n",
    "- Use feature sub-sampling by set feature_fraction\n",
    "\n",
    "- Use bigger training data\n",
    "\n",
    "- Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n",
    "\n",
    "- Try max_depth to avoid growing deep tree\n",
    "\n",
    "- Try extra_trees\n",
    "\n",
    "- Try increasing path_smooth\n",
    "\n",
    "# Optuna + CV + custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 2\n",
    "    n_folds = 2\n",
    "    num_boost_round = 10000\n",
    "    seeds = [1, 42, 228, 265, 21, 8081988, 5062023, 666, 1488]\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability ùëù is replaced with max(min(ùëù,1‚àí10‚àí15),10‚àí15)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1 - y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def bll_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def calc_log_loss_weight(y_true): \n",
    "    '''w0, w1 assign different weights to individual data points during training.'''\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1\n",
    "\n",
    "def lgbm_opt(features, boosting_type, n_trials):\n",
    "    print('fdafa')\n",
    "    X, y = train_df[features], train_df.Class\n",
    "#     X, y = generated_features_train, train_df.Class\n",
    "    \n",
    "    def objective(trial):\n",
    "        bll_list = list()\n",
    "        \n",
    "        for i in range(CFG.n_repeats):\n",
    "            print(f'Repeat {blu}#{i+1}')\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=8062023+i)\n",
    "\n",
    "            # Stratify based on Class and Alpha (3 types of conditions)\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(X=train_df[features], y=greeks.iloc[:,1:3]), start = 1): \n",
    "\n",
    "                # Split the dataset according to the fold indexes.\n",
    "                X_train = X.iloc[train_idx]\n",
    "                X_val = X.iloc[val_idx]\n",
    "                y_train = y.iloc[train_idx]\n",
    "                y_val = y.iloc[val_idx]\n",
    "\n",
    "                dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "                dvalid = lgb.Dataset(X_val, label=y_val)\n",
    "                \n",
    "                param = {\n",
    "#                     'device': 'gpu',\n",
    "#                     'gpu_platform_id': 0,\n",
    "#                     'gpu_device_id': 0,\n",
    "                    'objective': 'binary',\n",
    "                    'metric': 'none',\n",
    "                    'is_unbalance': True,\n",
    "                    'early_stopping_round' : 50, \n",
    "#                     'verbosity': 0,\n",
    "                    'boosting_type': boosting_type, # trial.suggest_categorical('boosting_type', ['goss']),\n",
    "                    'force_col_wise': False, # Use only with CPU devices\n",
    "\n",
    "                    'subsample_for_bin': 300000, # Number of data that sampled to construct feature discrete bins; setting this \n",
    "                                                 # to larger value will give better training result but may increase train time\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 300, 1000),      \n",
    "                    'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 3e-1),\n",
    "                    'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "                    'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "                    'num_leaves': trial.suggest_int('num_leaves', 2, 256), # Max number of leaves in one tree\n",
    "                    'max_bin': trial.suggest_int('max_bin', 32, 255), # Max number of bins that feature values will be \n",
    "                                                                       # bucketed in. small number of bins may reduce training \n",
    "                                                                       # accuracy but may deal with overfitting\n",
    "                    'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0), # Randomly select a subset of features \n",
    "                                                                                           # if feature_fraction < 1.0\n",
    "                    'bagging_fraction': None, # Randomly select part of data without \n",
    "#                                             # resampling if bagging_fraction < 1.0\n",
    "                    'bagging_freq': trial.suggest_int('bagging_freq', 1, 7), # Perform bagging at every k iteration\n",
    "                    'min_data_in_leaf': trial.suggest_int('min_child_samples', 5, 100), # Minimal number of data in one leaf\n",
    "                                                                                        # aliases: min_child_samples, \n",
    "                    'min_sum_hessian_in_leaf': trial.suggest_float('min_sum_hessian_in_leaf', 1e-4, 1e-1), # Stop trying to split \n",
    "                                                                                                           # leave if sum of it's\n",
    "                                                                                                           # hessian less than k\n",
    "#                     'cat_smooth': trial.suggest_float('cat_smooth', 10.0, 100.0), # this can reduce the effect of noises in \n",
    "#                                                                                   # categorical features, especially for \n",
    "#                                                                                   # categories with few data\n",
    "                    'verbose': -1\n",
    "                }\n",
    "                \n",
    "                if boosting_type != 'goss':\n",
    "                    param['bagging_fraction'] = trial.suggest_float('bagging_fraction', 0.4, 1.0)\n",
    "                \n",
    "                # Add a callback for pruning.\n",
    "                pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'balanced_log_loss')\n",
    "                gbm = lgb.train(\n",
    "                    param, dtrain, valid_sets=[dvalid], verbose_eval=100, callbacks=[pruning_callback], \n",
    "                    feval=bll_metric\n",
    "                )\n",
    "\n",
    "                preds = gbm.predict(X_val)\n",
    "                bll = balanced_log_loss(y_val, preds)\n",
    "                bll_list.append(bll)\n",
    "                \n",
    "        return np.mean(bll_list)\n",
    "            \n",
    "    study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    df = study.trials_dataframe().sort_values('value')\n",
    "    df.to_csv(f'optuna_lgbm_{boosting_type}_fold_{fold}.csv')\n",
    "            \n",
    "for bt in ['goss', 'gbdt', 'dart']:\n",
    "    if bt == 'dart':\n",
    "        lgbm_opt(features, bt, n_trials=1000)\n",
    "    else:\n",
    "        lgbm_opt(features, bt, n_trials=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585cb63c",
   "metadata": {},
   "source": [
    "Create a dataframe from the study and select columns with neccessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study.trials_dataframe()\n",
    "df.sort_values('value')# .iloc[:, [1] + list(range(5, 14))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
