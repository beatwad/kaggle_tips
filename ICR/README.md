# ICR Kaggle Competition
Each version of model must be tagged with v.A.B.C.D system version: A - feature ver., B - model ver,. C - validation ver, D - inference ver.

## TODO list
* âœ… Run and train ***TabPFN*** model **-->** *Increased* LB position and *decreased* log_loss in during training
* âœ… Nested CV
* âœ… Balance class samples (undersampling, post-processing, SMOTE)
* âœ… Data leakage exploit
* âœ… Filter original features (gain importance + permutation importance + BORUTA SHAP)
* âœ… Analyze time features
* âœ… Should we delete objects with the outliers? Or cap the outliers values? (Use IsolationForest to detect outliers)
* âœ… Ensemble 5-10 optimised with Optuna LGBM, CatBoost and XGBoost models, train all of them on 10-20 Fold data
* âœ… Analyze features with high correlation, should we drop some of them?
* âœ… Encode Epsilon as ordinal, encode test as max(Epsilon) + 1 - doesn't work, LB score gets significantly worse
* âœ… Try RandomUnderSampling - doesn't work, makes only worse
* âœ… Feature engineering (log, square, sqrt, plus, minus) - doesn't work
* âœ… Delete objects on which models make mistakes - CV improves significantly, LB - decreases
* âœ… Try to drop DA - CV improves, LB slightly decreases
* âœ… Train each model on individual KFold split
* âœ… Ensemble TabPFNs with different number of ensembles in settings - doesn't work
* âœ… Try to do different RandomUnderSample for every model
* âœ… TabPFN, as all transformers, may be sensitive to uninformative features. And this dataset has a lot of them. Try to drop features one-by-one and check if TabPFN performance increases
* âœ… Delete outliers using AdjustedScaler from adjdatatools (works good)
* âœ… Try KNN-imputing. Or don't use imputation at all?
* âœ… Add post-processing based on additional target data (doesn't work)
* âº Try regularization for LogReg
* âº Try to set different class weights on inference, and check if score is changed. If it is - we can fit our models to this class distribution
* âº Try multi-label classification
* âº Very interesting FE, should try: https://www.kaggle.com/code/tatudoug/logistic-regression-baseline
* ğŸŸ¦ Try different number of folds for LogReg
* ğŸŸ¦ Try different number of models for LogReg
* ğŸŸ¦ Try to train on full dataset with different n_estimators number
* ğŸŸ¦ Group by first and last letter of feature name, try to find dependencies between group name/mean/mode/median/min/max/std/nunique/count and target
* ğŸŸ¦ Ensemble TabPFN with LGBM, CatBoost, XGBoost, KNN, LinReg and AutoGluon
* ğŸŸ¦ Add DNN, optimize it's architechture
* ğŸŸ¦ Add TabNet




âœ… - Done <br>
ğŸŸ¦ - Planning <br>
âº - In progress <br>
â¸ - Put on hold <br>
âŒ - Canceled <br>
âš ï¸ - Check it out <br>

## Ideas
* ğŸŸ¦ Post-processing: DO NOT use code like test[test['class_0'] < 0.13] = 0 or test[test['class_0'] > 0.87] = 1. 
     One false classified object gives an error = -log(1e-15) = 34.53 => log-loss increases dramtically! So if use
     post-processing, use test[test['class_0'] < 0.13] = 0.1/0.01 or test[test['class_0'] > 0.87] = 0.9/0.99


## Experiments
* Simple *LGBM* with no fine-tuning: **LB 0.44**
* Fine-tuned *LGBM* by AutoML: **LB 0.31**
* *TabPFN*: **LB: 0.26** 
* Fine-tuned *LGBM* + Nested CV: **CV/LB: 0.2/0.17** 
* Fine-tuned with Optuna 20 *LGBM* + *XGBoost* + *CatBoost* + Stacking (10 Folds): **CV/LB: 0.158/0.17** 
* Fine-tuned *LGBM* 20 models avg + Feature selection: **CV/LB: 0.166/0.17** 
* Fine-tuned *LGBM* 20 models avg + Feature selection (also drop DA): **CV/LB: 0.163/0.18** 

## Resources:
* https://arxiv.org/abs/2207.01848
* https://arxiv.org/abs/2211.02941
* https://github.com/automl/TabPFN
* https://www.kaggle.com/code/beatwad/dealing-with-very-small-datasets/edit
* https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html


