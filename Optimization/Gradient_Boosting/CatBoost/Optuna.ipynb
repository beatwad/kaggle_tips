{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d96c41",
   "metadata": {},
   "source": [
    "# Optuna for CatBoostRegressor\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import optuna\n",
    "import catboost as cat\n",
    "from catboost import Pool\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476c596",
   "metadata": {},
   "source": [
    "# Optuna + CV + custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    n_repeats = 2\n",
    "    n_folds = 2\n",
    "    num_boost_round = 10000\n",
    "    seeds = [1, 42, 228, 265, 21, 8081988, 5062023, 666, 1488]\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "    \n",
    "    # In order to avoid the extremes of the log function, each predicted probability ùëù is replaced with max(min(ùëù,1‚àí10‚àí15),10‚àí15)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1 - y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def catboost_opt(features, n_trials):\n",
    "    X, y = train_df[features], train_df.Class\n",
    "    \n",
    "    def objective(trial):\n",
    "        bll_list = list()\n",
    "        \n",
    "        # Parameters\n",
    "        params = {\n",
    "            'auto_class_weights': 'Balanced',\n",
    "            'task_type': 'GPU',\n",
    "            'eval_metric': 'Logloss',\n",
    "            'loss_function': 'Logloss', \n",
    "            'random_seed': 10062023,\n",
    "            'od_type': 'Iter', # Type of overfitting detector - stop after k iteraions\n",
    "            'od_wait': 100, # Overfitting detector - stop training after k iterations without metric improvement\n",
    "            'metric_period': 100, # Show metric each k iterations\n",
    "            'grow_policy':trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']), \n",
    "             # Hyperparamters (in order of importance decreasing)\n",
    "            'iterations' : 3000, # trial.suggest_int('iterations', 300, 1200),        \n",
    "            'learning_rate' : trial.suggest_loguniform('learning_rate', 1e-3, 3e-1),    \n",
    "            'l2_leaf_reg': trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 100),\n",
    "             # decrease to deal with overfit\n",
    "            'depth' : trial.suggest_int('depth', 4, 10),  # Max tree depth         \n",
    "             # decrease to deal with overfit\n",
    "            'max_leaves': trial.suggest_int('num_leaves', 4, 128),  # Max number of leaves in one tree\n",
    "            'subsample': trial.suggest_float('subsample', 0.3, 0.7) # randomly select part of data without return\n",
    "            'colsample_bylevel': trial.suggest_float('subsample', 0.3, 0.7) # the percentage of features to use at each \n",
    "                                                                            # split selection\n",
    "                                                                            # alias: rsm\n",
    "             # increase to deal with overfit\n",
    "            'random_strength': trial.suggest_int('random_strength', 0, 100), # The amount of randomness to use \n",
    "                                                                             # for scoring splits when the tree structure\n",
    "                                                                             # is selected. Helps to avoid overfitting\n",
    "            'bagging_temperature' : trial.suggest_loguniform('bagging_temperature', 0, 100),       # Assigns random \n",
    "                                                                                                   # weights to objects\n",
    "            # this feature value can be increased to 1024 for important features:\n",
    "            # per_float_feature_quantization='0:border_count=1024'\n",
    "            'border_count': trial.suggest_categorical('border_count', 254), # The number of splits for numerical features\n",
    "                                                                            # bigger is better but slowly\n",
    "                                                                            # alias: max_bin\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 5, 100), # Minimal number of data in one leaf\n",
    "                                                                               # aliases: min_child_samples, \n",
    "        }\n",
    "        if params['grow_policy'] == 'SymmetricTree': \n",
    "            params['boosting_type']= trial.suggest_categorical('boosting_type', ['Ordered', 'Plain'])\n",
    "        else:\n",
    "            params['boosting_type'] = 'Plain'\n",
    "        \n",
    "        for i in range(CFG.n_repeats):\n",
    "            print(f'Repeat {blu}#{i+1}')\n",
    "\n",
    "            # Create an oof array for inner loop\n",
    "            oof = np.zeros(train_df.shape[0])\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=8062023+i)\n",
    "\n",
    "            # Stratify based on Class and Alpha (3 types of conditions)\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(X=train_df[features], y=greeks.iloc[:,1:3]), start = 1): \n",
    "\n",
    "                # Split the dataset according to the fold indexes.\n",
    "                X_train = X.iloc[train_idx]\n",
    "                X_val = X.iloc[val_idx]\n",
    "                y_train = y.iloc[train_idx]\n",
    "                y_val = y.iloc[val_idx]\n",
    "          \n",
    "                train_pool = Pool(X_train, y_train, cat_features=['EJ'])\n",
    "                val_pool = Pool(X_val, y_val, cat_features=['EJ'])\n",
    "\n",
    "                # Learning\n",
    "                model = cat.CatBoostClassifier(**params)     \n",
    "                model.fit(train_pool, eval_set=val_pool)\n",
    "                # Predict\n",
    "                preds = model.predict_proba(val_pool)[:,1]\n",
    "                # Evaluation\n",
    "                bll = balanced_log_loss(y_val, preds)\n",
    "                bll_list.append(bll)\n",
    "                \n",
    "        return np.mean(bll_list)\n",
    "            \n",
    "    study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    df = study.trials_dataframe()\n",
    "    df.sort_values('value').iloc[:, [1] + list(range(5, 14))]\n",
    "    df.to_csv(f'optuna_catboost_fold_.csv')\n",
    "            \n",
    "catboost_opt(features, n_trials=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1647417a",
   "metadata": {},
   "source": [
    "Launch Optuna study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, timeout=3600) # change timeout if you want to make optimization process longer\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f0c29e",
   "metadata": {},
   "source": [
    "Create a dataframe from the study and select columns with neccessary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study.trials_dataframe()\n",
    "df.sort_values('value').iloc[:, [1] + list(range(5, 14))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d03f4",
   "metadata": {},
   "source": [
    "For Better Accuracy:\n",
    "\n",
    "- Use large max_bin (may be slower)\n",
    "\n",
    "- Use small learning_rate with large num_iterations\n",
    "\n",
    "- Use large num_leaves (may cause over-fitting)\n",
    "\n",
    "- Use bigger training data\n",
    "\n",
    "- Try dart\n",
    "\n",
    "Deal with Over-fitting:\n",
    "\n",
    "- Use small max_bin\n",
    "\n",
    "- Use small num_leaves\n",
    "\n",
    "- Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "\n",
    "- Use bagging by set bagging_fraction and bagging_freq\n",
    "\n",
    "- Use feature sub-sampling by set feature_fraction\n",
    "\n",
    "- Use bigger training data\n",
    "\n",
    "- Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n",
    "\n",
    "- Try max_depth to avoid growing deep tree\n",
    "\n",
    "- Try extra_trees\n",
    "\n",
    "- Try increasing path_smooth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
