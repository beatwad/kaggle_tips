{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85eede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_opt(features, booster, n_trials):\n",
    "    X, y = train_df[features], train_df.Class\n",
    "    \n",
    "    def objective(trial):\n",
    "        bll_list = list()\n",
    "        \n",
    "        params = {\n",
    "            \"n_estimators\": 3000, # trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "            \"random_state\": 14062023,\n",
    "            \"early_stopping_rounds\": 100,\n",
    "            \"verbosity\": 0,\n",
    "            # disbalance ratio between 0 and 1 classes\n",
    "            \"scale_pos_weight\": 4.71,\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"eval_metric\": \"logloss\",\n",
    "            # use exact for small dataset.\n",
    "            \"tree_method\": \"exact\",\n",
    "            # defines booster, gblinear for linear functions.\n",
    "            \"booster\": booster, # trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]), \n",
    "            # L1 regularization weight.\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            # L2 regularization weight.\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            # sampling ratio for training data.\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n",
    "            # sampling according to each tree.\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n",
    "        }\n",
    "\n",
    "        if params[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            params[\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True) # alias eta\n",
    "            # maximum depth of the tree, signifies complexity of the tree.\n",
    "            params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 10)\n",
    "            # minimum child weight, larger the term more conservative the tree.\n",
    "            params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            # defines how selective algorithm is.\n",
    "            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "        if params[\"booster\"] == \"dart\":\n",
    "            params[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            params[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            params[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "            params[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "        \n",
    "        for i in range(CFG.n_optimize_repeats):\n",
    "            print(f'{blu}Repeat #{i+1}')\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=CFG.n_optimize_folds, shuffle=True, random_state=8062023+i)\n",
    "\n",
    "            # Stratify based on Class and Alpha (3 types of conditions)\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(X=train_df[features], y=greeks.iloc[:,1:3]), start = 1): \n",
    "\n",
    "                # Split the dataset according to the fold indexes.\n",
    "                X_train = X.iloc[train_idx]\n",
    "                X_val = X.iloc[val_idx]\n",
    "                y_train = y.iloc[train_idx]\n",
    "                y_val = y.iloc[val_idx]\n",
    "\n",
    "                # Learning\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "#                 bst = xgb.train(param, dtrain, num_boost_round=10000,\n",
    "#                                 evals=[(dtrain, 'train'), (dvalid, 'valid')], \n",
    "#                                 verbose_eval=1000, early_stopping_rounds=100) \n",
    "                model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=1000)\n",
    "                # Predict\n",
    "#                 preds = bst.predict(dvalid)\n",
    "                preds = model.predict_proba(X_val)[:,1]\n",
    "                # Evaluation\n",
    "                bll = balanced_log_loss(y_val, preds)\n",
    "                bll_list.append(bll)\n",
    "                \n",
    "        return np.mean(bll_list)\n",
    "            \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    df = study.trials_dataframe()\n",
    "    df.sort_values('value').iloc[:, [1] + list(range(5, 14))]\n",
    "    df.to_csv(f'optuna_xgb.csv')\n",
    "            \n",
    "if CFG.xgb_optimize:\n",
    "    for booster in [\"gbtree\", \"gblinear\"]: # , \"dart\"]:\n",
    "        xgboost_opt(features, booster, n_trials=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
