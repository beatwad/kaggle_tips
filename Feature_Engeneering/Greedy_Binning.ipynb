{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c20c3d4",
   "metadata": {},
   "source": [
    "# Greedy Binning\n",
    "\n",
    "Can be used to prepare data for NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *\n",
    "\n",
    "root = args.root\n",
    "\n",
    "oof = pd.read_csv('./output/LGB_with_series_feature/oof.csv')\n",
    "sub = pd.read_csv('./output/LGB_with_series_feature/submission.csv.zip')\n",
    "\n",
    "def pad_target(x):\n",
    "    t = np.zeros(13)\n",
    "    t[:-len(x)] = np.nan\n",
    "    t[-len(x):] = x\n",
    "    return list(t)\n",
    "\n",
    "tmp1 = oof.groupby('customer_ID',sort=False)['target'].agg(lambda x:pad_target(x))\n",
    "tmp2 = sub.groupby('customer_ID',sort=False)['prediction'].agg(lambda x:pad_target(x))\n",
    "\n",
    "tmp = tmp1.append(tmp2)\n",
    "\n",
    "tmp = pd.DataFrame(data=tmp.tolist(),columns=['target%s'%i for i in range(1,14)])\n",
    "\n",
    "\n",
    "df = []\n",
    "for fn in ['cat','num','diff','rank_num','last3_cat','last3_num','last3_diff', 'last6_num','ym_rank_num']:\n",
    "    if len(df) == 0:\n",
    "        df.append(pd.read_feather(f'{root}/{fn}_feature.feather'))\n",
    "    else:\n",
    "        df.append(pd.read_feather(f'{root}/{fn}_feature.feather').drop([id_name],axis=1))\n",
    "    if 'last' in fn :\n",
    "        df[-1] = df[-1].add_prefix('_'.join(fn.split('_')[:-1])+'_')\n",
    "\n",
    "df.append(tmp)\n",
    "\n",
    "df = pd.concat(df,axis=1)\n",
    "print(df.shape)\n",
    "df.to_feather(f'{root}/all_feature.feather')\n",
    "\n",
    "del df\n",
    "\n",
    "def one_hot_encoding(df,cols,is_drop=True):\n",
    "    for col in cols:\n",
    "        print('one hot encoding:',col)\n",
    "        dummies = pd.get_dummies(pd.Series(df[col]),prefix='oneHot_%s'%col)\n",
    "        df = pd.concat([df,dummies],axis=1)\n",
    "    if is_drop:\n",
    "        df.drop(cols,axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "\n",
    "df = pd.read_feather(f'./input/train.feather').append(pd.read_feather(f'./input/test.feather')).reset_index(drop=True)\n",
    "df = df.drop(['S_2'],axis=1)\n",
    "df = one_hot_encoding(df,cat_features,True)\n",
    "for col in tqdm(df.columns):\n",
    "    if col not in ['customer_ID','S_2']:\n",
    "        df[col] /= 100\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "df.to_feather('./input/nn_series.feather')\n",
    "\n",
    "def GreedyFindBin(distinct_values, counts, num_distinct_values, max_bin, total_cnt, min_data_in_bin=3):\n",
    "# INPUT:\n",
    "#   distinct_values - the array of distinct feature values, the feature values are monotonically increasing\n",
    "#   counts - the number of samples that corresponds to the value of the feature\n",
    "#   num_distinct_values - the number of feature values\n",
    "#   max_bin - max number of buckets\n",
    "#   total_cnt - number of samples\n",
    "#   min_data_in_bin - min number of samples that bucket contains\n",
    "\n",
    "# bin_upper_bound - it's an array of record bucket boundaries\n",
    "    bin_upper_bound=list();\n",
    "    assert(max_bin>0)\n",
    "\n",
    "    if num_distinct_values <= max_bin:\n",
    "        cur_cnt_inbin = 0\n",
    "        for i in range(num_distinct_values-1):\n",
    "            cur_cnt_inbin += counts[i]\n",
    "            # If the value ratio of a feature 'min_data_in_bin' is small, accumulate the next value，\n",
    "            # until than 'min_data_in_bin' is greater\n",
    "            if cur_cnt_inbin >= min_data_in_bin:\n",
    "                # Take the mean of the current value and the next value as the cut-off point of the bucket \n",
    "                # 'bin_upper_bound'\n",
    "                bin_upper_bound.append((distinct_values[i] + distinct_values[i + 1]) / 2.0)\n",
    "                cur_cnt_inbin = 0\n",
    "        # 对于最后一个桶的上界则为无穷大\n",
    "        cur_cnt_inbin += counts[num_distinct_values - 1];\n",
    "        bin_upper_bound.append(float('Inf'))\n",
    "    else:\n",
    "        # The number of feature values is larger than max_bin, \n",
    "        # indicating that several feature values need to share one bin\n",
    "        if min_data_in_bin>0:\n",
    "            max_bin=min(max_bin,total_cnt//min_data_in_bin)\n",
    "            max_bin=max(max_bin,1)\n",
    "        # mean size for one bin\n",
    "        mean_bin_size=total_cnt/max_bin\n",
    "        rest_bin_cnt = max_bin\n",
    "        rest_sample_cnt = total_cnt\n",
    "        # definition of 'is_big_count_value' arrays: Initially set the number \n",
    "        # of each distinct value of the feature to be small（false）\n",
    "        is_big_count_value=[False]*num_distinct_values\n",
    "        # If the number of an eigenvalue is greater than mean_bin_size, then these features need a separate bin\n",
    "        for i in range(num_distinct_values):\n",
    "        # If the number of an eigenvalue is greater than mean_bin_size, \n",
    "        # then set the corresponding eigenvalue is_big_count_value is true\n",
    "            if counts[i] >= mean_bin_size:\n",
    "                is_big_count_value[i] = True\n",
    "                rest_bin_cnt-=1\n",
    "                rest_sample_cnt -= counts[i]\n",
    "        # The number of samples of the remaining feature values is averaged \n",
    "        # for each remaining bin： mean size for one bin\n",
    "        mean_bin_size = rest_sample_cnt/rest_bin_cnt\n",
    "        upper_bounds=[float('Inf')]*max_bin\n",
    "        lower_bounds=[float('Inf')]*max_bin\n",
    "\n",
    "        bin_cnt = 0\n",
    "        lower_bounds[bin_cnt] = distinct_values[0]\n",
    "        cur_cnt_inbin = 0\n",
    "        # Re-traverse all eigenvalues (including large and small numbers)\n",
    "        for i in range(num_distinct_values-1):\n",
    "            # If the current number of eigenvalues is small\n",
    "            if not is_big_count_value[i]:\n",
    "                rest_sample_cnt -= counts[i]\n",
    "            cur_cnt_inbin += counts[i]\n",
    "\n",
    "            # like cur_cnt_in bin if it is too small, accumulate the next value \n",
    "            # until the condition is met and enter the loop.\n",
    "            # need a new bin if the current feature needs to be separated into a bin，\n",
    "            # or the current count of several features exceeds mean_bin_size，\n",
    "            # or the next one needs to be bucketed independently\n",
    "            if is_big_count_value[i] or cur_cnt_inbin >= mean_bin_size or \\\n",
    "            is_big_count_value[i + 1] and cur_cnt_inbin >= max(1.0, mean_bin_size * 0.5):\n",
    "                upper_bounds[bin_cnt] = distinct_values[i]\n",
    "                bin_cnt+=1\n",
    "                lower_bounds[bin_cnt] = distinct_values[i + 1] \n",
    "                if bin_cnt >= max_bin - 1:\n",
    "                    break\n",
    "                cur_cnt_inbin = 0\n",
    "                if not is_big_count_value[i]:\n",
    "                    rest_bin_cnt-=1\n",
    "                    mean_bin_size = rest_sample_cnt / rest_bin_cnt\n",
    "#             bin_cnt+=1\n",
    "        # update bin upper bound compared with the number of feature values max_bin\n",
    "        # a small number of operations are similar, taking the mean of the current value \n",
    "        # and the next value as the dividing point of the bucket\n",
    "        for i in range(bin_cnt-1):\n",
    "            bin_upper_bound.append((upper_bounds[i] + lower_bounds[i + 1]) / 2.0)\n",
    "        bin_upper_bound.append(float('Inf'))\n",
    "    return bin_upper_bound\n",
    "\n",
    "cat_features = [\"B_30\",\"B_38\",\"D_114\",\"D_116\",\"D_117\",\"D_120\",\"D_126\",\"D_63\",\"D_64\",\"D_66\",\"D_68\"]\n",
    "eps = 1e-3\n",
    "\n",
    "dfs = []\n",
    "for fn in ['cat','num','diff','rank_num','last3_cat','last3_num','last3_diff', 'last6_num','ym_rank_num']:\n",
    "    if len(dfs) == 0:\n",
    "        dfs.append(pd.read_feather(f'{root}/{fn}_feature.feather'))\n",
    "    else:\n",
    "        dfs.append(pd.read_feather(f'{root}/{fn}_feature.feather').drop(['customer_ID'],axis=1))\n",
    "\n",
    "    if 'last' in fn:\n",
    "        dfs[-1] = dfs[-1].add_prefix('_'.join(fn.split('_')[:-1])+'_')\n",
    "\n",
    "for df in dfs:\n",
    "    for col in tqdm(df.columns):\n",
    "        if col not in ['customer_ID','S_2']:\n",
    "            # v_min = df[col].min()\n",
    "            # v_max = df[col].max()\n",
    "            # df[col] = (df[col]-v_min+eps) / (v_max-v_min+eps)\n",
    "            vc = df[col].value_counts().sort_index()\n",
    "            bins = GreedyFindBin(vc.index.values,vc.values,len(vc),255,vc.sum())\n",
    "            df[col] = np.digitize(df[col],[-np.inf]+bins)\n",
    "            df.loc[df[col]==len(bins)+1,col] = 0\n",
    "            df[col] = df[col] / df[col].max()\n",
    "\n",
    "tmp = tmp.fillna(0)\n",
    "dfs.append(tmp)\n",
    "df = pd.concat(dfs,axis=1)\n",
    "\n",
    "df.to_feather('./input/nn_all_feature.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
