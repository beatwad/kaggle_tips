{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:42:47.505814Z",
     "iopub.status.busy": "2022-04-15T06:42:47.505449Z",
     "iopub.status.idle": "2022-04-15T06:42:56.453703Z",
     "shell.execute_reply": "2022-04-15T06:42:56.452792Z",
     "shell.execute_reply.started": "2022-04-15T06:42:47.505756Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/scikit/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016475,
     "end_time": "2022-01-25T15:39:01.467349",
     "exception": false,
     "start_time": "2022-01-25T15:39:01.450874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  UMP TF-Record: CombinatorialPurgedGroupKFold\n",
    "\n",
    "In this notebook, I am going to create TF-Record for UMP dataset using CombinatorialPurgedGroupKFold CV strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-04-15T06:42:56.457642Z",
     "iopub.status.busy": "2022-04-15T06:42:56.457356Z",
     "iopub.status.idle": "2022-04-15T06:43:01.712518Z",
     "shell.execute_reply": "2022-04-15T06:43:01.711549Z",
     "shell.execute_reply.started": "2022-04-15T06:42:56.457607Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "class CombinatorialPurgedGroupKFold():\n",
    "    def __init__(self, n_splits = 6, n_test_splits = 2, purge = 1, pctEmbargo = 0.01, **kwargs):\n",
    "        self.n_splits = n_splits\n",
    "        self.n_test_splits = n_test_splits\n",
    "        self.purge = purge\n",
    "        self.pctEmbargo = pctEmbargo\n",
    "        \n",
    "    def split(self, X, y = None, groups = None):\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "            \n",
    "        u, ind = np.unique(groups, return_index = True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_groups = len(unique_groups)\n",
    "        group_dict = {}\n",
    "        for idx in range(len(X)):\n",
    "            if groups[idx] in group_dict:\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "                \n",
    "        n_folds = comb(self.n_splits, self.n_test_splits, exact = True)\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "            \n",
    "        mbrg = int(n_groups * self.pctEmbargo)\n",
    "        if mbrg < 0:\n",
    "            raise ValueError(\n",
    "                \"The number of 'embargoed' groups should not be negative\")\n",
    "        \n",
    "        split_dict = {}\n",
    "        group_test_size = n_groups // self.n_splits\n",
    "        for split in range(self.n_splits):\n",
    "            if split == self.n_splits - 1:\n",
    "                split_dict[split] = unique_groups[int(split * group_test_size):].tolist()\n",
    "            else:\n",
    "                split_dict[split] = unique_groups[int(split * group_test_size):int((split + 1) * group_test_size)].tolist()\n",
    "        \n",
    "        for test_splits in combinations(range(self.n_splits), self.n_test_splits):\n",
    "            test_groups = []\n",
    "            banned_groups = []\n",
    "            for split in test_splits:\n",
    "                test_groups += split_dict[split]\n",
    "                banned_groups += unique_groups[split_dict[split][0] - self.purge:split_dict[split][0]].tolist()\n",
    "                banned_groups += unique_groups[split_dict[split][-1] + 1:split_dict[split][-1] + self.purge + mbrg + 1].tolist()\n",
    "            train_groups = [i for i in unique_groups if (i not in banned_groups) and (i not in test_groups)]\n",
    "\n",
    "            train_idx = []\n",
    "            test_idx = []\n",
    "            for train_group in train_groups:\n",
    "                train_idx += group_dict[train_group]\n",
    "            for test_group in test_groups:\n",
    "                test_idx += group_dict[test_group]\n",
    "            yield train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a small sample data to understand this CV strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:43:01.714366Z",
     "iopub.status.busy": "2022-04-15T06:43:01.714059Z",
     "iopub.status.idle": "2022-04-15T06:43:01.745753Z",
     "shell.execute_reply": "2022-04-15T06:43:01.744541Z",
     "shell.execute_reply.started": "2022-04-15T06:43:01.714326Z"
    }
   },
   "outputs": [],
   "source": [
    "n_splits = 6\n",
    "n_test_splits = 1\n",
    "elements = list(range(10 * (n_splits + n_test_splits)))\n",
    "groups = [element // n_splits for element in elements]\n",
    "data = pd.DataFrame({\"group\": groups, \"element\": elements})\n",
    "kfold = CombinatorialPurgedGroupKFold(n_splits, n_test_splits)\n",
    "\n",
    "for index, (train_indices, test_indices) in enumerate(kfold.split(data, groups=data[\"group\"])):\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Fold {index}\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Train indices:\", train_indices, \"Length:\", len(train_indices))\n",
    "    print(\"Test Indices:\", test_indices, \"Length:\", len(test_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015291,
     "end_time": "2022-01-25T15:39:09.296817",
     "exception": false,
     "start_time": "2022-01-25T15:39:09.281526",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:43:01.748064Z",
     "iopub.status.busy": "2022-04-15T06:43:01.747701Z",
     "iopub.status.idle": "2022-04-15T06:43:18.068755Z",
     "shell.execute_reply": "2022-04-15T06:43:18.067831Z",
     "shell.execute_reply.started": "2022-04-15T06:43:01.748018Z"
    },
    "papermill": {
     "duration": 16.88418,
     "end_time": "2022-01-25T15:39:26.19638",
     "exception": false,
     "start_time": "2022-01-25T15:39:09.3122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_features = 300\n",
    "features = [f'f_{i}' for i in range(n_features)]\n",
    "train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:43:18.072068Z",
     "iopub.status.busy": "2022-04-15T06:43:18.071731Z",
     "iopub.status.idle": "2022-04-15T06:43:24.003229Z",
     "shell.execute_reply": "2022-04-15T06:43:24.002266Z",
     "shell.execute_reply.started": "2022-04-15T06:43:18.072025Z"
    }
   },
   "outputs": [],
   "source": [
    "test_time_id_len = 942961\n",
    "\n",
    "calendar_df = pd.read_csv(\"../input/chinese-holidays/holidays_of_china_from_2014_to_2030.csv\", parse_dates=[\"date\"], date_parser=lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "\n",
    "# leave only national holidays\n",
    "calendar_df = calendar_df.loc[(calendar_df.type.isin([\"National holiday\", \"Common local holiday\"]))]\n",
    "\n",
    "# fill with everyday from 2014 to 2022\n",
    "calendar_df = (\n",
    "    pd.DataFrame({\"date\": pd.date_range(start=\"2014-01-01\", end=\"2023-01-01\")}).merge(calendar_df, on=\"date\", how=\"left\")\n",
    "    .assign(weekday=lambda x: x.date.dt.day_name(), year=lambda x: x.date.dt.year)\n",
    ")\n",
    "\n",
    "# remove weekends and national holidays and align with time_id\n",
    "calendar_df = (\n",
    "    calendar_df.loc[(~calendar_df.weekday.isin([\"Sunday\", \"Saturday\"]))&(calendar_df.name.isna())]\n",
    "    .reset_index(drop=True)\n",
    "    .head(train.time_id.max()+1)\n",
    "    .dropna(axis=1)\n",
    ")\n",
    "\n",
    "calendar_df['quarter'] = calendar_df['date'].dt.to_period('Q')\n",
    "calendar_df['time_id'] = calendar_df.index\n",
    "\n",
    "le = LabelEncoder()\n",
    "calendar_df['quarter'] = le.fit_transform(calendar_df['quarter'])\n",
    "\n",
    "train = train.merge(calendar_df[['time_id', 'year', 'quarter']], how='left', on='time_id')\n",
    "\n",
    "time_id = train.pop(\"time_id\")\n",
    "quarter = train.pop(\"quarter\")\n",
    "year = train.pop(\"year\")\n",
    "\n",
    "del calendar_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set cluster_id feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:43:24.005434Z",
     "iopub.status.busy": "2022-04-15T06:43:24.005101Z",
     "iopub.status.idle": "2022-04-15T06:43:29.767112Z",
     "shell.execute_reply": "2022-04-15T06:43:29.766137Z",
     "shell.execute_reply.started": "2022-04-15T06:43:24.005390Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "inv_id_to_cluster = pd.read_pickle('../input/part-1-1-ubiquant-clustering/clustered_inv_index.pkl')\n",
    "inv_id_to_cluster.index.name = 'investment_id'\n",
    "\n",
    "train = train.merge(inv_id_to_cluster, how='left', on='investment_id')\n",
    "train.cluster.fillna(0, inplace=True)\n",
    "train.cluster = train.cluster.astype(np.uint16)\n",
    "\n",
    "cluster = train.pop('cluster')\n",
    "num_clusters = cluster.unique().shape[0] - 1\n",
    "cluster = pd.get_dummies(cluster, drop_first=True, prefix='cluster_')\n",
    "\n",
    "# Sorted groups 'investment_id' by 'cluster_id': \n",
    "# investment_id_cluster_dict = dict()\n",
    "# for unique in cluster_id_feature.unique():\n",
    "#     investment_id_cluster_dict.update({unique: inv_id_to_cluster[inv_id_to_cluster['cluster'] == unique].index.to_numpy()})\n",
    "    \n",
    "# Hot Encode cluster feature\n",
    "# cluster_id_feature = tf.keras.utils.to_categorical(cluster_id_feature, num_classes=num_classes, dtype='uint8')\n",
    "    \n",
    "del inv_id_to_cluster\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set A-shares sub section feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:43:29.770476Z",
     "iopub.status.busy": "2022-04-15T06:43:29.770238Z",
     "iopub.status.idle": "2022-04-15T06:43:37.604886Z",
     "shell.execute_reply": "2022-04-15T06:43:37.603790Z",
     "shell.execute_reply.started": "2022-04-15T06:43:29.770447Z"
    }
   },
   "outputs": [],
   "source": [
    "map_info = pd.read_csv('../input/ubiquant-a-shares/map_info.csv')[['Sub_Section', 'Main_Section', 'Market_Value', 'investment_id']]\n",
    "map_info.rename({'Sub_Section': 'sub_section', 'Main_Section': 'main_section', 'Market_Value': 'market_value'}, axis=1, inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "map_info['sub_section'] = le.fit_transform(map_info['sub_section'])\n",
    "map_info['sub_section'] += 1\n",
    "map_info['main_section'] += 1\n",
    "\n",
    "train = train.merge(map_info, how='left', on='investment_id')\n",
    "train[['main_section', 'sub_section']] = train[['main_section', 'sub_section']].fillna(0)\n",
    "train[['main_section', 'investment_id', 'sub_section']] = train[['main_section', 'investment_id', 'sub_section']].astype(np.uint16)\n",
    "train['market_value'] = train['market_value'].fillna(train['market_value'].mean())\n",
    "train[['market_value']] = train[['market_value']].astype(np.uint64)\n",
    "\n",
    "sub_section = train.pop('sub_section')\n",
    "num_sub_sections = sub_section.unique().shape[0]\n",
    "sub_section = pd.get_dummies(sub_section)\n",
    "\n",
    "main_section = train.pop('main_section')\n",
    "num_main_sections = main_section.unique().shape[0] - 1\n",
    "main_section = pd.get_dummies(main_section, drop_first=True)\n",
    "\n",
    "market_value = train.pop('market_value')\n",
    "\n",
    "# # Sorted groups 'investment_id' by 'cluster_id': \n",
    "# investment_id_sub_section_dict = dict()\n",
    "# for unique in sub_section_feature.unique():\n",
    "#     investment_id_sub_section_dict.update({unique: map_info[map_info['sub_section'] == unique].index.to_numpy()})\n",
    "    \n",
    "# Hot Encode sub section feature\n",
    "# sub_section_feature = tf.keras.utils.to_categorical(sub_section_feature, num_classes=num_sections, dtype='uint8')\n",
    "\n",
    "del map_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unnecessary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:43:37.606831Z",
     "iopub.status.busy": "2022-04-15T06:43:37.606458Z",
     "iopub.status.idle": "2022-04-15T06:43:37.612974Z",
     "shell.execute_reply": "2022-04-15T06:43:37.611851Z",
     "shell.execute_reply.started": "2022-04-15T06:43:37.606757Z"
    },
    "papermill": {
     "duration": 0.041856,
     "end_time": "2022-01-25T15:39:26.254473",
     "exception": false,
     "start_time": "2022-01-25T15:39:26.212617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "investment_id = train.pop(\"investment_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:43:37.615106Z",
     "iopub.status.busy": "2022-04-15T06:43:37.614733Z",
     "iopub.status.idle": "2022-04-15T06:43:37.630180Z",
     "shell.execute_reply": "2022-04-15T06:43:37.628745Z",
     "shell.execute_reply.started": "2022-04-15T06:43:37.615059Z"
    },
    "papermill": {
     "duration": 0.04224,
     "end_time": "2022-01-25T15:39:26.375506",
     "exception": false,
     "start_time": "2022-01-25T15:39:26.333266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = train.pop(\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017564,
     "end_time": "2022-01-25T15:39:30.64918",
     "exception": false,
     "start_time": "2022-01-25T15:39:30.631616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create TF-Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:43:37.631883Z",
     "iopub.status.busy": "2022-04-15T06:43:37.631576Z",
     "iopub.status.idle": "2022-04-15T06:43:37.652368Z",
     "shell.execute_reply": "2022-04-15T06:43:37.650873Z",
     "shell.execute_reply.started": "2022-04-15T06:43:37.631838Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_record(i):\n",
    "    dic = {}\n",
    "    dic[\"features\"] = tf.train.Feature(float_list=tf.train.FloatList(value=list(train.iloc[i])))\n",
    "    dic[\"time_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[time_id.iloc[i]]))\n",
    "    dic[\"year\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[year.iloc[i]]))\n",
    "    dic[\"quarter\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[quarter.iloc[i]]))\n",
    "    dic[\"cluster\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(cluster.iloc[i])))\n",
    "    dic[\"sub_section\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(sub_section.iloc[i])))\n",
    "    dic[\"main_section\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(main_section.iloc[i])))\n",
    "    dic[\"market_value\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[market_value.iloc[i]]))\n",
    "    dic[\"target\"] = tf.train.Feature(float_list=tf.train.FloatList(value=[y.iloc[i]]))\n",
    "    record_bytes = tf.train.Example(features=tf.train.Features(feature=dic)).SerializeToString()\n",
    "    return record_bytes\n",
    "    \n",
    "def decode_function(record_bytes):\n",
    "      return tf.io.parse_single_example(\n",
    "      # Data\n",
    "      record_bytes,\n",
    "      # Schema\n",
    "      {\n",
    "          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n",
    "          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"year\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"quarter\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"cluster\": tf.io.FixedLenFeature([num_clusters], dtype=tf.int64), \n",
    "          \"sub_section\": tf.io.FixedLenFeature([num_sub_sections], dtype=tf.int64),\n",
    "          \"main_section\": tf.io.FixedLenFeature([num_main_sections], dtype=tf.int64),\n",
    "          \"market_value\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n",
    "      }\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the whole dataset, it will take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:44:07.580462Z",
     "iopub.status.busy": "2022-04-15T06:44:07.579174Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "n_splits = 5\n",
    "n_test_splits = 1\n",
    "# kfold = CombinatorialPurgedGroupKFold(n_splits, n_test_splits)\n",
    "kfold = StratifiedGroupKFold(n_splits)\n",
    "for fold, (train_indices, test_indices) in enumerate(kfold.split(train, investment_id, groups=time_id)):\n",
    "    if fold != 4:\n",
    "        continue\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Fold {fold}\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Train Sample size:\", len(train_indices))\n",
    "    print(\"Test Sample size:\", len(test_indices))\n",
    "    train_save_path = f\"fold_{fold}_train.tfrecords\"\n",
    "    begin = time.time()\n",
    "    print(f\"Creating {train_save_path}\")\n",
    "    with tf.io.TFRecordWriter(train_save_path) as file_writer:\n",
    "        for i in train_indices:\n",
    "            file_writer.write(create_record(i))\n",
    "    print(\"Elapsed time: %.2f\"%(time.time() - begin))\n",
    "    test_save_path = f\"fold_{fold}_test.tfrecords\"\n",
    "    begin = time.time()\n",
    "    print(f\"Creating {test_save_path}\")\n",
    "    with tf.io.TFRecordWriter(test_save_path) as file_writer:\n",
    "        for i in test_indices:\n",
    "            file_writer.write(create_record(i))\n",
    "    print(\"Elapsed time: %.2f\"%(time.time() - begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write unique Investment Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-15T06:43:38.018635Z",
     "iopub.status.busy": "2022-04-15T06:43:38.017883Z",
     "iopub.status.idle": "2022-04-15T06:43:38.052465Z",
     "shell.execute_reply": "2022-04-15T06:43:38.051710Z",
     "shell.execute_reply.started": "2022-04-15T06:43:38.018584Z"
    }
   },
   "outputs": [],
   "source": [
    "investment_ids = investment_id.unique()\n",
    "investment_id_df = pd.DataFrame({\"investment_id\": investment_ids})\n",
    "investment_id_df.to_csv(\"investment_ids.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
