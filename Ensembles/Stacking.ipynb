{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c17dc1f5",
   "metadata": {},
   "source": [
    "## LGBM load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "if CFG.kaggle:\n",
    "    param_list = glob.glob(\"/kaggle/input/icr-lgbm-optuna/optuna_lgbm.csv\")\n",
    "else:\n",
    "    param_list = glob.glob(\"optuna_lgbm.csv\")\n",
    "\n",
    "models = list()\n",
    "best_lgbm_params = list()\n",
    "\n",
    "lgbm_params = pd.DataFrame()\n",
    "\n",
    "for f in param_list:\n",
    "    tmp = pd.read_csv(f, index_col='Unnamed: 0')\n",
    "    if lgbm_params.shape[0] == 0:\n",
    "        lgbm_params = tmp\n",
    "    else:\n",
    "        lgbm_params = pd.concat([lgbm_params, tmp])\n",
    "        \n",
    "lgbm_params = lgbm_params.sort_values('value').head(CFG.n_stacking_models)\n",
    "param_cols = [c for c in lgbm_params.columns if c.startswith('params_')]\n",
    "lgbm_params = lgbm_params[param_cols]\n",
    "\n",
    "for idx, row in lgbm_params.iterrows():\n",
    "    row_dict = {k[7:]: v for k, v in row.items()}\n",
    "    row_dict['objective'] = 'binary'\n",
    "    row_dict['metric'] = 'none'\n",
    "#     row_dict['subsample_for_bin'] = 300000\n",
    "    row_dict['force_col_wise'] = False\n",
    "    row_dict['n_estimators'] = CFG.n_estimators\n",
    "    row_dict['early_stopping_round'] = CFG.early_stopping_rounds\n",
    "    row_dict['boosting_type'] = 'goss'\n",
    "    row_dict['verbose'] = -1\n",
    "    row_dict['max_bin'] = 255\n",
    "    \n",
    "    row_dict['num_leaves'] = int(row_dict['num_leaves'])\n",
    "    row_dict['max_depth'] = int(row_dict['max_depth'])\n",
    "    row_dict['min_data_in_leaf'] = int(row_dict['min_data_in_leaf'])\n",
    "    row_dict['bagging_freq'] = int(row_dict['bagging_freq'])\n",
    "    row_dict['learning_rate'] = 0.06433232950390658 # float(row_dict['learning_rate'])\n",
    "    \n",
    "    if not CFG.undersample:\n",
    "        row_dict['is_unbalance'] = True\n",
    "        row_dict['class_weight'] = 'balanced'\n",
    "        # row_dict['scale_pos_weight'] = class_imbalance\n",
    "    \n",
    "    if row_dict['boosting_type'] == 'goss':\n",
    "        row_dict['subsample'] = None\n",
    "        \n",
    "    best_lgbm_params.append(row_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e36958e0",
   "metadata": {},
   "source": [
    "## CatBoost load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "if CFG.kaggle:\n",
    "    param_list = glob.glob(\"/kaggle/input/icr-lgbm-optuna/optuna_catboost.csv\")\n",
    "else:\n",
    "    param_list = glob.glob(\"optuna_catboost.csv\")\n",
    "\n",
    "models = list()\n",
    "best_cb_params = list()\n",
    "\n",
    "cb_params = pd.DataFrame()\n",
    "\n",
    "for f in param_list:\n",
    "    tmp = pd.read_csv(f, index_col='Unnamed: 0')\n",
    "    if cb_params.shape[0] == 0:\n",
    "        cb_params = tmp\n",
    "    else:\n",
    "        cb_params = pd.concat([cb_params, tmp])\n",
    "        \n",
    "cb_params = cb_params.sort_values('value').head(CFG.n_stacking_models)\n",
    "param_cols = [c for c in cb_params.columns if c.startswith('params_')]\n",
    "cb_params = cb_params[param_cols]\n",
    "\n",
    "\n",
    "for idx, row in cb_params.iterrows():\n",
    "    row_dict = {k[7:]: v for k, v in row.items()}\n",
    "    row_dict['task_type'] = 'CPU'\n",
    "    row_dict['eval_metric'] = 'Logloss'\n",
    "    row_dict['loss_function'] = 'Logloss'\n",
    "    row_dict['random_seed'] = 13062023\n",
    "    row_dict['verbose'] = 0\n",
    "    row_dict['od_type'] = 'Iter'\n",
    "    row_dict['iterations'] = 10000 # CFG.n_estimators\n",
    "    row_dict['od_wait'] = CFG.early_stopping_rounds\n",
    "    row_dict['border_count'] = 254\n",
    "    \n",
    "    if not CFG.undersample:\n",
    "        row_dict['auto_class_weights'] = 'Balanced'\n",
    "        # row_dict['scale_pos_weight'] = class_imbalance\n",
    "        \n",
    "    if row_dict[\"task_type\"] != \"GPU\":\n",
    "        row_dict['colsample_bylevel'] = None\n",
    "    \n",
    "    if row_dict[\"bootstrap_type\"] != \"Bayesian\":\n",
    "        row_dict['bagging_temperature'] = None\n",
    "        \n",
    "    if row_dict[\"bootstrap_type\"] not in [\"Poisson\", \"Bernoulli\", \"MVS\"]:\n",
    "        row_dict['subsample'] = None\n",
    "    \n",
    "    if row_dict['grow_policy'] == 'Lossguide':\n",
    "        row_dict['max_leaves'] = int(row_dict['max_leaves'])\n",
    "    else:\n",
    "        row_dict['max_leaves'] = None\n",
    "    \n",
    "    if row_dict['grow_policy'] != 'SymmetricTree':\n",
    "        row_dict['boosting_type'] = 'Plain'\n",
    "    \n",
    "    best_cb_params.append(row_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aaccaa4",
   "metadata": {},
   "source": [
    "## XGBoost load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "if CFG.kaggle:\n",
    "    param_list = glob.glob(\"/kaggle/input/icr-lgbm-optuna/optuna_xgb.csv\")\n",
    "else:\n",
    "    param_list = glob.glob(\"optuna_xgb.csv\")\n",
    "\n",
    "models = list()\n",
    "best_xb_params = list()\n",
    "\n",
    "xb_params = pd.DataFrame()\n",
    "\n",
    "for f in param_list:\n",
    "    tmp = pd.read_csv(f, index_col='Unnamed: 0')\n",
    "    if xb_params.shape[0] == 0:\n",
    "        xb_params = tmp\n",
    "    else:\n",
    "        xb_params = pd.concat([xb_params, tmp])\n",
    "        \n",
    "xb_params = xb_params.sort_values('value').head(CFG.n_stacking_models)\n",
    "param_cols = [c for c in xb_params.columns if c.startswith('params_')]\n",
    "xb_params = xb_params[param_cols]\n",
    "\n",
    "for idx, row in xb_params.iterrows():\n",
    "    row_dict = {k[7:]: v for k, v in row.items()}\n",
    "    row_dict['n_estimators'] = CFG.n_estimators\n",
    "    row_dict['early_stopping_rounds'] = CFG.early_stopping_rounds\n",
    "    row_dict['random_state'] = 14062023\n",
    "    row_dict['verbosity'] = 0\n",
    "    row_dict['objective'] = \"binary:logistic\"\n",
    "    row_dict['eval_metric'] = \"logloss\"\n",
    "    row_dict['tree_method'] = \"exact\"\n",
    "    row_dict['booster'] = \"gbtree\"\n",
    "\n",
    "    if not CFG.undersample:\n",
    "        row_dict['scale_pos_weight'] = class_imbalance\n",
    "\n",
    "    if row_dict[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "        row_dict[\"max_depth\"] = int(row_dict[\"max_depth\"])\n",
    "        row_dict[\"min_child_weight\"] = int(row_dict[\"min_child_weight\"])\n",
    "    else:\n",
    "        row_dict[\"learning_rate\"] = None\n",
    "        row_dict[\"max_depth\"] = None\n",
    "        row_dict[\"min_child_weight\"] = None\n",
    "        row_dict[\"gamma\"] = None\n",
    "        row_dict[\"grow_policy\"] = None     \n",
    "\n",
    "    if row_dict[\"booster\"] != \"dart\":\n",
    "        row_dict[\"sample_type\"] = None\n",
    "        row_dict[\"normalize_type\"] = None\n",
    "        row_dict[\"rate_drop\"] = None\n",
    "        row_dict[\"skip_drop\"] = None\n",
    "\n",
    "    best_xb_params.append(row_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "651f99e9",
   "metadata": {},
   "source": [
    "## Train with the Cross Validation + any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def pp_prob(p):\n",
    "    c0 = p[:,0].sum()\n",
    "    c1 = p[:,1:].sum()\n",
    "    new_p = p * np.array([[1/(c0 if i==0 else c1) for i in range(p.shape[1])]])\n",
    "    new_p = new_p / np.sum(new_p,axis=1,keepdims=1)\n",
    "    return np.sum(new_p[:,1:],1,keepdims=False)\n",
    "\n",
    "def model_train(how, best_params):\n",
    "\n",
    "    oof_level2 = np.zeros([train_df['Class'].shape[0], len(best_params) + 1])\n",
    "    oof_level2[:, len(best_params)] = train_df['Class']\n",
    "    oof_level2_test = np.zeros([test_df.shape[0], len(best_params)])\n",
    "    \n",
    "    for i, params in tqdm(enumerate(best_params), total=len(best_params)):\n",
    "        \n",
    "        if how == 'tabpfn':\n",
    "            X, y, test = train_df_tabpfn[num_cols], train_df['Class'], test_df_tabpfn[num_cols]\n",
    "        else:\n",
    "            X, y, test = train_df[features], train_df['Class'], test_df[features]\n",
    "    \n",
    "        if CFG.n_stacking_folds > 0:\n",
    "            if CFG.k_fold:\n",
    "                kf = KFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=80620231+i)\n",
    "                y_fold = y\n",
    "            elif CFG.strat_k_fold:\n",
    "                kf = StratifiedKFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=80620231+i)\n",
    "                y_fold = y\n",
    "            else:\n",
    "                kf = MultilabelStratifiedKFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=80620231+i)\n",
    "                y_fold = greeks.iloc[:,1:4]\n",
    "            \n",
    "            print(f\"Training with {blu}{len(features)}{res} features\")\n",
    "\n",
    "            for fold, (fit_idx, val_idx) in enumerate(kf.split(X=X, y=y_fold)):\n",
    "                # Split the dataset according to the fold indexes.\n",
    "                X_train = X.iloc[fit_idx]\n",
    "                X_val = X.iloc[val_idx]\n",
    "                y_train = y.iloc[fit_idx]\n",
    "                y_val = y.iloc[val_idx]\n",
    "\n",
    "                # Make random under- or oversampling to balance classes\n",
    "                if CFG.undersample or CFG.oversample:\n",
    "                    if CFG.undersample:\n",
    "                        positive_count_train = y_train.value_counts()[1]\n",
    "                        sampler = RandomUnderSampler(sampling_strategy={0: positive_count_train * class_imbalance, \n",
    "                                                                        1: positive_count_train}, \n",
    "                                                    random_state=15062023+i, \n",
    "                                                    replacement=True)\n",
    "                    elif CFG.oversample:\n",
    "                        negative_count_train = y_train.value_counts()[0]\n",
    "                        sampler = RandomOverSampler(sampling_strategy={0: negative_count_train, \n",
    "                                                                    1: negative_count_train // class_imbalance}, \n",
    "                                                    random_state=2306020231+i)\n",
    "\n",
    "                    X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
    "                \n",
    "                if how == 'lgbm':\n",
    "                    model = lgb.LGBMClassifier(**params)\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=bll_metric, verbose=0)\n",
    "                    best_iter = model.best_iteration_\n",
    "                elif how == 'xgboost':\n",
    "                    model = xgb.XGBClassifier(**params)\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0)\n",
    "                    best_iter = model.get_booster().best_iteration\n",
    "                elif how == 'catboost':\n",
    "                    train_pool = Pool(X_train, y_train, cat_features=['EJ'])\n",
    "                    val_pool = Pool(X_val, y_val, cat_features=['EJ'])   \n",
    "                    model = cat.CatBoostClassifier(**params)\n",
    "                    model.fit(train_pool, eval_set=val_pool, verbose=0)\n",
    "                    best_iter = model.best_iteration_\n",
    "                elif how == 'tabpfn':\n",
    "                    model = TabPFNClassifier(N_ensemble_configurations=64, device='cuda:0')\n",
    "                    model.fit(X_train, y_train, overwrite_warning=True)\n",
    "                    best_iter = 0\n",
    "                else:\n",
    "                    return None, None\n",
    "                    \n",
    "                if how == 'tabpfn':\n",
    "                    val_preds = pp_prob(model.predict_proba(X_val))\n",
    "                    oof_level2_test[:, i] += pp_prob(model.predict_proba(test))\n",
    "                else:\n",
    "                    val_preds = model.predict_proba(X_val)[:,1]\n",
    "                    oof_level2_test[:, i] += model.predict_proba(test)[:,1]\n",
    "                \n",
    "                oof_level2[val_idx, i] = val_preds\n",
    "\n",
    "                val_score = balanced_log_loss(y_val, val_preds)\n",
    "                \n",
    "                print(f'Fold: {blu}{fold:>3}{res}| bll_metric: {blu}{val_score:.5f}{res}'\n",
    "                        f' | Best iteration: {blu}{best_iter:>4}{res}')  \n",
    "        else:\n",
    "            if how == 'lgbm':\n",
    "                model = lgb.LGBMClassifier(**params)\n",
    "                model.fit(X, y, verbose=0)\n",
    "            elif how == 'xgboost':\n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                model.fit(X, y, verbose=0)\n",
    "            elif how == 'catboost':\n",
    "                train_pool = Pool(X, y, cat_features=['EJ'])\n",
    "                model = cat.CatBoostClassifier(**params)\n",
    "                model.fit(train_pool, verbose=0)\n",
    "            elif how == 'tabpfn':\n",
    "                model = TabPFNClassifier(N_ensemble_configurations=64, device='cuda:0')\n",
    "                model.fit(X, y, overwrite_warning=True)\n",
    "            else:\n",
    "                return None, None\n",
    "\n",
    "            oof_level2_test[:, i] += model.predict_proba(test)[:,1]\n",
    "        \n",
    "    return oof_level2, oof_level2_test / max(CFG.n_stacking_folds, 1)\n",
    "\n",
    "oof_train_list = list()\n",
    "oof_test_list = list()\n",
    "\n",
    "if CFG.lgbm_train:\n",
    "    oof_level2_lgbm, oof_level2_test_lgbm = model_train('lgbm', best_lgbm_params)\n",
    "    oof_train_list.append(oof_level2_lgbm[:,:-1])\n",
    "    oof_test_list.append(oof_level2_test_lgbm)\n",
    "    y = oof_level2_lgbm[:,-1]\n",
    "\n",
    "if CFG.xgb_train:\n",
    "    oof_level2_xgb, oof_level2_test_xgb = model_train('xgboost', best_xgb_params)\n",
    "    oof_train_list.append(oof_level2_xgb[:,:-1])\n",
    "    oof_test_list.append(oof_level2_test_xgb)\n",
    "    y = oof_level2_xgb[:,-1]\n",
    "\n",
    "if CFG.cb_train:\n",
    "    oof_level2_cb, oof_level2_test_cb = model_train('catboost', best_cb_params)\n",
    "    oof_train_list.append(oof_level2_cb[:,:-1])\n",
    "    oof_test_list.append(oof_level2_test_cb)\n",
    "    y = oof_level2_cb[:,-1]\n",
    "\n",
    "if CFG.tabpfn_train:\n",
    "    oof_level2_tabpfn, oof_level2_test_tabpfn = model_train('tabpfn', [i for i in range(CFG.n_stacking_models_tabpfn)])\n",
    "    oof_train_list.append(oof_level2_tabpfn[:,:-1])\n",
    "    oof_test_list.append(oof_level2_test_tabpfn)\n",
    "    y = oof_level2_tabpfn[:,-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba604f33",
   "metadata": {},
   "source": [
    "# Stacking LGBM + CatBoost + XGBoost with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204709b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "oof_level2 = np.concatenate(oof_train_list, axis=1)\n",
    "oof_level2_test = np.concatenate(oof_test_list, axis=1)\n",
    "\n",
    "X = oof_level2\n",
    "\n",
    "# mean bll\n",
    "print(balanced_log_loss(y, np.mean(X, axis=1)))\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(X, y)\n",
    "\n",
    "pred = lr.predict_proba(X)[:,1]\n",
    "\n",
    "# lr bll\n",
    "print(balanced_log_loss(y, pred))\n",
    "\n",
    "weights = lr.coef_[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2ce02d1",
   "metadata": {},
   "source": [
    "# LGBM + TSS + Stacking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d945bfb",
   "metadata": {},
   "source": [
    "## Create metafeatures for the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5121d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates for the second level\n",
    "dates_level2 = [28, 29, 30, 31, 32, 33]\n",
    "\n",
    "# train dates\n",
    "dates_train = X_train.date_block_num\n",
    "# dates_train_level2 = dates_train[dates_train.isin(dates)]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin(dates_level2)]\n",
    "\n",
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], len(best_params)+1])\n",
    "X_train_level2[:, len(best_params)] = y_train_level2\n",
    "\n",
    "meta_index_begin = 0\n",
    "meta_index_end = 0\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in tqdm_notebook(dates_level2):\n",
    "    \n",
    "    # split data\n",
    "    train_index = X_train.loc[dates_train <  cur_block_num].index\n",
    "    test_index  = X_train.loc[dates_train == cur_block_num].index\n",
    "    \n",
    "    X_train_l2 = X_train.loc[train_index, :]\n",
    "    X_test_l2 =  X_train.loc[test_index, :]\n",
    "\n",
    "    y_train_l2 = y_train[train_index]\n",
    "    y_test_l2 =  y_train[test_index]\n",
    "    \n",
    "    meta_index_end += y_test_l2.shape[0]\n",
    "    \n",
    "    # predict metafeatures for each of LGBM regressors\n",
    "    for i, params in enumerate(tqdm_notebook(best_params)):\n",
    "        lgb = LGBMRegressor(**params)\n",
    "        reg = lgb.fit(X_train_l2, y_train_l2)\n",
    "        pred = lgb.predict(X_test_l2)\n",
    "        X_train_level2[meta_index_begin:meta_index_end, i] = pred\n",
    "\n",
    "        del lgb, reg, pred\n",
    "        gc.collect()\n",
    "        \n",
    "    meta_index_begin = meta_index_end\n",
    "        \n",
    "X_train_level2 = pd.DataFrame(X_train_level2, columns=[f'lgbr_{i+1}' for i in range(len(best_params))]+['gt'])\n",
    "X_train_level2.to_pickle('LGBM_X_train_level2.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03cebfa7",
   "metadata": {},
   "source": [
    "## Create metafeatures for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_level2 = np.zeros([X_test.shape[0], len(best_params)])\n",
    "\n",
    "for i, params in enumerate(tqdm_notebook(best_params)):\n",
    "    lgb = LGBMRegressor(**params)\n",
    "    reg = lgb.fit(X_train, y_train)\n",
    "    pred = lgb.predict(X_test)\n",
    "    X_test_level2[:, i] = pred\n",
    "    \n",
    "    del lgb, reg, pred\n",
    "    gc.collect()\n",
    "    \n",
    "X_test_level2 = pd.DataFrame(X_test_level2, columns=[f'lgbr_{i+1}' for i in range(len(best_params))])\n",
    "X_test_level2.to_pickle('LGBM_X_test_level2.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a5cada6",
   "metadata": {},
   "source": [
    "## Stacking with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = X_train_level2[[c for c in X_train_level2.columns if c != 'gt']]\n",
    "y = X_train_level2['gt']\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "pred = lr.predict(X_test_level2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
