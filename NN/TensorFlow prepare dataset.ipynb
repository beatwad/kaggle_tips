{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/scikit/scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2022-04-15T06:42:47.505449Z","iopub.execute_input":"2022-04-15T06:42:47.505814Z","iopub.status.idle":"2022-04-15T06:42:56.453703Z","shell.execute_reply.started":"2022-04-15T06:42:47.505756Z","shell.execute_reply":"2022-04-15T06:42:56.452792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  UMP TF-Record: CombinatorialPurgedGroupKFold\n\nIn this notebook, I am going to create TF-Record for UMP dataset using CombinatorialPurgedGroupKFold CV strategy.","metadata":{"papermill":{"duration":0.016475,"end_time":"2022-01-25T15:39:01.467349","exception":false,"start_time":"2022-01-25T15:39:01.450874","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport json\nimport numpy as np\nfrom scipy.special import comb\nfrom itertools import combinations\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nclass CombinatorialPurgedGroupKFold():\n    def __init__(self, n_splits = 6, n_test_splits = 2, purge = 1, pctEmbargo = 0.01, **kwargs):\n        self.n_splits = n_splits\n        self.n_test_splits = n_test_splits\n        self.purge = purge\n        self.pctEmbargo = pctEmbargo\n        \n    def split(self, X, y = None, groups = None):\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n            \n        u, ind = np.unique(groups, return_index = True)\n        unique_groups = u[np.argsort(ind)]\n        n_groups = len(unique_groups)\n        group_dict = {}\n        for idx in range(len(X)):\n            if groups[idx] in group_dict:\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n                \n        n_folds = comb(self.n_splits, self.n_test_splits, exact = True)\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n            \n        mbrg = int(n_groups * self.pctEmbargo)\n        if mbrg < 0:\n            raise ValueError(\n                \"The number of 'embargoed' groups should not be negative\")\n        \n        split_dict = {}\n        group_test_size = n_groups // self.n_splits\n        for split in range(self.n_splits):\n            if split == self.n_splits - 1:\n                split_dict[split] = unique_groups[int(split * group_test_size):].tolist()\n            else:\n                split_dict[split] = unique_groups[int(split * group_test_size):int((split + 1) * group_test_size)].tolist()\n        \n        for test_splits in combinations(range(self.n_splits), self.n_test_splits):\n            test_groups = []\n            banned_groups = []\n            for split in test_splits:\n                test_groups += split_dict[split]\n                banned_groups += unique_groups[split_dict[split][0] - self.purge:split_dict[split][0]].tolist()\n                banned_groups += unique_groups[split_dict[split][-1] + 1:split_dict[split][-1] + self.purge + mbrg + 1].tolist()\n            train_groups = [i for i in unique_groups if (i not in banned_groups) and (i not in test_groups)]\n\n            train_idx = []\n            test_idx = []\n            for train_group in train_groups:\n                train_idx += group_dict[train_group]\n            for test_group in test_groups:\n                test_idx += group_dict[test_group]\n            yield train_idx, test_idx","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-15T06:42:56.457356Z","iopub.execute_input":"2022-04-15T06:42:56.457642Z","iopub.status.idle":"2022-04-15T06:43:01.712518Z","shell.execute_reply.started":"2022-04-15T06:42:56.457607Z","shell.execute_reply":"2022-04-15T06:43:01.711549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's use a small sample data to understand this CV strategy.","metadata":{}},{"cell_type":"code","source":"n_splits = 6\nn_test_splits = 1\nelements = list(range(10 * (n_splits + n_test_splits)))\ngroups = [element // n_splits for element in elements]\ndata = pd.DataFrame({\"group\": groups, \"element\": elements})\nkfold = CombinatorialPurgedGroupKFold(n_splits, n_test_splits)\n\nfor index, (train_indices, test_indices) in enumerate(kfold.split(data, groups=data[\"group\"])):\n    print(\"=\" * 100)\n    print(f\"Fold {index}\")\n    print(\"=\" * 100)\n    print(\"Train indices:\", train_indices, \"Length:\", len(train_indices))\n    print(\"Test Indices:\", test_indices, \"Length:\", len(test_indices))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T06:43:01.714059Z","iopub.execute_input":"2022-04-15T06:43:01.714366Z","iopub.status.idle":"2022-04-15T06:43:01.745753Z","shell.execute_reply.started":"2022-04-15T06:43:01.714326Z","shell.execute_reply":"2022-04-15T06:43:01.744541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## Import dataset","metadata":{"papermill":{"duration":0.015291,"end_time":"2022-01-25T15:39:09.296817","exception":false,"start_time":"2022-01-25T15:39:09.281526","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()","metadata":{"papermill":{"duration":16.88418,"end_time":"2022-01-25T15:39:26.19638","exception":false,"start_time":"2022-01-25T15:39:09.3122","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-15T06:43:01.747701Z","iopub.execute_input":"2022-04-15T06:43:01.748064Z","iopub.status.idle":"2022-04-15T06:43:18.068755Z","shell.execute_reply.started":"2022-04-15T06:43:01.748018Z","shell.execute_reply":"2022-04-15T06:43:18.067831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set time features","metadata":{}},{"cell_type":"code","source":"test_time_id_len = 942961\n\ncalendar_df = pd.read_csv(\"../input/chinese-holidays/holidays_of_china_from_2014_to_2030.csv\", parse_dates=[\"date\"], date_parser=lambda x: datetime.strptime(x, '%Y-%m-%d'))\n\n# leave only national holidays\ncalendar_df = calendar_df.loc[(calendar_df.type.isin([\"National holiday\", \"Common local holiday\"]))]\n\n# fill with everyday from 2014 to 2022\ncalendar_df = (\n    pd.DataFrame({\"date\": pd.date_range(start=\"2014-01-01\", end=\"2023-01-01\")}).merge(calendar_df, on=\"date\", how=\"left\")\n    .assign(weekday=lambda x: x.date.dt.day_name(), year=lambda x: x.date.dt.year)\n)\n\n# remove weekends and national holidays and align with time_id\ncalendar_df = (\n    calendar_df.loc[(~calendar_df.weekday.isin([\"Sunday\", \"Saturday\"]))&(calendar_df.name.isna())]\n    .reset_index(drop=True)\n    .head(train.time_id.max()+1)\n    .dropna(axis=1)\n)\n\ncalendar_df['quarter'] = calendar_df['date'].dt.to_period('Q')\ncalendar_df['time_id'] = calendar_df.index\n\nle = LabelEncoder()\ncalendar_df['quarter'] = le.fit_transform(calendar_df['quarter'])\n\ntrain = train.merge(calendar_df[['time_id', 'year', 'quarter']], how='left', on='time_id')\n\ntime_id = train.pop(\"time_id\")\nquarter = train.pop(\"quarter\")\nyear = train.pop(\"year\")\n\ndel calendar_df\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T06:43:18.071731Z","iopub.execute_input":"2022-04-15T06:43:18.072068Z","iopub.status.idle":"2022-04-15T06:43:24.003229Z","shell.execute_reply.started":"2022-04-15T06:43:18.072025Z","shell.execute_reply":"2022-04-15T06:43:24.002266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set cluster_id feature","metadata":{}},{"cell_type":"code","source":"%%time\ninv_id_to_cluster = pd.read_pickle('../input/part-1-1-ubiquant-clustering/clustered_inv_index.pkl')\ninv_id_to_cluster.index.name = 'investment_id'\n\ntrain = train.merge(inv_id_to_cluster, how='left', on='investment_id')\ntrain.cluster.fillna(0, inplace=True)\ntrain.cluster = train.cluster.astype(np.uint16)\n\ncluster = train.pop('cluster')\nnum_clusters = cluster.unique().shape[0] - 1\ncluster = pd.get_dummies(cluster, drop_first=True, prefix='cluster_')\n\n# Sorted groups 'investment_id' by 'cluster_id': \n# investment_id_cluster_dict = dict()\n# for unique in cluster_id_feature.unique():\n#     investment_id_cluster_dict.update({unique: inv_id_to_cluster[inv_id_to_cluster['cluster'] == unique].index.to_numpy()})\n    \n# Hot Encode cluster feature\n# cluster_id_feature = tf.keras.utils.to_categorical(cluster_id_feature, num_classes=num_classes, dtype='uint8')\n    \ndel inv_id_to_cluster\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T06:43:24.005101Z","iopub.execute_input":"2022-04-15T06:43:24.005434Z","iopub.status.idle":"2022-04-15T06:43:29.767112Z","shell.execute_reply.started":"2022-04-15T06:43:24.005390Z","shell.execute_reply":"2022-04-15T06:43:29.766137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set A-shares sub section feature","metadata":{}},{"cell_type":"code","source":"map_info = pd.read_csv('../input/ubiquant-a-shares/map_info.csv')[['Sub_Section', 'Main_Section', 'Market_Value', 'investment_id']]\nmap_info.rename({'Sub_Section': 'sub_section', 'Main_Section': 'main_section', 'Market_Value': 'market_value'}, axis=1, inplace=True)\n\nle = LabelEncoder()\nmap_info['sub_section'] = le.fit_transform(map_info['sub_section'])\nmap_info['sub_section'] += 1\nmap_info['main_section'] += 1\n\ntrain = train.merge(map_info, how='left', on='investment_id')\ntrain[['main_section', 'sub_section']] = train[['main_section', 'sub_section']].fillna(0)\ntrain[['main_section', 'investment_id', 'sub_section']] = train[['main_section', 'investment_id', 'sub_section']].astype(np.uint16)\ntrain['market_value'] = train['market_value'].fillna(train['market_value'].mean())\ntrain[['market_value']] = train[['market_value']].astype(np.uint64)\n\nsub_section = train.pop('sub_section')\nnum_sub_sections = sub_section.unique().shape[0]\nsub_section = pd.get_dummies(sub_section)\n\nmain_section = train.pop('main_section')\nnum_main_sections = main_section.unique().shape[0] - 1\nmain_section = pd.get_dummies(main_section, drop_first=True)\n\nmarket_value = train.pop('market_value')\n\n# # Sorted groups 'investment_id' by 'cluster_id': \n# investment_id_sub_section_dict = dict()\n# for unique in sub_section_feature.unique():\n#     investment_id_sub_section_dict.update({unique: map_info[map_info['sub_section'] == unique].index.to_numpy()})\n    \n# Hot Encode sub section feature\n# sub_section_feature = tf.keras.utils.to_categorical(sub_section_feature, num_classes=num_sections, dtype='uint8')\n\ndel map_info\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-15T06:43:29.770238Z","iopub.execute_input":"2022-04-15T06:43:29.770476Z","iopub.status.idle":"2022-04-15T06:43:37.604886Z","shell.execute_reply.started":"2022-04-15T06:43:29.770447Z","shell.execute_reply":"2022-04-15T06:43:37.603790Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop unnecessary features","metadata":{}},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")","metadata":{"papermill":{"duration":0.041856,"end_time":"2022-01-25T15:39:26.254473","exception":false,"start_time":"2022-01-25T15:39:26.212617","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-15T06:43:37.606458Z","iopub.execute_input":"2022-04-15T06:43:37.606831Z","iopub.status.idle":"2022-04-15T06:43:37.612974Z","shell.execute_reply.started":"2022-04-15T06:43:37.606757Z","shell.execute_reply":"2022-04-15T06:43:37.611851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set target features","metadata":{}},{"cell_type":"code","source":"y = train.pop(\"target\")","metadata":{"papermill":{"duration":0.04224,"end_time":"2022-01-25T15:39:26.375506","exception":false,"start_time":"2022-01-25T15:39:26.333266","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-15T06:43:37.614733Z","iopub.execute_input":"2022-04-15T06:43:37.615106Z","iopub.status.idle":"2022-04-15T06:43:37.630180Z","shell.execute_reply.started":"2022-04-15T06:43:37.615059Z","shell.execute_reply":"2022-04-15T06:43:37.628745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create TF-Record","metadata":{"papermill":{"duration":0.017564,"end_time":"2022-01-25T15:39:30.64918","exception":false,"start_time":"2022-01-25T15:39:30.631616","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_record(i):\n    dic = {}\n    dic[\"features\"] = tf.train.Feature(float_list=tf.train.FloatList(value=list(train.iloc[i])))\n    dic[\"time_id\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[time_id.iloc[i]]))\n    dic[\"year\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[year.iloc[i]]))\n    dic[\"quarter\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[quarter.iloc[i]]))\n    dic[\"cluster\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(cluster.iloc[i])))\n    dic[\"sub_section\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(sub_section.iloc[i])))\n    dic[\"main_section\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=list(main_section.iloc[i])))\n    dic[\"market_value\"] = tf.train.Feature(int64_list=tf.train.Int64List(value=[market_value.iloc[i]]))\n    dic[\"target\"] = tf.train.Feature(float_list=tf.train.FloatList(value=[y.iloc[i]]))\n    record_bytes = tf.train.Example(features=tf.train.Features(feature=dic)).SerializeToString()\n    return record_bytes\n    \ndef decode_function(record_bytes):\n      return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\n          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"year\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"quarter\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"cluster\": tf.io.FixedLenFeature([num_clusters], dtype=tf.int64), \n          \"sub_section\": tf.io.FixedLenFeature([num_sub_sections], dtype=tf.int64),\n          \"main_section\": tf.io.FixedLenFeature([num_main_sections], dtype=tf.int64),\n          \"market_value\": tf.io.FixedLenFeature([], dtype=tf.int64),\n          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n      }\n  )","metadata":{"execution":{"iopub.status.busy":"2022-04-15T06:43:37.631576Z","iopub.execute_input":"2022-04-15T06:43:37.631883Z","iopub.status.idle":"2022-04-15T06:43:37.652368Z","shell.execute_reply.started":"2022-04-15T06:43:37.631838Z","shell.execute_reply":"2022-04-15T06:43:37.650873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now create the whole dataset, it will take a long time.","metadata":{}},{"cell_type":"code","source":"%%time\nimport time\nn_splits = 5\nn_test_splits = 1\n# kfold = CombinatorialPurgedGroupKFold(n_splits, n_test_splits)\nkfold = StratifiedGroupKFold(n_splits)\nfor fold, (train_indices, test_indices) in enumerate(kfold.split(train, investment_id, groups=time_id)):\n    if fold != 4:\n        continue\n    print(\"=\" * 100)\n    print(f\"Fold {fold}\")\n    print(\"=\" * 100)\n    print(\"Train Sample size:\", len(train_indices))\n    print(\"Test Sample size:\", len(test_indices))\n    train_save_path = f\"fold_{fold}_train.tfrecords\"\n    begin = time.time()\n    print(f\"Creating {train_save_path}\")\n    with tf.io.TFRecordWriter(train_save_path) as file_writer:\n        for i in train_indices:\n            file_writer.write(create_record(i))\n    print(\"Elapsed time: %.2f\"%(time.time() - begin))\n    test_save_path = f\"fold_{fold}_test.tfrecords\"\n    begin = time.time()\n    print(f\"Creating {test_save_path}\")\n    with tf.io.TFRecordWriter(test_save_path) as file_writer:\n        for i in test_indices:\n            file_writer.write(create_record(i))\n    print(\"Elapsed time: %.2f\"%(time.time() - begin))","metadata":{"execution":{"iopub.status.busy":"2022-04-15T06:44:07.579174Z","iopub.execute_input":"2022-04-15T06:44:07.580462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Write unique Investment Ids","metadata":{}},{"cell_type":"code","source":"investment_ids = investment_id.unique()\ninvestment_id_df = pd.DataFrame({\"investment_id\": investment_ids})\ninvestment_id_df.to_csv(\"investment_ids.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-15T06:43:38.017883Z","iopub.execute_input":"2022-04-15T06:43:38.018635Z","iopub.status.idle":"2022-04-15T06:43:38.052465Z","shell.execute_reply.started":"2022-04-15T06:43:38.018584Z","shell.execute_reply":"2022-04-15T06:43:38.051710Z"},"trusted":true},"execution_count":null,"outputs":[]}]}