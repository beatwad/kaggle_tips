{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T20:52:51.609734Z",
     "iopub.status.busy": "2022-04-19T20:52:51.609101Z",
     "iopub.status.idle": "2022-04-19T20:52:53.894362Z",
     "shell.execute_reply": "2022-04-19T20:52:53.893294Z",
     "shell.execute_reply.started": "2022-04-19T20:52:51.609701Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.data.ops.dataset_ops import PrefetchDataset\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T20:52:53.896208Z",
     "iopub.status.busy": "2022-04-19T20:52:53.895938Z",
     "iopub.status.idle": "2022-04-19T20:52:53.902136Z",
     "shell.execute_reply": "2022-04-19T20:52:53.901297Z",
     "shell.execute_reply.started": "2022-04-19T20:52:53.896176Z"
    }
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    train=True,\n",
    "    test=False,\n",
    "    inference=True,\n",
    "    seed=42,\n",
    "    folds=5,\n",
    "    cv_method=\"stacking\", # available methods: single, stacking, full\n",
    "    data_path=Path(\"../input/ubiquant-market-prediction-half-precision-pickle\"),\n",
    "    tf_record_dataset_path=Path(\"../input/purged-5fold-tf-records\"),\n",
    "    model_path=Path(\"../input/5-fold-tf-models\"),\n",
    "    BATCH_SIZE = 4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T20:52:54.352486Z",
     "iopub.status.busy": "2022-04-19T20:52:54.351951Z",
     "iopub.status.idle": "2022-04-19T20:52:54.364595Z",
     "shell.execute_reply": "2022-04-19T20:52:54.363992Z",
     "shell.execute_reply.started": "2022-04-19T20:52:54.352452Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_function(record_bytes):\n",
    "      return tf.io.parse_single_example(\n",
    "      # Data\n",
    "      record_bytes,\n",
    "      # Schema\n",
    "      {\n",
    "          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n",
    "          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"year\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"quarter\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"cluster\": tf.io.FixedLenFeature([num_clusters], dtype=tf.int64), \n",
    "          \"sub_section\": tf.io.FixedLenFeature([num_sub_sections], dtype=tf.int64),\n",
    "          \"main_section\": tf.io.FixedLenFeature([num_main_sections], dtype=tf.int64),\n",
    "          \"market_value\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"time_ids_count\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"num_or_cat\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n",
    "      }\n",
    "  )\n",
    "    \n",
    "def preprocess(item):\n",
    "    return (item[\"features\"], \n",
    "            item[\"time_id\"],\n",
    "            item[\"time_ids_count\"], \n",
    "            item[\"num_or_cat\"]\n",
    "           ), item[\"target\"]\n",
    "\n",
    "def make_dataset(file_paths, batch_size=args.BATCH_SIZE, mode=\"train\"):\n",
    "    ds = tf.data.TFRecordDataset(file_paths)\n",
    "    ds = ds.map(decode_function)\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(batch_size * 4)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-11T14:20:34.4346Z",
     "iopub.status.busy": "2022-03-11T14:20:34.433761Z",
     "iopub.status.idle": "2022-03-11T14:20:34.44124Z",
     "shell.execute_reply": "2022-03-11T14:20:34.440227Z",
     "shell.execute_reply.started": "2022-03-11T14:20:34.434547Z"
    }
   },
   "source": [
    "# Build model\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T20:52:54.366496Z",
     "iopub.status.busy": "2022-04-19T20:52:54.365943Z",
     "iopub.status.idle": "2022-04-19T20:52:54.388122Z",
     "shell.execute_reply": "2022-04-19T20:52:54.387383Z",
     "shell.execute_reply.started": "2022-04-19T20:52:54.366465Z"
    }
   },
   "outputs": [],
   "source": [
    "def correlation(x, y, axis=-2):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    \n",
    "    xvar = tf.reduce_sum(tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    yvar = tf.reduce_sum(tf.math.squared_difference(y, ymean), axis=axis)\n",
    "\n",
    "    cov = tf.reduce_sum((x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xvar * yvar)\n",
    "    \n",
    "    return tf.constant(1.0, dtype=x.dtype) - corr\n",
    "\n",
    "def correlation_loss(x,y, axis=-2):\n",
    "    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n",
    "    while trying to have the same mean and variance\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xsqsum * ysqsum)\n",
    "    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\n",
    "\n",
    "def ccc(x, y):\n",
    "    x_mean = tf.reduce_mean(x)\n",
    "    y_mean = tf.reduce_mean(y)\n",
    "    covariance = (x - x_mean) * (y - y_mean)\n",
    "    x_var = tf.reduce_mean(tf.math.squared_difference(x, x_mean))\n",
    "    y_var = tf.reduce_mean(tf.math.squared_difference(y, y_mean))\n",
    "    return 2 * covariance / (x_var + y_var + tf.math.square(x_mean - y_mean) + 1e-7)\n",
    "\n",
    "def ccc_loss(x, y):\n",
    "    return 1.0 - ccc(x, y)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    print(f'LR is {lr}')\n",
    "    return lr\n",
    "\n",
    "class EarlyStoppingByLimits(Callback):\n",
    "    def __init__(self, train_limit=0.5, val_limit=0.5):\n",
    "        super(Callback, self).__init__()\n",
    "        self.train_limit = train_limit\n",
    "        self.val_limit = val_limit\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        if loss < self.train_limit or val_loss < self.val_limit:\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T20:52:54.389986Z",
     "iopub.status.busy": "2022-04-19T20:52:54.389559Z",
     "iopub.status.idle": "2022-04-19T20:52:54.40599Z",
     "shell.execute_reply": "2022-04-19T20:52:54.405317Z",
     "shell.execute_reply.started": "2022-04-19T20:52:54.389949Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    # features\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n",
    "    feature_x = tf.keras.layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = tf.keras.layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = tf.keras.layers.Dense(256, activation='swish')(feature_x)\n",
    "    \n",
    "    # time_id\n",
    "    time_id_inputs = tf.keras.Input((1, ), dtype=tf.int64)\n",
    "    time_id_x = tf.keras.layers.Dense(16, activation='swish')(time_id_inputs)\n",
    "    time_id_x = tf.keras.layers.Dense(16, activation='swish')(time_id_x)\n",
    "    time_id_x = tf.keras.layers.Dense(16, activation='swish')(time_id_x)\n",
    "    \n",
    "    # time_ids_count\n",
    "    time_ids_count_inputs = tf.keras.Input((1, ), dtype=tf.int64)\n",
    "    time_ids_count_x = tf.keras.layers.Dense(16, activation='swish')(time_ids_count_inputs)\n",
    "    time_ids_count_x = tf.keras.layers.Dense(16, activation='swish')(time_ids_count_x)\n",
    "    time_ids_count_x = tf.keras.layers.Dense(16, activation='swish')(time_ids_count_x)\n",
    "    \n",
    "    # num_or_cat\n",
    "    num_or_cat_inputs = tf.keras.Input((1, ), dtype=tf.float32)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=1)([time_id_x, time_ids_count_x, feature_x, num_or_cat_inputs])\n",
    "    \n",
    "    x = tf.keras.layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    rmse = tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[features_inputs, time_id_inputs, time_ids_count_inputs, num_or_cat_inputs], outputs=[output])\n",
    "#     model.compile(optimizer=tf.optimizers.Adam(1e-3), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(1e-4),  loss=ccc_loss, metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-04-19T20:52:54.40765Z",
     "iopub.status.busy": "2022-04-19T20:52:54.407084Z",
     "iopub.status.idle": "2022-04-19T20:52:54.915711Z",
     "shell.execute_reply": "2022-04-19T20:52:54.915014Z",
     "shell.execute_reply.started": "2022-04-19T20:52:54.407614Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "## 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T20:52:54.917453Z",
     "iopub.status.busy": "2022-04-19T20:52:54.917212Z",
     "iopub.status.idle": "2022-04-19T21:18:56.665727Z",
     "shell.execute_reply": "2022-04-19T21:18:56.664677Z",
     "shell.execute_reply.started": "2022-04-19T20:52:54.917423Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "models = []\n",
    "tf.random.set_seed(args.seed)\n",
    "\n",
    "for i in range(args.folds):\n",
    "    train_path = args.tf_record_dataset_path.joinpath(f\"fold_{i}_train.tfrecords\")\n",
    "    valid_path = args.tf_record_dataset_path.joinpath(f\"fold_{i}_test.tfrecords\")\n",
    "    valid_ds = make_dataset([valid_path], mode=\"valid\")\n",
    "    model = get_model2()\n",
    "    if args.train :\n",
    "        train_ds = make_dataset([train_path])\n",
    "        sched_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "#         early_stop = EarlyStoppingByLimits (train_limit=0.75, val_limit=0.85)\n",
    "        history = model.fit(train_ds, epochs=30, validation_data=valid_ds, shuffle=True, callbacks=[sched_callback, checkpoint, early_stop])\n",
    "        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n",
    "            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n",
    "            plt.title(metric.upper())\n",
    "            plt.show()\n",
    "        del train_ds\n",
    "    else:\n",
    "        model.load_weights(args.model_path.joinpath(f\"model_{i}.tf\"))\n",
    "    models.append(model)\n",
    "    y_vals = []\n",
    "    for _, y in valid_ds:\n",
    "        y_vals += list(y.numpy().reshape(-1))\n",
    "    y_val = np.array(y_vals)\n",
    "    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\n",
    "    print(f\"Pearson Score: {pearson_score}\")\n",
    "\n",
    "    del valid_ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with PCA averaging\n",
    "\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T21:25:28.720459Z",
     "iopub.status.busy": "2022-04-19T21:25:28.719482Z",
     "iopub.status.idle": "2022-04-19T21:25:28.736299Z",
     "shell.execute_reply": "2022-04-19T21:25:28.735365Z",
     "shell.execute_reply.started": "2022-04-19T21:25:28.720415Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "\n",
    "def preprocess_test(feature, \n",
    "                    time_id, \n",
    "                    time_ids_count, \n",
    "                    num_or_cat, \n",
    "                    ):\n",
    "    return (feature, \n",
    "            time_id, \n",
    "            time_ids_count, \n",
    "            num_or_cat,\n",
    "           ), 0\n",
    "\n",
    "def make_test_dataset(feature, \n",
    "                      time_id, \n",
    "                      time_ids_count, \n",
    "                      num_or_cat,\n",
    "                      batch_size=args.BATCH_SIZE):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((feature, \n",
    "                                              time_id, \n",
    "                                              time_ids_count, \n",
    "                                              num_or_cat\n",
    "                                             )))\n",
    "    ds = ds.map(preprocess_test)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def pca_averaging(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    res = np.hstack(y_preds)\n",
    "    if len(res) > 1:\n",
    "        res = -1 * pca.fit_transform(res)\n",
    "    else:\n",
    "        res = np.mean(res, axis=1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction with stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T21:18:56.711327Z",
     "iopub.status.busy": "2022-04-19T21:18:56.710683Z",
     "iopub.status.idle": "2022-04-19T21:18:56.737371Z",
     "shell.execute_reply": "2022-04-19T21:18:56.736477Z",
     "shell.execute_reply.started": "2022-04-19T21:18:56.711283Z"
    }
   },
   "outputs": [],
   "source": [
    "env = ubiquant.make_env()\n",
    "iter_test = env.iter_test() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T21:26:26.002886Z",
     "iopub.status.busy": "2022-04-19T21:26:26.002502Z",
     "iopub.status.idle": "2022-04-19T21:26:26.026968Z",
     "shell.execute_reply": "2022-04-19T21:26:26.025711Z",
     "shell.execute_reply.started": "2022-04-19T21:26:26.002845Z"
    }
   },
   "outputs": [],
   "source": [
    "time_id = 1220\n",
    "test_time_id_len = 942961\n",
    "\n",
    "if args.inference:\n",
    "    features = [f\"f_{i}\" for i in range(300)]\n",
    "    \n",
    "    for (test_df, sample_prediction_df) in iter_test:\n",
    "        # extract time_id\n",
    "        try:\n",
    "            test_time_id = int(test_df['row_id'].values[0].split('_')[0]) # extract time_id from row_id\n",
    "            test_df[\"time_id\"] = test_time_id\n",
    "        except:\n",
    "            test_df[\"time_id\"] = time_id\n",
    "            test_time_id = 0\n",
    "        # in case of error just increase time_id on 1\n",
    "        if test_time_id:\n",
    "            time_id = test_time_id + 1\n",
    "        else:\n",
    "            test_df[\"time_id\"] = time_id\n",
    "            time_id += 1\n",
    "        \n",
    "        # get number of time_ids in the test_df\n",
    "        test_df['time_ids_count'] = len(test_df)\n",
    "        \n",
    "        # create num_or_cat feature for the test_df\n",
    "        unqiues = len(test_df['f_7'].unique())\n",
    "        if unqiues/len(test_df) > 0.5:\n",
    "            test_df['num_or_cat'] = 0\n",
    "        else:\n",
    "            test_df['num_or_cat'] = 1\n",
    "        \n",
    "        ds = make_test_dataset(test_df[features], \n",
    "                                test_df[\"time_id\"],\n",
    "                                test_df[\"time_ids_count\"], \n",
    "                                test_df[\"num_or_cat\"])\n",
    "        \n",
    "        sample_prediction_df['target'] = pca_averaging(models, ds)\n",
    "        env.predict(sample_prediction_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
