{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install necesessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:08:27.424103Z",
     "iopub.status.busy": "2022-02-02T08:08:27.423689Z",
     "iopub.status.idle": "2022-02-02T08:09:49.856299Z",
     "shell.execute_reply": "2022-02-02T08:09:49.855110Z",
     "shell.execute_reply.started": "2022-02-02T08:08:27.423995Z"
    }
   },
   "outputs": [],
   "source": [
    "# # norfair dependencies\n",
    "# %cd /kaggle/input/norfair031py3/\n",
    "# !pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n",
    "# !pip install rich-9.13.0-py3-none-any.whl\n",
    "\n",
    "# !mkdir /kaggle/working/tmp\n",
    "# !cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n",
    "# %cd /kaggle/working/tmp/filterpy-1.4.5/\n",
    "# !pip install .\n",
    "# !rm -rf /kaggle/working/tmp\n",
    "\n",
    "# # norfair\n",
    "# %cd /kaggle/input/norfair031py3/\n",
    "# !pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:49.859197Z",
     "iopub.status.busy": "2022-02-02T08:09:49.858899Z",
     "iopub.status.idle": "2022-02-02T08:09:52.410774Z",
     "shell.execute_reply": "2022-02-02T08:09:52.409527Z",
     "shell.execute_reply.started": "2022-02-02T08:09:49.859163Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import ast\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import torch\n",
    "import sys\n",
    "from PIL import Image as Img\n",
    "from IPython.display import display\n",
    "from norfair import Detection, Tracker\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from fastai.vision.all import *\n",
    "import timm\n",
    "\n",
    "sys.path.append('tensorflow-great-barrier-reef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:52.412634Z",
     "iopub.status.busy": "2022-02-02T08:09:52.412387Z",
     "iopub.status.idle": "2022-02-02T08:09:53.976959Z",
     "shell.execute_reply": "2022-02-02T08:09:53.975775Z",
     "shell.execute_reply.started": "2022-02-02T08:09:52.412602Z"
    }
   },
   "outputs": [],
   "source": [
    "FOLD = 2\n",
    "ANNS = True\n",
    "ann_str = '' if ANNS else '_na'\n",
    "\n",
    "ROOT_DIR  = 'tensorflow-great-barrier-reef'\n",
    "DATASET_PATH = 'tensorflow-great-barrier-reef/train_images/'\n",
    "\n",
    "# !mkdir -p /root/.config/Ultralytics\n",
    "# !cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/\n",
    "\n",
    "f_best = f'yolov5_models/f{FOLD}_na_10_adam_25.pt'\n",
    "f_last = f'yolov5_models/f{FOLD}_na_10_adam_25.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation\n",
    "\n",
    "## Evaluation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:53.981580Z",
     "iopub.status.busy": "2022-02-02T08:09:53.981182Z",
     "iopub.status.idle": "2022-02-02T08:09:54.010464Z",
     "shell.execute_reply": "2022-02-02T08:09:54.009244Z",
     "shell.execute_reply.started": "2022-02-02T08:09:53.981527Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_iou(bboxes1, bboxes2, bbox_mode='xywh'):\n",
    "    assert len(bboxes1.shape) == 2 and bboxes1.shape[1] == 4\n",
    "    assert len(bboxes2.shape) == 2 and bboxes2.shape[1] == 4\n",
    "    \n",
    "    bboxes1 = bboxes1.copy()\n",
    "    bboxes2 = bboxes2.copy()\n",
    "    \n",
    "    if bbox_mode == 'xywh':\n",
    "        bboxes1[:, 2:] += bboxes1[:, :2]\n",
    "        bboxes2[:, 2:] += bboxes2[:, :2]\n",
    "\n",
    "    x11, y11, x12, y12 = np.split(bboxes1, 4, axis=1)\n",
    "    x21, y21, x22, y22 = np.split(bboxes2, 4, axis=1)\n",
    "    xA = np.maximum(x11, np.transpose(x21))\n",
    "    yA = np.maximum(y11, np.transpose(y21))\n",
    "    xB = np.minimum(x12, np.transpose(x22))\n",
    "    yB = np.minimum(y12, np.transpose(y22))\n",
    "    interArea = np.maximum((xB - xA + 1), 0) * np.maximum((yB - yA + 1), 0)\n",
    "    boxAArea = (x12 - x11 + 1) * (y12 - y11 + 1)\n",
    "    boxBArea = (x22 - x21 + 1) * (y22 - y21 + 1)\n",
    "    iou = interArea / (boxAArea + np.transpose(boxBArea) - interArea)\n",
    "    return iou\n",
    "\n",
    "def f_beta(tp, fp, fn, beta=2):\n",
    "    return (1+beta**2)*tp / ((1+beta**2)*tp+beta**2*fn+fp)\n",
    "\n",
    "def calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th, verbose=False):\n",
    "    gt_bboxes = gt_bboxes.copy()\n",
    "    pred_bboxes = pred_bboxes.copy()\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for k, pred_bbox in enumerate(pred_bboxes): # fixed in ver.7\n",
    "        ious = calc_iou(gt_bboxes, pred_bbox[None, 1:])\n",
    "        max_iou = ious.max()\n",
    "        if max_iou > iou_th:\n",
    "            tp += 1\n",
    "            gt_bboxes = np.delete(gt_bboxes, ious.argmax(), axis=0)\n",
    "        else:\n",
    "            fp += 1\n",
    "        if len(gt_bboxes) == 0:\n",
    "            fp += len(pred_bboxes) - (k + 1) # fix in ver.7\n",
    "            break\n",
    "\n",
    "    fn = len(gt_bboxes)\n",
    "    return tp, fp, fn\n",
    "\n",
    "def calc_is_correct(gt_bboxes, pred_bboxes):\n",
    "    \"\"\"\n",
    "    gt_bboxes: (N, 4) np.array in xywh format\n",
    "    pred_bboxes: (N, 5) np.array in conf+xywh format\n",
    "    \"\"\"\n",
    "    if len(gt_bboxes) == 0 and len(pred_bboxes) == 0:\n",
    "        tps, fps, fns = 0, 0, 0\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    elif len(gt_bboxes) == 0:\n",
    "        tps, fps, fns = 0, len(pred_bboxes)*11, 0\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    elif len(pred_bboxes) == 0:\n",
    "        tps, fps, fns = 0, 0, len(gt_bboxes)*11\n",
    "        return tps, fps, fns\n",
    "    \n",
    "    pred_bboxes = pred_bboxes[pred_bboxes[:,0].argsort()[::-1]] # sort by conf\n",
    "    \n",
    "    tps, fps, fns = 0, 0, 0\n",
    "    for iou_th in np.arange(0.3, 0.85, 0.05):\n",
    "        tp, fp, fn = calc_is_correct_at_iou_th(gt_bboxes, pred_bboxes, iou_th)\n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "    return tps, fps, fns\n",
    "\n",
    "def calc_f2_score(gt_bboxes_list, pred_bboxes_list, verbose=False):\n",
    "    \"\"\"\n",
    "    gt_bboxes_list: list of (N, 4) np.array in xywh format\n",
    "    pred_bboxes_list: list of (N, 5) np.array in conf+xywh format\n",
    "    \"\"\"\n",
    "    tps, fps, fns, total_pred, total_gt = 0, 0, 0, 0, 0\n",
    "    for gt_bboxes, pred_bboxes in zip(gt_bboxes_list, pred_bboxes_list):\n",
    "        tp, fp, fn = calc_is_correct(gt_bboxes, pred_bboxes)\n",
    "        tps += tp\n",
    "        fps += fp\n",
    "        fns += fn\n",
    "        total_pred += len(pred_bboxes)\n",
    "        total_gt += len(gt_bboxes)\n",
    "        if verbose:\n",
    "            num_gt = len(gt_bboxes)\n",
    "            num_pred = len(pred_bboxes)\n",
    "            print(f'num_gt:{num_gt:<3} num_pred:{num_pred:<3} tp:{tp:<3} fp:{fp:<3} fn:{fn:<3}')\n",
    "    return f_beta(tps, fps, fns, beta=2), tps/(tps+fps), tps/(tps+fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:54.013155Z",
     "iopub.status.busy": "2022-02-02T08:09:54.012011Z",
     "iopub.status.idle": "2022-02-02T08:09:54.028769Z",
     "shell.execute_reply": "2022-02-02T08:09:54.027970Z",
     "shell.execute_reply.started": "2022-02-02T08:09:54.013116Z"
    }
   },
   "outputs": [],
   "source": [
    "from norfair import Detection, Tracker\n",
    "\n",
    "# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\n",
    "def to_norfair(detects, frame_id):\n",
    "    result = []\n",
    "    for x_min, y_min, x_max, y_max, score in detects:\n",
    "        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n",
    "        w, h = x_max - x_min, y_max - y_min\n",
    "        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\n",
    "def euclidean_distance(detection, tracked_object):\n",
    "    return np.linalg.norm(detection.points - tracked_object.estimate)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:54.031095Z",
     "iopub.status.busy": "2022-02-02T08:09:54.030151Z",
     "iopub.status.idle": "2022-02-02T08:09:54.047426Z",
     "shell.execute_reply": "2022-02-02T08:09:54.046243Z",
     "shell.execute_reply.started": "2022-02-02T08:09:54.031051Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_prediction(img, bboxes, gts, show=True):\n",
    "    colors = [(0, 0, 255)]\n",
    "\n",
    "    obj_names = [\"s\"]\n",
    "\n",
    "    for box in bboxes:\n",
    "#         cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (255,0,0), 2)\n",
    "        cv2.rectangle(img, (int(box[1]), int(box[2])), (int(box[1] + box[3]), int(box[2] + box[4])), (255,0,0), 2)\n",
    "        cv2.putText(img, f'{box[0]}', (int(box[1]), int(box[2])-3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 1, cv2.LINE_AA)\n",
    "        \n",
    "    for gt in gts:\n",
    "        cv2.rectangle(img, (int(gt[0]), int(gt[1])), (int(gt[0]+gt[2]), int(gt[1]+gt[3])), (0,255,0), 2)\n",
    "    \n",
    "    if show:\n",
    "        img = Img.fromarray(img).resize((960, 540))\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cots(learner, img, bbox, bbox_conf, conf_ratio=0.5, visualize=False):   \n",
    "    cropped_img = img[bbox[1] : bbox[3], bbox[0] : bbox[2]]\n",
    "#     cropped_img = Img.fromarray(cropped_img).resize((240, 200))\n",
    "#     cropped_img = np.asarray(cropped_img)\n",
    "    \n",
    "    with learner.no_bar():\n",
    "        preds = learner.predict(cropped_img)\n",
    "    pred_conf = float(preds[2][0])\n",
    "    \n",
    "    conf = bbox_conf + (pred_conf-0.5) * conf_ratio\n",
    "    conf = max(conf, 0)\n",
    "    conf = min(conf, 1)\n",
    "    \n",
    "    if visualize:\n",
    "        display(Img.fromarray(cropped_img))\n",
    "        print(bbox_conf, bbox)\n",
    "        print(preds)\n",
    "        print(conf)\n",
    "        print('+'*80)\n",
    "    \n",
    "    return conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GT bboxes from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:09:54.049824Z",
     "iopub.status.busy": "2022-02-02T08:09:54.049456Z",
     "iopub.status.idle": "2022-02-02T08:10:12.763284Z",
     "shell.execute_reply": "2022-02-02T08:10:12.762194Z",
     "shell.execute_reply.started": "2022-02-02T08:09:54.049780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539c3ce21064475eac02fef3779b5b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4729613b579f448e878d8c525576b595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4879b4f0ee584f8e8bd81606d52a6909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No BBox: 79.07% | With BBox: 20.93%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7957d7bf664742b94a143080e372d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir = f'{DATASET_PATH}'\n",
    "imgs = [dir + f for f in ('video_2/5748.jpg',\n",
    "                          'video_2/5772.jpg',\n",
    "                          'video_2/5820.jpg',\n",
    "                          'video_1/4159.jpg', \n",
    "                          'video_1/4183.jpg', \n",
    "                          'video_1/4501.jpg', \n",
    "                          'video_1/5375.jpg', \n",
    "                          'video_1/5414.jpg',\n",
    "                          'video_1/5495.jpg',\n",
    "                          'video_1/4775.jpg', \n",
    "                          'video_0/9794.jpg', \n",
    "                          'video_0/4502.jpg', \n",
    "                          'video_0/9651.jpg', \n",
    "                          'video_0/9700.jpg',  \n",
    "                          'video_0/9674.jpg',\n",
    "                          'video_0/20.jpg', \n",
    "                          'video_0/17.jpg', \n",
    "                          'video_1/5474.jpg', \n",
    "                          'video_0/0.jpg')]\n",
    "\n",
    "def get_path(row):\n",
    "    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n",
    "    return row\n",
    "\n",
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes\n",
    "\n",
    "# Train Data\n",
    "df = pd.read_csv(f'{ROOT_DIR}/train.csv')\n",
    "# df = df[df.video_id == FOLD]\n",
    "df = df.progress_apply(get_path, axis=1)\n",
    "df['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\n",
    "df.head(2)\n",
    "\n",
    "df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\n",
    "data = (df.num_bbox>0).value_counts(normalize=True)*100\n",
    "print(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")\n",
    "\n",
    "df['bboxes'] = df.annotations.progress_apply(get_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_bbox</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_bbox\n",
       "fold          \n",
       "0         1100\n",
       "1          704\n",
       "2          654\n",
       "3          577\n",
       "4          564\n",
       "5          285\n",
       "6          238\n",
       "7          252\n",
       "8          274\n",
       "9          271"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLD_NUM = 10\n",
    "\n",
    "train = df[df.num_bbox>0]\n",
    "kf = GroupKFold(n_splits = FOLD_NUM) \n",
    "train = train.reset_index(drop=True)\n",
    "train['fold'] = -1\n",
    "for f, (train_idx, val_idx) in enumerate(kf.split(train, y = train.video_id.tolist(), groups=train.sequence)):\n",
    "    train.loc[val_idx, 'fold'] = f\n",
    "\n",
    "train.groupby('fold').agg({'num_bbox': 'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model\n",
    "\n",
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:10:12.765435Z",
     "iopub.status.busy": "2022-02-02T08:10:12.765076Z",
     "iopub.status.idle": "2022-02-02T08:10:12.796357Z",
     "shell.execute_reply": "2022-02-02T08:10:12.794860Z",
     "shell.execute_reply.started": "2022-02-02T08:10:12.765397Z"
    }
   },
   "outputs": [],
   "source": [
    "def clahe_hsv(img):\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    h, s, v = hsv_img[:,:,0], hsv_img[:,:,1], hsv_img[:,:,2]\n",
    "    clahe = cv2.createCLAHE(clipLimit = 15.0, tileGridSize = (20,20))\n",
    "    v = clahe.apply(v)\n",
    "\n",
    "    hsv_img = np.dstack((h,s,v))\n",
    "\n",
    "    rgb = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return rgb\n",
    "\n",
    "def _load_model(path, conf, iou):\n",
    "    model = torch.hub.load('yolov5', \n",
    "                          'custom', \n",
    "                          path = path,\n",
    "                          source='local',\n",
    "                          force_reload=True)  # local repo\n",
    "    model.conf = conf\n",
    "    model.iou = iou\n",
    "    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n",
    "    model.multi_label = False  # NMS multiple labels per box\n",
    "    model.max_det = 1000  # maximum number of detections per image\n",
    "    return model\n",
    "\n",
    "def evaluate(path, test_df, conf, iou, img_size, area_thr, conf_thr=0.5, conf_ratio=0.5, augment=True, tracking=True, visualize=False):\n",
    "    # Tracker will update tracks based on detections from current frame\n",
    "    tracker = Tracker(\n",
    "        distance_function=euclidean_distance, \n",
    "        distance_threshold=30,\n",
    "        hit_inertia_min=3,\n",
    "        hit_inertia_max=6,\n",
    "        initialization_delay=1,\n",
    "    )\n",
    "        \n",
    "    # Save frame_id into detection to know which tracks have no detections on current frame\n",
    "    frame_id = 0\n",
    "    \n",
    "    model = _load_model(path, conf, iou)\n",
    "    gt_bboxes_list, prd_bboxes_list = [], []\n",
    "\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        bboxes = np.empty((0,5), int)\n",
    "        gt_bboxes, pred_bboxes = [], []\n",
    "        \n",
    "        img_path = row.image_path\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         img = clahe_hsv(img)\n",
    "\n",
    "        # get GT bboxes for evaluation\n",
    "        for gt in row.bboxes:\n",
    "            gt_bbox = np.array(list(map(float, gt)))\n",
    "            gt_bboxes.append(gt_bbox)\n",
    "        gt_bboxes_list.append(np.array(gt_bboxes))\n",
    "\n",
    "        r = model(img, size=img_size, augment=augment)\n",
    "        learner_resnet18 = load_learner('fastai_models/resnet18.pkl')\n",
    "        \n",
    "        \n",
    "        if r.pandas().xyxy[0].shape[0] == 0:\n",
    "            anno = ''\n",
    "        else:\n",
    "            for idx, row in r.pandas().xyxy[0].iterrows():\n",
    "                  bboxes = np.append(bboxes, [[row.xmin, row.ymin, row.xmax, row.ymax, row.confidence]], axis=0)\n",
    "        \n",
    "        predictions = []\n",
    "        detects = []\n",
    "        \n",
    "        if len(bboxes) > 0:\n",
    "            # if image classifier recognize cots - increase bbox confidence, else decrease it\n",
    "            for bbox in bboxes:\n",
    "                score, xmin, ymin, xmax, ymax = bbox[4], int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n",
    "                bbox_conf = is_cots(learner_resnet18, img, [xmin, ymin, xmax, ymax], score, conf_ratio, visualize=False) \n",
    "                \n",
    "                if bbox_conf > conf_thr:\n",
    "                    width, height = xmax-xmin, ymax-ymin\n",
    "                    detects.append(bbox)\n",
    "                    predictions.append('{:.2f} {} {} {} {}'.format(bbox[4], int(bbox[0]), int(bbox[1]), width, height))\n",
    "                    pred_bboxes.append(np.array([score, int(bbox[0]), int(bbox[1]), width, height]))\n",
    "    \n",
    "        #  Tracking\n",
    "        if tracking:\n",
    "            # Update tracks using detects from current frame\n",
    "            tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n",
    "            for tobj in tracked_objects:\n",
    "                bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n",
    "                if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n",
    "                    continue\n",
    "\n",
    "                # Add objects that have no detections on current frame to predictions\n",
    "                xc, yc = tobj.estimate[0]\n",
    "                x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n",
    "                score = tobj.last_detection.scores[0]\n",
    "                area = bbox_width * bbox_height\n",
    "                if area >= area_thr:\n",
    "                    predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n",
    "                    pred_bboxes.append(np.array([score, x_min, y_min, bbox_width, bbox_height]))\n",
    "        \n",
    "        if visualize and idx < 3:\n",
    "            display(show_prediction(img, pred_bboxes, gt_bboxes))\n",
    "        \n",
    "        # get pred bboxes for evaluation\n",
    "        prd_bboxes_list.append(np.array(pred_bboxes))\n",
    "        \n",
    "        prediction_str = ' '.join(predictions)\n",
    "        \n",
    "        frame_id += 1\n",
    "\n",
    "    f2_score, p, r = calc_f2_score(gt_bboxes_list, prd_bboxes_list, verbose=False)\n",
    "\n",
    "    return f2_score, p, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search optimal hypreparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:10:12.798897Z",
     "iopub.status.busy": "2022-02-02T08:10:12.798373Z",
     "iopub.status.idle": "2022-02-02T08:10:12.813938Z",
     "shell.execute_reply": "2022-02-02T08:10:12.812779Z",
     "shell.execute_reply.started": "2022-02-02T08:10:12.798844Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v6.0-244-g9cf80b7 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1070, 8112MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc0d7a71d4642cf9bc839e83dcb90d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 is 0.7208568523647583, Precision is 0.5898333285521673, Recall is 0.7632428820668918\n"
     ]
    }
   ],
   "source": [
    "# sequences = train.loc[train.fold == FOLD, 'sequence'].unique()\n",
    "test_df = df[df.video_id == FOLD]\n",
    "\n",
    "area_thr = 0\n",
    "img_size = 2700\n",
    "conf = 0.1\n",
    "iou = 0.4\n",
    "conf_thr = 0.6\n",
    "conf_ratio = 0.6\n",
    "\n",
    "augment = True\n",
    "tracking = True\n",
    "\n",
    "f2, p, r = evaluate(f_best, test_df, conf, iou, img_size, area_thr, conf_thr, conf_ratio, augment, tracking, visualize=False)\n",
    "print(f'F2 is {f2}, Precision is {p}, Recall is {r}')\n",
    "\n",
    "# # search for the best image size\n",
    "# size_dict = {}\n",
    "# best_size = 0\n",
    "# best_f2 = 0\n",
    "\n",
    "# for img_size in np.arange(2600, 3800, 200):\n",
    "#     f2, p, r = evaluate(f_best, test_df, conf, iou, img_size, area_thr, conf_thr, conf_ratio, augment, tracking, visualize=False)\n",
    "#     size_dict[conf] = f2\n",
    "#     print(80*'=')\n",
    "#     print(f'Image size is {img_size}, F2 is {f2}, Precision is {p}, Recall is {r}')\n",
    "#     print(80*'=')\n",
    "#     if f2 > best_f2:\n",
    "#         best_f2 = f2\n",
    "#         best_size = img_size\n",
    "        \n",
    "# print(f'Best image size is {best_size}, best F2 is {best_f2}')\n",
    "    \n",
    "# # search for the best confidence threshold\n",
    "# conf_dict = {}\n",
    "# best_conf = 0\n",
    "# best_f2 = 0\n",
    "\n",
    "# for conf_thr in np.arange(0.3, 0.7, 0.05):\n",
    "#     f2, p, r = evaluate(f_best, test_df, conf, iou, img_size, area_thr, conf_thr, conf_ratio, augment, tracking, visualize=False)\n",
    "#     conf_dict[conf_thr] = f2\n",
    "#     print(80*'=')\n",
    "#     print(f'Confidence threshold is {conf_thr}, F2 is {f2}, Precision is {p}, Recall is {r}')\n",
    "#     print(80*'=')\n",
    "#     if f2 > best_f2:\n",
    "#         best_f2 = f2\n",
    "#         best_conf = conf_thr\n",
    "        \n",
    "# print(f'Best confidence threshold is {best_conf}, best F2 is {best_f2}')\n",
    "\n",
    "# # search for the best IOU\n",
    "# iou_dict = {}\n",
    "# best_iou = 0\n",
    "# best_f2 = 0\n",
    "# conf_thr = 0.5\n",
    "# conf_ratio = 0.3\n",
    "\n",
    "# for iou in np.arange(0.35, 0.5, 0.05):\n",
    "#     f2, p, r = evaluate(f_best, test_df, conf, iou, img_size, area_thr, conf_thr, conf_ratio, augment, tracking, visualize=False)\n",
    "#     print(80*'=')\n",
    "#     print(f'IOU is {iou}, F2 is {f2}, Precision is {p}, Recall is {r}')\n",
    "#     print(80*'=')\n",
    "#     if f2 > best_f2:\n",
    "#         best_f2 = f2\n",
    "#         best_iou = iou\n",
    "\n",
    "# print(f'Best IOU is {best_iou}, best F2 is {best_f2}')\n",
    "\n",
    "# # search for the best area\n",
    "# area_dict = {}\n",
    "# best_area = 0\n",
    "# best_f2 = 0\n",
    "\n",
    "# for area_thr in np.arange(0, 450, 50):\n",
    "#     f2, p, r = evaluate(f_best, test_df, conf, iou, img_size, area_thr, conf_thr, conf_ratio, augment, tracking, visualize=False)\n",
    "#     area_dict[area_thr] = f2\n",
    "#     print(80*'=')\n",
    "#     print(f'Area threshold is {area_thr}, F2 is {f2}, Precision is {p}, Recall is {r}')\n",
    "#     print(80*'=')\n",
    "#     if f2 > best_f2:\n",
    "#         best_f2 = f2\n",
    "#         best_area = area_thr\n",
    "\n",
    "# print(f'Best area is {best_area}, best F2 is {best_f2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v6.0-244-g9cf80b7 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1070, 8112MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cf12d066bf446881f8fdbc8c1a5a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 is 0.7275503231500726, Precision is 0.6244806940371457, Recall is 0.75886261553881\n"
     ]
    }
   ],
   "source": [
    "conf_thr = 0.6\n",
    "conf_ratio = 0.5\n",
    "\n",
    "augment = True\n",
    "tracking = True\n",
    "\n",
    "f2, p, r = evaluate(f_best, test_df, conf, iou, img_size, area_thr, conf_thr, conf_ratio, augment, tracking, visualize=False)\n",
    "print(f'F2 is {f2}, Precision is {p}, Recall is {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v6.0-244-g9cf80b7 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1070, 8112MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3548dae5389e4dcba31bed0eac838670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 is 0.7105877506179621, Precision is 0.546352021961778, Recall is 0.7683284457478006\n"
     ]
    }
   ],
   "source": [
    "conf_thr = 0.6\n",
    "conf_ratio = 0.7\n",
    "\n",
    "augment = True\n",
    "tracking = True\n",
    "\n",
    "f2, p, r = evaluate(f_best, test_df, conf, iou, img_size, area_thr, conf_thr, conf_ratio, augment, tracking, visualize=False)\n",
    "print(f'F2 is {f2}, Precision is {p}, Recall is {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v6.0-244-g9cf80b7 torch 1.10.2 CUDA:0 (NVIDIA GeForce GTX 1070, 8112MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 280 layers, 12308200 parameters, 0 gradients, 16.2 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e52a3eaf62141aba71b5f7d66740335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 is 0.7375072784006927, Precision is 0.754102740248836, Recall is 0.7334719180370467\n"
     ]
    }
   ],
   "source": [
    "conf_thr = 0.6\n",
    "conf_ratio = 0.4\n",
    "\n",
    "augment = True\n",
    "tracking = False\n",
    "\n",
    "f2, p, r = evaluate(f_best, test_df, conf, iou, img_size, area_thr, conf_thr, conf_ratio, augment, tracking, visualize=False)\n",
    "print(f'F2 is {f2}, Precision is {p}, Recall is {r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F2_na_10_adam, conf_thr 0.5, conf_ratio 0.3, image size is 2700, F2 = 0.725\n",
    "- F2_na_10_adam, conf_thr 0.5, conf_ratio 0.3, image size is 2800, F2 = 0.720\n",
    "- F2_na_10_adam, conf_thr 0.5, conf_ratio 0.3, image size is 2900, F2 = 0.719\n",
    "- F2_na_10_adam, conf_thr 0.5, conf_ratio 0.3, image size is 3000, F2 = 0.717\n",
    "- F2_na_10_adam, conf_thr 0.5, conf_ratio 0.3, image size is 3200, F2 = 0.712\n",
    "- F2_na_10_adam, conf_thr 0.5, conf_ratio 0.3, image size is 3400, F2 = 0.703"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- F0_best - Conf 0.28, image size is 3200, F2 is 0.578\n",
    "- F1_best - Conf 0.28, image size is 2560, F2 is 0.558\n",
    "- F2_best - Conf 0.28, image size is 3200, F2 is 0.700\n",
    "\n",
    "\n",
    "- F0_na_20_best - Conf 0.28, image size is 3200, F2 is 0.581\n",
    "- F1_na_20_best - Conf 0.28, image size is 2560, F2 is 0.558\n",
    "- F2_na_20_best - Conf 0.28, image size is 3200, F2 is 0.678\n",
    "\n",
    "\n",
    "- F0_na_20_last - Conf 0.28, image size is 3200, F2 is 0.58\n",
    "- F1_na_20_last - Conf 0.28, image size is 2560, F2 is 0.545  3200 - 0.544\n",
    "- F2_na_20_last - Conf 0.28, image size is 3200, F2 is 0.706\n",
    "\n",
    "\n",
    "- F0_na_10_best - Conf 0.28, image size is 3200, F2 is \n",
    "- F1_na_10_best - Conf 0.28, image size is 2560, F2 is\n",
    "- F2_na_10_best - Conf 0.28, image size is 3200, F2 is \n",
    "\n",
    "\n",
    "- F0_na_10_last - Conf 0.28, image size is 3200, F2 is \n",
    "- F1_na_10_last - Conf 0.28, image size is 2560, F2 is\n",
    "- F2_na_10_last - Conf 0.28, image size is 3200, F2 is \n",
    "\n",
    "Recommended image size is 3200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference\n",
    "\n",
    "## Initialize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:10:12.816988Z",
     "iopub.status.busy": "2022-02-02T08:10:12.816557Z",
     "iopub.status.idle": "2022-02-02T08:10:12.861731Z",
     "shell.execute_reply": "2022-02-02T08:10:12.860988Z",
     "shell.execute_reply.started": "2022-02-02T08:10:12.816951Z"
    }
   },
   "outputs": [],
   "source": [
    "import greatbarrierreef\n",
    "env = greatbarrierreef.make_env()# initialize the environment\n",
    "iter_test = env.iter_test()      # an iterator which loops over the test set and sample submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-02T08:10:12.863574Z",
     "iopub.status.busy": "2022-02-02T08:10:12.863004Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_dict = {\n",
    "    'id': [],\n",
    "    'prediction_string': [],\n",
    "}\n",
    "\n",
    "model = _load_model(yolov5s6_1920_batch_8_groupk_f0, conf, iou)\n",
    "\n",
    "#######################################################\n",
    "#                      Tracking                       #\n",
    "#######################################################\n",
    "\n",
    "# Tracker will update tracks based on detections from current frame\n",
    "# Matching based on euclidean distance between bbox centers of detections \n",
    "# from current frame and tracked_objects based on previous frames\n",
    "# You can check it's parameters in norfair docs\n",
    "# https://github.com/tryolabs/norfair/blob/master/docs/README.md\n",
    "tracker = Tracker(\n",
    "    distance_function=euclidean_distance, \n",
    "    distance_threshold=30,\n",
    "    hit_inertia_min=3,\n",
    "    hit_inertia_max=6,\n",
    "    initialization_delay=1,\n",
    ")\n",
    "\n",
    "# Save frame_id into detection to know which tracks have no detections on current frame\n",
    "frame_id = 0\n",
    "#######################################################\n",
    "\n",
    "for img, pred_df in tqdm(iter_test):\n",
    "    anno = ''\n",
    "    bboxes = np.empty((0,5), int)\n",
    "    predictions, detects = [], []\n",
    "    \n",
    "    r = model(img, size=img_size, augment=augment)\n",
    "\n",
    "    if r.pandas().xyxy[0].shape[0] == 0:\n",
    "        anno = ''\n",
    "    else:\n",
    "        for idx, row in r.pandas().xyxy[0].iterrows():\n",
    "            bboxes = np.append(bboxes, [[row.xmin, row.ymin, row.xmax, row.ymax, row.confidence]], axis=0)\n",
    "        \n",
    "        for bbox in bboxes:\n",
    "            detects.append(bbox)\n",
    "            width, height = int(bbox[2]-bbox[0]), int(bbox[3]-bbox[1])\n",
    "            area = width * height\n",
    "            if area >= area_thr:\n",
    "                predictions.append('{:.2f} {} {} {} {}'.format(bbox[4], int(bbox[0]), int(bbox[1]), width, height))\n",
    "                                   \n",
    "    #######################################################\n",
    "    #                      Tracking                       #\n",
    "    #######################################################\n",
    "    \n",
    "    # Update tracks using detects from current frame\n",
    "    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n",
    "    for tobj in tracked_objects:\n",
    "        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n",
    "        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n",
    "            continue\n",
    "            \n",
    "        # Add objects that have no detections on current frame to predictions\n",
    "        xc, yc = tobj.estimate[0]\n",
    "        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n",
    "        score = tobj.last_detection.scores[0]\n",
    "        area = bbox_width * bbox_height\n",
    "        if area >= area_thr:\n",
    "            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n",
    "    #######################################################\n",
    "    \n",
    "    prediction_str = ' '.join(predictions)\n",
    "    pred_df['annotations'] = prediction_str\n",
    "    env.predict(pred_df)\n",
    "\n",
    "    print('Prediction:', prediction_str)\n",
    "    frame_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('submission.csv')\n",
    "sub_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
