{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17dc1f5",
   "metadata": {},
   "source": [
    "# LGBM + CV + stacking\n",
    "\n",
    "## Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "param_list = glob.glob(\"optuna_lgbm*.csv\")\n",
    "models = list()\n",
    "best_params = list()\n",
    "\n",
    "lgbm_params = pd.DataFrame()\n",
    "\n",
    "for f in param_list:\n",
    "    gb_type = [f.split('_')][0][2][:-4]\n",
    "    tmp = pd.read_csv(f, index_col='Unnamed: 0')\n",
    "    tmp['params_boosting_type'] = gb_type\n",
    "    if lgbm_params.shape[0] == 0:\n",
    "        lgbm_params = tmp\n",
    "    else:\n",
    "        lgbm_params = pd.concat([lgbm_params, tmp])\n",
    "        \n",
    "lgbm_params = lgbm_params.sort_values('value').head(20)\n",
    "param_cols = [c for c in lgbm_params.columns if c.startswith('params_')]\n",
    "lgbm_params = lgbm_params[param_cols]\n",
    "\n",
    "\n",
    "for idx, row in lgbm_params.iterrows():\n",
    "    row_dict = {k[7:]: v for k, v in row.items()}\n",
    "    row_dict['objective'] = 'binary'\n",
    "    row_dict['metric'] = 'none'\n",
    "#     row_dict['subsample_for_bin'] = 300000\n",
    "    row_dict['force_col_wise'] = False\n",
    "    row_dict['early_stopping_rounds'] = 50\n",
    "    row_dict['verbose'] = -1\n",
    "    row_dict['max_bin'] = 255\n",
    "    row_dict['bagging_freq'] = int(row_dict['bagging_freq'])\n",
    "    if row_dict['bagging_fraction'] != row_dict['bagging_fraction']:\n",
    "        row_dict['bagging_fraction'] = None\n",
    "    row_dict['min_child_samples'] = int(row_dict['min_child_samples'])\n",
    "    row_dict['n_estimators'] = 3000 #int(row_dict['n_estimators'])\n",
    "    \n",
    "    row_dict['learning_rate'] = 0.06733232950390658\n",
    "    row_dict['num_leaves'] = int(row_dict['num_leaves'])\n",
    "    row_dict['max_depth'] = int(row_dict['max_depth'])\n",
    "    row_dict['is_unbalance'] = True\n",
    "    row_dict['class_weight'] = 'balanced'\n",
    "    row_dict['verbose'] = -1\n",
    "    \n",
    "    best_params.append(row_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064c43c",
   "metadata": {},
   "source": [
    "## Stacking with the Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "\n",
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def lgbm_training():\n",
    "    models_ = list()\n",
    "    bll_list = list()\n",
    "    weights_ = list()\n",
    "    \n",
    "    X, y = train_df[features], train_df.Class\n",
    "#     X, y = generated_features_train, train_df.Class\n",
    "     \n",
    "    kf = MultilabelStratifiedKFold(n_splits=n_folds, shuffle=True, random_state=8062023+20)\n",
    "    metric = balanced_log_loss\n",
    "    eval_results_ = {}     # used to store evaluation results for each fold\n",
    "\n",
    "    oof_level2 = np.zeros([y.shape[0], len(best_params) + 1])\n",
    "    oof_level2[:, len(best_params)] = y\n",
    "\n",
    "    print(f\"Training with {blu}{X.shape[1]}{res} features\")\n",
    "\n",
    "    for fold, (fit_idx, val_idx) in tqdm(enumerate(kf.split(X=train_df, y=greeks.iloc[:,1:3]), start = 1),\n",
    "                                         total=n_folds):\n",
    "        \n",
    "        # Split the dataset according to the fold indexes.\n",
    "        X_train = X.iloc[fit_idx]\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_train = y.iloc[fit_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        for i, params in enumerate(best_params):\n",
    "            \n",
    "            clf = lgb.LGBMClassifier(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n",
    "                    eval_metric=bll_metric, verbose=-1)\n",
    "            models_.append(clf)\n",
    "\n",
    "            val_preds = clf.predict_proba(X_val)[:,1]\n",
    "            oof_level2[val_idx, i] = val_preds\n",
    "\n",
    "            val_score = balanced_log_loss(y_val, val_preds)\n",
    "            best_iter = clf.best_iteration_\n",
    "\n",
    "            print(clf.best_iteration_)\n",
    "            \n",
    "            print(f'Fold: {blu}{fold:>3}{res}| bll_metric: {blu}{val_score:.5f}{res}'\n",
    "                  f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "        \n",
    "    return oof_level2\n",
    "\n",
    "oof_level2 = lgbm_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83fc60",
   "metadata": {},
   "source": [
    "## Stacking with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204709b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = oof_level2[:,:-1]\n",
    "y = oof_level2[:,-1]\n",
    "\n",
    "# mean bll\n",
    "print(balanced_log_loss(y, np.mean(X, axis=1)))\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "lr.fit(X, y)\n",
    "\n",
    "pred = lr.predict_proba(X)[:,1]\n",
    "\n",
    "# lr bll\n",
    "print(balanced_log_loss(y, pred))\n",
    "\n",
    "weights_ = lr.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce02d1",
   "metadata": {},
   "source": [
    "# LGBM + TSS + Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d945bfb",
   "metadata": {},
   "source": [
    "### Create metafeatures for the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5121d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates for the second level\n",
    "dates_level2 = [28, 29, 30, 31, 32, 33]\n",
    "\n",
    "# train dates\n",
    "dates_train = X_train.date_block_num\n",
    "# dates_train_level2 = dates_train[dates_train.isin(dates)]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin(dates_level2)]\n",
    "\n",
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], len(best_params)+1])\n",
    "X_train_level2[:, len(best_params)] = y_train_level2\n",
    "\n",
    "meta_index_begin = 0\n",
    "meta_index_end = 0\n",
    "\n",
    "# Now fill `X_train_level2` with metafeatures\n",
    "for cur_block_num in tqdm_notebook(dates_level2):\n",
    "    \n",
    "    # split data\n",
    "    train_index = X_train.loc[dates_train <  cur_block_num].index\n",
    "    test_index  = X_train.loc[dates_train == cur_block_num].index\n",
    "    \n",
    "    X_train_l2 = X_train.loc[train_index, :]\n",
    "    X_test_l2 =  X_train.loc[test_index, :]\n",
    "\n",
    "    y_train_l2 = y_train[train_index]\n",
    "    y_test_l2 =  y_train[test_index]\n",
    "    \n",
    "    meta_index_end += y_test_l2.shape[0]\n",
    "    \n",
    "    # predict metafeatures for each of LGBM regressors\n",
    "    for i, params in enumerate(tqdm_notebook(best_params)):\n",
    "        lgb = LGBMRegressor(**params)\n",
    "        reg = lgb.fit(X_train_l2, y_train_l2)\n",
    "        pred = lgb.predict(X_test_l2)\n",
    "        X_train_level2[meta_index_begin:meta_index_end, i] = pred\n",
    "\n",
    "        del lgb, reg, pred\n",
    "        gc.collect()\n",
    "        \n",
    "    meta_index_begin = meta_index_end\n",
    "        \n",
    "X_train_level2 = pd.DataFrame(X_train_level2, columns=[f'lgbr_{i+1}' for i in range(len(best_params))]+['gt'])\n",
    "X_train_level2.to_pickle('LGBM_X_train_level2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cebfa7",
   "metadata": {},
   "source": [
    "### Create metafeatures for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_level2 = np.zeros([X_test.shape[0], len(best_params)])\n",
    "\n",
    "for i, params in enumerate(tqdm_notebook(best_params)):\n",
    "    lgb = LGBMRegressor(**params)\n",
    "    reg = lgb.fit(X_train, y_train)\n",
    "    pred = lgb.predict(X_test)\n",
    "    X_test_level2[:, i] = pred\n",
    "    \n",
    "    del lgb, reg, pred\n",
    "    gc.collect()\n",
    "    \n",
    "X_test_level2 = pd.DataFrame(X_test_level2, columns=[f'lgbr_{i+1}' for i in range(len(best_params))])\n",
    "X_test_level2.to_pickle('LGBM_X_test_level2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5cada6",
   "metadata": {},
   "source": [
    "### Stacking with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3fa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = X_train_level2[[c for c in X_train_level2.columns if c != 'gt']]\n",
    "y = X_train_level2['gt']\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "pred = lr.predict(X_test_level2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
