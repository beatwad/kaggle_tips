{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is copy of this one https://www.kaggle.com/steamedsheep/yolov5-is-all-you-need I just follow the advices from discussion and changed the input size from 3000 to 10000**","metadata":{}},{"cell_type":"markdown","source":"# Install necesessary libraries","metadata":{}},{"cell_type":"code","source":"# norfair dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n# norfair\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:49:16.054624Z","iopub.execute_input":"2022-02-14T14:49:16.055324Z","iopub.status.idle":"2022-02-14T14:50:31.209999Z","shell.execute_reply.started":"2022-02-14T14:49:16.055229Z","shell.execute_reply":"2022-02-14T14:50:31.209147Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n\nimport cv2\nimport sys\nimport torch\nimport ast\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nfrom norfair import Detection, Tracker\n\nfrom PIL import Image\nfrom IPython.display import display\n\nsys.path.append('../input/tensorflow-great-barrier-reef')\nsys.path.append('../input/weightedboxesfusion/')\nsys.path.append('../input/boxes-mask')\nsys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n\nimport timm\nfrom fastai.vision.all import *\nfrom ensemble_boxes.ensemble_boxes_wbf import weighted_boxes_fusion\nfrom boxes_mask import box_voting\n\n!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/\n\nimport greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:53:59.844843Z","iopub.execute_input":"2022-02-14T14:53:59.845457Z","iopub.status.idle":"2022-02-14T14:54:01.239597Z","shell.execute_reply.started":"2022-02-14T14:53:59.845419Z","shell.execute_reply":"2022-02-14T14:54:01.238587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"def _load_model(path, conf, iou):\n    model = torch.hub.load('../input/yolov5-lib-ds', \n                          'custom', \n                          path = path,\n                          source='local',\n                          force_reload=True)  # local repo\n    model.conf = conf\n    model.iou = iou\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model\n\n\npath1 = '../input/yolov5s6-1920-b8-10pct-adam-35ep-video/f0.pt'\nmodel1 = _load_model(path1, 0.1, 0.4)\npath2 = '../input/yolov5s6-1920-b8-10pct-adam-35ep-video/f1.pt'\nmodel2 = _load_model(path2, 0.1, 0.4)\npath3 = '../input/yolov5s6-1920-b8-10pct-adam-35ep-video/f2.pt'\nmodel3 = _load_model(path3, 0.1, 0.4)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:50:37.115784Z","iopub.execute_input":"2022-02-14T14:50:37.116069Z","iopub.status.idle":"2022-02-14T14:50:48.281257Z","shell.execute_reply.started":"2022-02-14T14:50:37.116032Z","shell.execute_reply":"2022-02-14T14:50:48.280544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference utils","metadata":{}},{"cell_type":"code","source":"def is_cots(learner, img, bbox, bbox_conf, conf_ratio=0.5, visualize=False):   \n    cropped_img = img[bbox[1] : bbox[3], bbox[0] : bbox[2]]\n#     cropped_img = Img.fromarray(cropped_img).resize((240, 200))\n#     cropped_img = np.asarray(cropped_img)\n    \n    with learner.no_bar():\n        preds = learner.predict(cropped_img)\n    pred_conf = preds[2][0]\n    \n    conf = bbox_conf + (pred_conf-0.5) * conf_ratio\n    conf = max(conf, 0)\n    conf = min(conf, 1)\n    \n    if visualize:\n        display(Img.fromarray(cropped_img))\n        print(bbox_conf, bbox)\n        print(preds)\n        print(conf)\n        print('+'*80)\n    \n    return conf","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:57:24.118495Z","iopub.execute_input":"2022-02-14T14:57:24.118759Z","iopub.status.idle":"2022-02-14T14:57:24.125536Z","shell.execute_reply.started":"2022-02-14T14:57:24.118731Z","shell.execute_reply":"2022-02-14T14:57:24.124803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tracking utils","metadata":{}},{"cell_type":"code","source":"##############################################################\n#                      Tracking helpers                      #\n##############################################################\n\nimport numpy as np\nfrom norfair import Detection, Tracker\n\n# Helper to convert bbox in format [x_min, y_min, x_max, y_max, score] to norfair.Detection class\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n        \n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n\n\n#######################################################\n#                      Tracking                       #\n#######################################################\n\n# Tracker will update tracks based on detections from current frame\n# Matching based on euclidean distance between bbox centers of detections \n# from current frame and tracked_objects based on previous frames\n# You can check it's parameters in norfair docs\n# https://github.com/tryolabs/norfair/blob/master/docs/README.md\ntracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:50:48.293458Z","iopub.execute_input":"2022-02-14T14:50:48.293733Z","iopub.status.idle":"2022-02-14T14:50:48.304254Z","shell.execute_reply.started":"2022-02-14T14:50:48.293697Z","shell.execute_reply":"2022-02-14T14:50:48.303117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare validation data","metadata":{}},{"cell_type":"code","source":"# def get_bbox(annots):\n#     bboxes = [list(annot.values()) for annot in annots]\n#     return bboxes\n\n# def get_path(row):\n#     row['image_path'] = f'/kaggle/input/tensorflow-great-barrier-reef/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n#     return row\n\n# df = pd.read_csv('/kaggle/input/tensorflow-great-barrier-reef/train.csv')\n# df['annotations'] = df['annotations'].apply(lambda x: ast.literal_eval(x))\n# df['num_bbox'] = df['annotations'].apply(lambda x: len(x))\n# df['bboxes'] = df.annotations.progress_apply(get_bbox)\n# df = df.apply(get_path, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:55:11.873082Z","iopub.execute_input":"2022-02-14T14:55:11.873787Z","iopub.status.idle":"2022-02-14T14:55:11.879461Z","shell.execute_reply.started":"2022-02-14T14:55:11.873751Z","shell.execute_reply":"2022-02-14T14:55:11.878768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# image_paths = df[df.num_bbox>4].sample(100)\n# image_paths.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-14T15:03:43.515126Z","iopub.execute_input":"2022-02-14T15:03:43.515878Z","iopub.status.idle":"2022-02-14T15:03:43.520736Z","shell.execute_reply.started":"2022-02-14T15:03:43.515841Z","shell.execute_reply":"2022-02-14T15:03:43.518581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensembling with WBF + Tracking","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = 1280\niou_thr = 0.5\nmin_box_size = 0\nconf_thr = 0.6\nconf_ratio = 0.7\n\nCOCO_CLASSES = (\n                  \"starfish\",\n                )\n\nsubmission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\n# WBF\ndef run_wbf(bboxes, scores, orig_image_size=IMG_SIZE, iou_thr=iou_thr, skip_box_thr=0.0, weights=None):\n    # normalize bboxes\n    for i in range(len(bboxes)):\n        for j in range(len(bboxes[i])):\n            for k in range(4):\n                bboxes[i][j][k] /= (orig_image_size-1)\n    labels = [np.ones(len(score)) for score in scores]\n    # fuse bboxes\n    bboxes, scores, labels = weighted_boxes_fusion(bboxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    # return bboxes to the original size\n    bboxes = [bbox*(orig_image_size-1) for bbox in bboxes]\n    return bboxes, scores\n\n# Box voting\ndef run_bbox_voting(bboxes, scores, all_bboxes, thresh=iou_thr, scoring_method='IOU_WAVG', beta=1.0, mask=None):\n    top_bboxes =  np.empty((0,5), float)\n    for bbox, score in zip(bboxes, scores):\n        top_bbox = np.array([[bbox[0], bbox[1], bbox[2], bbox[3], score]])\n        top_bboxes = np.append(top_bboxes, top_bbox, axis=0)\n    bboxes = box_voting(top_bboxes, all_bboxes, thresh, scoring_method, beta, mask)\n    bboxes, scores = bboxes[:, :4], bboxes[:, 4]\n    return bboxes, scores\n\ndef draw_predictions(img, bboxes, scores, classes_dict):\n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        cls_id = 0\n        score = scores[i]\n        x0 = int(box[0])\n        y0 = int(box[1])\n        x1 = int(box[2])\n        y1 = int(box[3])\n\n        cv2.rectangle(img, (x0, y0), (x1, y1), (0, 255, 0), 2)\n        cv2.putText(img, '{}:{:.1f}%'.format(classes_dict[cls_id], score * 100), (x0, y0 - 3), cv2.FONT_HERSHEY_PLAIN, 0.8, (0,255,0), thickness = 1)\n    return img\n\n# Set parameters for fastai binary image classificator \nlearner_resnet18 = load_learner(f'../input/fastai-models/resnet18.pkl')\n# Save frame_id into detection to know which tracks have no detections on current frame\nframe_id = 0\n\nglobal bboxes, scores, labels\n# for idx, row in tqdm(image_paths.iterrows()):\n#     img = cv2.imread(row.image_path)\n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nfor img, pred_df in tqdm(iter_test):\n    predictions, detects = [], []\n    bboxes, scores, labels = [], [], []\n    all_bboxes = np.empty((0,5), float)\n    \n    r1 = model1(img, size=2700, augment=True)\n    r2 = model2(img, size=2700, augment=True)\n    r3 = model3(img, size=2700, augment=True)\n    \n    def get_scores_labels_boxes(model_output):\n        global bboxes, scores, labels, all_bboxes\n        output = model_output.pandas().xyxy[0]\n        if output.shape[0] != 0:\n            bbox, score, label = [], [], []\n            for idx, row in output.iterrows():\n                # Skip too small boxes\n                len_x, len_y = row['xmax'] - row['xmin'], row['ymax'] - row['ymin']\n                if len_x < min_box_size or len_y < min_box_size:\n                    continue\n                \n                all_bboxes = np.append(all_bboxes, np.array([row[['xmin', 'ymin', 'xmax', 'ymax', 'confidence']].values]), axis=0)\n                bbox.append(row[['xmin', 'ymin', 'xmax', 'ymax']].values)\n                score.append(row['confidence'])\n                label.append(row['class'])\n            \n            if bbox:\n                bboxes.append(bbox)\n                scores.append(score)\n                labels.append(label)   \n                                   \n    get_scores_labels_boxes(r1)\n    get_scores_labels_boxes(r2)\n    get_scores_labels_boxes(r3)\n\n    if len(bboxes) > 0:\n        bboxes, scores = run_wbf(bboxes, scores)\n        bboxes, scores = run_bbox_voting(bboxes, scores, all_bboxes)\n        for bbox, score in zip(bboxes, scores):\n            xmin, ymin, xmax, ymax = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n            bbox_conf = is_cots(learner_resnet18, img, [xmin, ymin, xmax, ymax], score, conf_ratio, visualize=False) \n            \n            if bbox_conf > conf_thr:\n                width, height = xmax-xmin, ymax-ymin\n                detects.append([xmin, ymin, xmax, ymax, score])\n                predictions.append('{:.2f} {} {} {} {}'.format(score, xmin, ymin, width, height))\n\n    #######################################################\n    #                      Tracking                       #\n    #######################################################\n    \n    # Update tracks using detects from current frame\n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        # Skip objects that were detected on current frame\n        if last_detected_frame_id == frame_id:  \n            continue\n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n        # Skip too small boxes\n        if bbox_width < min_box_size or bbox_height < min_box_size:\n            continue\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    #######################################################\n    \n    prediction_str = ' '.join(predictions)\n    pred_df['annotations'] = prediction_str\n    env.predict(pred_df)\n\n#     print('Prediction:', prediction_str)\n    frame_id += 1\n\n    # Draw predictions\n#     out_image = draw_predictions(img, bboxes, scores, COCO_CLASSES)\n\n    # Since we load image using OpenCV we have to convert it \n#     out_image = cv2.cvtColor(out_image, cv2.COLOR_BGR2RGB)\n#     display(Image.fromarray(out_image))","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:57:30.724644Z","iopub.execute_input":"2022-02-14T14:57:30.724895Z","iopub.status.idle":"2022-02-14T15:00:50.79543Z","shell.execute_reply.started":"2022-02-14T14:57:30.724867Z","shell.execute_reply":"2022-02-14T15:00:50.794333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df = pd.read_csv('submission.csv')\nsub_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-14T14:50:49.011702Z","iopub.status.idle":"2022-02-14T14:50:49.01213Z","shell.execute_reply.started":"2022-02-14T14:50:49.011891Z","shell.execute_reply":"2022-02-14T14:50:49.011917Z"},"trusted":true},"execution_count":null,"outputs":[]}]}