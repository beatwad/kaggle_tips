# ICR Kaggle Competition

## TODO list
* âœ… Run and train ***TabPFN*** model **-->** *Increased* LB position and *decreased* log_loss in during training
* âœ… Nested CV
* ğŸŸ¦ TabPFN, as all transformers, may be sensitive to uninformative features. And this dataset has a lot of them. Try to drop features one-by-one and check if TabPFN performance increased
* âº Feature engineering (log, square, sqrt, plus, minus)
* ğŸŸ¦ Should we delete objects with outliers? Or cap the outliers values? (Use IsolationForest to detect outliers)
* ğŸŸ¦ Analyze features with high correlation, should we drop some of them?
* ğŸŸ¦ Add mean distance (cosine, Manhattan, Euclidean etc) to N nearest neighbours for each class, increase confidence
     for object class if that object has a lot of Nearest Neigbours with the same class
* ğŸŸ¦ Add similarity approach with distance features (cosine, Manhattan, Euclidean etc)
* ğŸŸ¦ Add post-processing based on additional target data (look into Ideas)
* ğŸŸ¦ Split binary prediction into multi-label
* ğŸŸ¦ Balance class samples (undersampling, post-processing, SMOTE)
* âœ… Data leakage exploit
* ğŸŸ¦ What type of imputation must be used? median/mean imputing of KNN-imputing? is NaNs are zero? or don't use at all?
* ğŸŸ¦ Ensemble TabPFNs with different number of ensembles in settings
* ğŸŸ¦ Ensemble TabPFN with LGBM and CatBoost and KNN
* ğŸŸ¦ 


âœ… - Done <br>
ğŸŸ¦ - Planning <br>
âº - In progress <br>
â¸ - Put on hold <br>
âŒ - Canceled <br>
âš ï¸ - Check it out <br>

## Ideas
* ğŸŸ¦ Yo Daug I've heard you like boosting, so let's predict with TabPFN/KNN and boost it's errors with boosting
* ğŸŸ¦ Post-processing: DO NOT use code like test[test['class_0'] < 0.13] = 0 or test[test['class_0'] > 0.87] = 1. 
     One false classified object gives an error = -log(1e-15) = 34.53 => log-loss increases dramtically! So if use
     post-processing, use test[test['class_0'] < 0.13] = 0.1/0.01 or test[test['class_0'] > 0.87] = 0.9/0.99

## Experiments
* Simple *LGBM* with no fine-tuning: **LB 0.44**
* Fine-tuned *LGBM* by AutoML: **LB 0.31**
* *TabPFN*: **LB: 0.26** 

## Resources:
* https://arxiv.org/abs/2207.01848
* https://arxiv.org/abs/2211.02941
* https://github.com/automl/TabPFN
* https://www.kaggle.com/code/beatwad/dealing-with-very-small-datasets/edit
* https://sebastianraschka.com/blog/2022/deep-learning-for-tabular-data.html

